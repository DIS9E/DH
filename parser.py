import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"
}

def parse_post(url):
    print(f"ğŸ“„ íŒŒì‹± ì¤‘: {url}")
    res = requests.get(url, headers=HEADERS)
    if res.status_code != 200:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {res.status_code}")
        return None

    soup = BeautifulSoup(res.text, "html.parser")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì œëª© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    title_tag = soup.select_one("h1.pagetitle.posttitle._js-pagetitle-text")
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë³¸ë¬¸ HTML â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    content_div = soup.select_one("div.text")
    html_content = str(content_div) if content_div else ""

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì£¼ì†Œ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    address = "ì •ë³´ ì—†ìŒ"
    addr_patterns = re.compile(r"(ÑƒĞ»\.|Ğ¿Ñ€\.|Ğ³\.|Ğ´Ğ¾Ğ¼|ÑƒĞ»Ğ¸Ñ†Ğ°)", re.IGNORECASE)
    for tag in soup.find_all("div", class_="text"):
        if addr_patterns.search(tag.get_text()):
            address = tag.get_text(strip=True)
            break

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì˜ì—…ì‹œê°„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    hours = []
    hour_pattern = re.compile(r"(Ñ\s*\d{1,2}:\d{2}\s*Ğ´Ğ¾\s*\d{1,2}:\d{2})", re.IGNORECASE)
    for tag in soup.find_all(text=hour_pattern):
        h = hour_pattern.search(tag)
        if h:
            hours.append(h.group(1))
    hours = "\n".join(hours) if hours else "ì •ë³´ ì—†ìŒ"

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì „í™”ë²ˆí˜¸ (í˜„ì¬ ì—†ìŒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    phone = ""

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ë‰´ í•­ëª© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    menu_items = re.findall(r"[Ğ-Ğ¯Ğ°-Ñ\w\s]+? Ğ·Ğ° \d+Ñ€", html_content)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¦¬ë·° (strong, blockquote ê¸°ë°˜) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    reviews = [t.get_text(strip=True) for t in BeautifulSoup(html_content, "html.parser").find_all(["strong", "blockquote"])]
    reviews = reviews[:3]

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì´ë¯¸ì§€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    images = []
    post_section = soup.select_one("div.text") or soup
    for img in post_section.select("img[src]"):
        src = img.get("src")
        if src and not src.startswith("data:"):
            images.append(urljoin(url, src))

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì§€ë„ URL (ì–€ë±ìŠ¤ iframeë§Œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    iframe = soup.select_one("iframe[src*='yandex']")
    map_url = iframe["src"] if iframe else ""

    return {
        "title": title,
        "html": html_content,
        "address": address,
        "hours": hours,
        "phone": phone,
        "menu_items": menu_items,
        "reviews": reviews,
        "images": images,
        "map_url": map_url,
        "source_url": url
    }

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    sample = "https://koko.by/cafehouse/13610-tako-burrito"
    from pprint import pprint
    pprint(parse_post(sample))

if __name__ == "__main__":
    sample = "https://koko.by/cafehouse/13610-tako-burrito"
    from pprint import pprint
    pprint(parse_post(sample))
