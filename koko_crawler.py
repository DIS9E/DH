# koko_crawler.py

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time

BASE_PAGE = "https://koko.by/category/cafehouse"
LOAD_MORE = "https://koko.by/load-more"
HEADERS = {
    "User-Agent": "Mozilla/5.0"
}

def crawl_all_posts(delay=1.0):
    session = requests.Session()
    session.headers.update(HEADERS)

    # 1) ì²« í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
    print("ğŸ” ì²« í˜ì´ì§€ ë¡œë“œ:", BASE_PAGE)
    res = session.get(BASE_PAGE)
    res.raise_for_status()
    soup = BeautifulSoup(res.text, "html.parser")

    # CSRF í† í° ì¶”ì¶œ
    csrf = soup.select_one("meta[name='csrf-token']")["content"]

    # ì´ˆê¸° ê²Œì‹œê¸€ ìˆ˜ì§‘ (js-post-item í•­ëª©ë“¤)
    posts = []
    for a in soup.select("div.w-post-name a.name__link"):
        href = a["href"].strip()
        title = a.get_text(strip=True)
        posts.append({"title": title, "url": urljoin(BASE_PAGE, href)})

    # 2) AJAXë¥¼ í†µí•œ ì¶”ê°€ ë¡œë“œ
    while True:
        offset = len(posts)
        print(f"ğŸ” AJAX ë¡œë“œ â€“ offset={offset}")

        data = {
            "offset": offset,
            "url": "/category/cafehouse"
        }
        headers = {
            "X-CSRF-Token": csrf,
            "Referer": BASE_PAGE
        }
        ajax = session.post(LOAD_MORE, data=data, headers=headers)
        ajax.raise_for_status()

        # ì„œë²„ê°€ ë°˜í™˜í•˜ëŠ” JSONì— HTML ë©ì–´ë¦¬ê°€ ìˆì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ í™•ì¸
        payload = ajax.json() if ajax.headers.get("Content-Type","").startswith("application/json") else {}
        html = payload.get("content", ajax.text)

        snippet = BeautifulSoup(html, "html.parser")
        new_items = snippet.select("div.w-post-name a.name__link")

        if not new_items:
            print("âœ… ë” ì´ìƒ ê²Œì‹œê¸€ ì—†ìŒ. ì¢…ë£Œ.")
            break

        for a in new_items:
            href = a["href"].strip()
            title = a.get_text(strip=True)
            full = urljoin(BASE_PAGE, href)
            if full not in {p["url"] for p in posts}:
                posts.append({"title": title, "url": full})

        time.sleep(delay)

    print(f"ğŸ”— ì´ {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ë¨")
    return posts

if __name__ == "__main__":
    all_posts = crawl_all_posts()
    for p in all_posts:
        print("-", p["title"], "â†’", p["url"])

        time.sleep(delay)

    print(f"ğŸ”— ì´ {len(results)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ë¨")
    return results
