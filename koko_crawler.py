# koko_crawler.py

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time

BASE_URL = "https://koko.by/category/cafehouse"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

def crawl_cafehouse_pages(base_url=BASE_URL, max_pages=50, delay=1.0):
    results = []
    page = 1

    while True:
        print(f"ğŸ” í¬ë¡¤ë§ ì¤‘: í˜ì´ì§€ {page}")
        url = f"{base_url}/page/{page}/" if page > 1 else base_url
        res = requests.get(url, headers=HEADERS)

        if res.status_code != 200:
            print(f"â›”ï¸ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {res.status_code}")
            break

        soup = BeautifulSoup(res.text, "html.parser")
        articles = soup.select("article")

        if not articles:
            print("âœ… ë” ì´ìƒ ê²Œì‹œê¸€ ì—†ìŒ. ì¢…ë£Œ.")
            break

        for article in articles:
            a_tag = article.select_one("a")
            title = a_tag.get("title", "").strip()
            link = a_tag.get("href", "").strip()
            if title and link:
                results.append({"title": title, "url": link})

        page += 1
        if page > max_pages:
            print("ğŸ“› ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ë„ë‹¬")
            break

        time.sleep(delay)  # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€ìš© ë”œë ˆì´

    return results

if __name__ == "__main__":
    posts = crawl_cafehouse_pages()
    print(f"ğŸ”— ì´ {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ë¨")
    for p in posts[:3]:  # ìƒ˜í”Œ ì¶œë ¥
        print(f"- {p['title']} â†’ {p['url']}")
