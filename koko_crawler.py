# koko_crawler.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
koko.by/category/cafehouse í¬ë¡¤ëŸ¬
â€¢ ì²« í˜ì´ì§€ì—ì„œë§Œ ëª¨ë“  ê²Œì‹œê¸€ ë§í¬+ì œëª© ìˆ˜ì§‘
â€¢ '/category/cafehouse/' íŒ¨í„´ìœ¼ë¡œ í•„í„°ë§
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time

BASE_URL = "https://koko.by/category/cafehouse"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

def crawl_cafehouse():
    print("ğŸ” í¬ë¡¤ë§ ì‹œì‘:", BASE_URL)
    res = requests.get(BASE_URL, headers=HEADERS)
    if res.status_code != 200:
        print(f"â›”ï¸ ìš”ì²­ ì‹¤íŒ¨: {res.status_code}")
        return []

    soup = BeautifulSoup(res.text, "html.parser")
    links = soup.select("a[href*='/category/cafehouse/']")
    results = []
    seen = set()

    for a in links:
        href = a.get("href", "").strip()
        title = a.get_text(strip=True)
        # ë³¸ë¬¸ ë¦¬ìŠ¤íŠ¸ ìƒë‹¨ì˜ â€œ/category/cafehouseâ€ ìì²´ ë§í¬ëŠ” ê±´ë„ˆë›°ê¸°
        if not title or href.rstrip("/") == BASE_URL.rstrip("/"):
            continue
        url = urljoin(BASE_URL, href)
        if url in seen:
            continue
        seen.add(url)
        results.append({"title": title, "url": url})

    print(f"ğŸ”— ì´ {len(results)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ë¨")
    return results

if __name__ == "__main__":
    posts = crawl_cafehouse()
    for p in posts:
        print(f"- {p['title']} â†’ {p['url']}")
