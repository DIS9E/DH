# koko_crawler.py

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time

BASE_URL = "https://koko.by/category/cafehouse"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

def crawl_cafehouse_pages(max_pages=10, delay=1.0):
    """
    ì¸í”¼ë‹ˆíŠ¸ ìŠ¤í¬ë¡¤ ë°©ì‹ ëŒ€ì‘:
    - ì²« í˜ì´ì§€: BASE_URL
    - 2í˜ì´ì§€ë¶€í„°: BASE_URL + '?page=N'
    """
    results = []
    seen = set()
    for page in range(1, max_pages+1):
        if page == 1:
            url = BASE_URL
        else:
            url = f"{BASE_URL}?page={page}"
        print(f"ğŸ” í¬ë¡¤ë§ ì¤‘: {url}")
        res = requests.get(url, headers=HEADERS)
        if res.status_code != 200:
            print(f"â›”ï¸ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {res.status_code}")
            break

        soup = BeautifulSoup(res.text, "html.parser")
        items = soup.select("a[href*='/category/cafehouse/']")
        new_count = 0
        for a in items:
            href = a.get("href").strip()
            title = a.get_text(strip=True)
            if not title or href.rstrip("/") == BASE_URL.rstrip("/"):
                continue
            full_url = urljoin(BASE_URL, href)
            if full_url in seen:
                continue
            seen.add(full_url)
            results.append({"title": title, "url": full_url})
            new_count += 1

        if new_count == 0:
            print("âœ… ë” ì´ìƒ ìƒˆë¡œìš´ ê²Œì‹œê¸€ì´ ì—†ìŒ. ì¢…ë£Œ.")
            break

        time.sleep(delay)

    print(f"ğŸ”— ì´ {len(results)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ë¨")
    return results

if __name__ == "__main__":
    posts = crawl_cafehouse_pages()
    for p in posts:
        print(f"- {p['title']} â†’ {p['url']}")
