#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
udf.name.py â€“ v3.9-stable c  (2025-07-15)
â€¢ ì´ëª¨ì§€ ê³ ì •/í”Œë ˆì´ìŠ¤í™€ë”/ì œëª© ì¤‘ë³µ ì œê±°
â€¢ ì„¹ì…˜ 500ìâ†‘ í™•ì¥ ì•ˆì •í™” + í—¤ë“œë¼ì´íŠ¸ í†¤
â€¢ ì¶œì²˜ ë§í¬ + by. ì—ë””í„° LEEğŸŒ³ ìë™ ì‚½ì…
â€¢ ì›Œë“œí”„ë ˆìŠ¤ í•‘ë°± ì°¨ë‹¨ + DEBUG ë¡œê·¸
"""

import os, sys, re, json, time, logging
from urllib.parse import urljoin, urlparse, urlunparse
import requests
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€
WP_URL   = os.getenv("WP_URL", "https://belatri.info").rstrip("/")
USER     = os.getenv("WP_USERNAME")
APP_PW   = os.getenv("WP_APP_PASSWORD")
OPEN_KEY = os.getenv("OPENAI_API_KEY")
if not all([USER, APP_PW, OPEN_KEY]):
    sys.exit("âŒ WP_USERNAME / WP_APP_PASSWORD / OPENAI_API_KEY ëˆ„ë½")

POSTS_API = f"{WP_URL}/wp-json/wp/v2/posts"
TAGS_API  = f"{WP_URL}/wp-json/wp/v2/tags"
UDF_BASE  = "https://udf.name/news/"
HEADERS   = {"User-Agent": "UDFCrawler/3.9-stable-c"}
SEEN_FILE = "seen_urls.json"
TARGET_CAT_ID = 20
norm = lambda u: urlunparse(urlparse(u)._replace(query="", params="", fragment=""))

# â”€â”€â”€â”€â”€â”€ seen â”€â”€â”€â”€â”€â”€
def load_seen():
    return set(json.load(open(SEEN_FILE))) if os.path.exists(SEEN_FILE) else set()
def save_seen(s):
    json.dump(list(s), open(SEEN_FILE, "w"), ensure_ascii=False, indent=2)
def wp_exists(u_norm):
    return bool(requests.get(POSTS_API,
        params={"search": u_norm, "per_page": 1},
        auth=(USER, APP_PW), timeout=10).json())
def sync_seen(seen):
    kept = {u for u in seen if wp_exists(norm(u))}
    if kept != seen: save_seen(kept)
    return kept

# â”€â”€â”€â”€â”€â”€ í¬ë¡¤ & íŒŒì‹± â”€â”€â”€â”€â”€â”€
def fetch_links():
    html = requests.get(UDF_BASE, headers=HEADERS, timeout=10).text
    soup = BeautifulSoup(html, "html.parser")
    return list({norm(urljoin(UDF_BASE, a["href"]))
        for a in soup.select("div.article1 div.article_title_news a[href]")})

def parse(url):
    r = requests.get(url, headers=HEADERS, timeout=10); r.raise_for_status()
    s = BeautifulSoup(r.text, "html.parser")
    h = s.find("h1", class_="newtitle")
    b = s.find("div", id="zooming")
    if not (h and b): return None
    img = s.find("img", class_="lazy") or s.find("img")
    src = (img.get("data-src") or img.get("src")) if img else None
    if src and any(x in src for x in ("placeholder", "default")):
        src = None
    cat = url.split("/news/")[1].split("/")[0]
    return dict(title=h.get_text(strip=True), html=str(b),
                image=urljoin(url, src) if src else None,
                url=url, cat=cat)

# â”€â”€â”€â”€â”€â”€ GPT í—¬í¼ â”€â”€â”€â”€â”€â”€
def chat(sys_p, user_p, max_tok=1800, temp=0.5, model="gpt-4o"):
    h = {"Authorization": f"Bearer {OPEN_KEY}",
         "Content-Type":  "application/json"}
    msgs = [{"role": "system", "content": sys_p},
            {"role": "user",   "content": user_p}]
    r = requests.post("https://api.openai.com/v1/chat/completions",
        headers=h, json={"model": model, "messages": msgs,
                         "temperature": temp, "max_tokens": max_tok},
        timeout=120)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"].strip()

SYS = ("ë‹¹ì‹ ì€ â€˜í—¤ë“œë¼ì´íŠ¸â€™ ë‰´ìŠ¤ë ˆí„° ìŠ¤íƒ€ì¼ì˜ í•œêµ­ì–´ ê¸°ìì…ë‹ˆë‹¤. "
       "ì¹œê·¼í•œ ì¡´ëŒ“ë§Â·ì§ˆë¬¸Â·ê°íƒ„ì„ ì ì ˆíˆ ì„ì–´ ì“°ë˜, ê°™ì€ ì´ëª¨ì§€ë¥¼ "
       "ê¸°ì‚¬ë§ˆë‹¤ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.")

STYLE_GUIDE = """
<h1>(ì´ëª¨ì§€ 1â€“3ê°œ) í¥ë¯¸ë¡œìš´ í•œêµ­ì–´ ì œëª©</h1>
<h2>âœï¸ í¸ì§‘ì ì£¼ â€” ê¸°ì‚¬ í•µì‹¬ì„ 2ë¬¸ì¥</h2>
<h3>ğŸ“Š ìµœì‹  ë°ì´í„°</h3>
<p>(extra_context ìˆ«ìÂ·ë§í¬, <strong>500ì ì´ìƒ</strong>)</p>
<h3>ğŸ’¬ ì „ë¬¸ê°€ ì „ë§</h3>
<p>(ì‹œë‚˜ë¦¬ì˜¤Â·ìˆ«ìÂ·ê¸°ê´€ ì¸ìš©, <strong>500ì ì´ìƒ</strong>)</p>
<h3>â“ Q&A</h3>
<ul><li>Q1â€¦?<br><strong>A.</strong> â€¦</li>
<li>Q2â€¦?<br><strong>A.</strong> â€¦</li>
<li>Q3â€¦?<br><strong>A.</strong> â€¦</li></ul>
<h3>(ë³¸ë¬¸ í•´ì„¤)</h3>
<p>ì›ë¬¸ ë¬¸ì¥ ëª¨ë‘ ìì—°ìŠ¤ëŸ½ê²Œ ì¬ë°°ì¹˜â€¦</p>
<p>ğŸ·ï¸ íƒœê·¸: ëª…ì‚¬ 3â€“6ê°œ</p>
<p>ì¶œì²˜: UDF.name ì›ë¬¸<br>by. ì—ë””í„° LEEğŸŒ³</p>
"""

PLACE_RGX = re.compile(r"(ê¸°ì‚¬ í•µì‹¬.*?2ë¬¸ì¥|í¥ë¯¸ë¡œìš´ í•œêµ­ì–´ ì œëª©:|extra_context.+?strong>|"
                       r"ë¬¸ë‹¨ì„.+?í™•ì¥.*?|ì–´ë–¤ ì£¼ì œì—.*ì•Œë ¤ì£¼ì„¸ìš”)!?", re.I)

def rewrite(art):
    prompt = f"{STYLE_GUIDE}\n\nâ—† ì›ë¬¸:\n{art['html']}\n\n" \
             f"â—† extra_context:\nâ€¢ í—¤ë“œë¼ì¸ í‚¤ì›Œë“œ: {art['title'][:60]}"
    txt = chat(SYS, prompt)
    txt = re.sub(r"<h1>.*?</h1>", "", txt, flags=re.S)  # GPT h1 ì œê±°
    logging.debug("GPT raw >>> %s â€¦", txt[:300].replace('\n', ' '))
    return txt

# â”€â”€â”€â”€â”€â”€ í™•ì¥Â·ì •ë¦¬ â”€â”€â”€â”€â”€â”€
def ensure_long(html, title):
    soup = BeautifulSoup(html, "html.parser")
    for t in soup.find_all(string=True):
        if title.strip() == t.strip():
            t.extract()
    for blk in soup.find_all(["p", "ul"]):
        if len(blk.get_text()) < 500:
            try:
                expanded = chat(
                    SYS,
                    f"<ë¬¸ë‹¨>{blk.get_text()}</ë¬¸ë‹¨>\n\nìœ„ ë¬¸ë‹¨ì„ "
                    "ê·¼ê±°Â·ìˆ«ìÂ·ì „ë§ í¬í•¨ 500ì ì´ìƒìœ¼ë¡œ í™•ì¥.",
                    max_tok=400, temp=0.7, model="gpt-4o-mini")
                blk.clear()
                blk.append(BeautifulSoup(expanded, "html.parser"))
            except Exception as e:
                logging.debug("í™•ì¥ ì‹¤íŒ¨: %s", e)
    html = PLACE_RGX.sub("", str(soup))
    return re.sub(r"\s{2,}", " ", html)

# â”€â”€â”€â”€â”€â”€ íƒœê·¸ & ì œëª© â”€â”€â”€â”€â”€â”€
CYRILLIC = re.compile(r"[Ğ-Ğ¯Ğ°-ÑĞÑ‘]")
def korean_title(src, ctx):
    if not CYRILLIC.search(src):
        return src
    return chat(
        SYS,
        f"ë‹¤ìŒ ëŸ¬ì‹œì•„ì–´ ì œëª©ì„ 45ìâ†“ í•œêµ­ì–´ ì¹´í”¼ë¼ì´í„° ìŠ¤íƒ€ì¼ "
        f"+ ì´ëª¨ì§€ 1â€“3ê°œë¡œ:\nÂ«{src}Â»\në¬¸ë§¥:{ctx[:200]}",
        max_tok=60, temp=0.9, model="gpt-4o-mini")

STOP = {"ë²¨ë¼ë£¨ìŠ¤", "ë‰´ìŠ¤", "ê¸°ì‚¬"}
def tag_names(txt):
    m = re.search(r"ğŸ·ï¸.*[:ï¼š]\s*(.+)", txt)
    out = []
    if m:
        for t in re.split(r"[,\s]+", m.group(1)):
            t = t.strip("â€“-#â€¢")
            if 1 < len(t) <= 20 and t not in STOP and t not in out:
                out.append(t)
    return out[:6]

def tag_id(name):
    q = requests.get(TAGS_API, params={"search": name, "per_page": 1},
                     auth=(USER, APP_PW), timeout=10).json()
    if q: return q[0]["id"]
    c = requests.post(TAGS_API, json={"name": name},
                      auth=(USER, APP_PW), timeout=10)
    return c.json()["id"] if c.status_code == 201 else None

# â”€â”€â”€â”€â”€â”€ ê²Œì‹œ â”€â”€â”€â”€â”€â”€
def publish(art, txt, tag_ids):
    soup = BeautifulSoup(txt, "html.parser")
    kor_title = korean_title(art["title"], soup.get_text(" ", strip=True))
    body = ensure_long(str(soup), kor_title)
    soup = BeautifulSoup(body, "html.parser")

    h1 = soup.new_tag("h1"); h1.string = kor_title
    soup.insert(0, h1)

    if not soup.find(string=re.compile("by\\. ì—ë””í„°")):
        footer = (f'<p>ì¶œì²˜: <a href="{art["url"]}">UDF.name ì›ë¬¸</a>'
                  "<br>by. ì—ë””í„° LEEğŸŒ³</p>")
        soup.append(BeautifulSoup(footer, "html.parser"))

    hidden = f"<a href='{art['url']}' style='display:none'>src</a>\n"
    img    = f"<p><img src='{art['image']}' alt=''></p>\n" if art["image"] else ""

    payload = {"title": kor_title, "content": hidden + img + str(soup),
               "status": "publish", "categories": [TARGET_CAT_ID],
               "tags": tag_ids, "ping_status": "closed"}
    r = requests.post(POSTS_API, json=payload,
                      auth=(USER, APP_PW), timeout=30); r.raise_for_status()
    logging.info("â†³ ê²Œì‹œ %s %s", r.status_code, r.json().get("id"))

# â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€
def main():
    logging.basicConfig(level=logging.DEBUG,
        format="%(asctime)s â”‚ %(levelname)s â”‚ %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S")
    seen = sync_seen(load_seen())
    links = fetch_links()
    todo = [u for u in links if norm(u) not in seen and not wp_exists(norm(u))]
    logging.info("ğŸ“° ìƒˆ ê¸°ì‚¬ %d / ì´ %d", len(todo), len(links))

    for url in todo:
        logging.info("â–¶ %s", url)
        art = parse(url); time.sleep(1)
        if not art: continue
        try:
            txt = rewrite(art)
        except Exception as e:
            logging.warning("GPT ì˜¤ë¥˜: %s", e); continue

        tag_ids = [tid for n in tag_names(txt) if (tid := tag_id(n))]
        try:
            publish(art, txt, tag_ids)
            seen.add(norm(url)); save_seen(seen)
        except Exception as e:
            logging.warning("ì—…ë¡œë“œ ì‹¤íŒ¨: %s", e)
        time.sleep(1.5)

if __name__ == "__main__":
    main()
