#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
udf.name.py â€“ v1.1 
â€¢ ì›ë¬¸ 100 % ìœ ì§€ + ì¹´í…Œê³ ë¦¬ë³„ ì™¸ë¶€ ë°ì´í„° ì‚½ì…
â€¢ Q&A ë‹µë³€Â·ë‚´ë¶€ ë§í¬Â·ì¶œì²˜ ì•µì»¤Â·ì´ë¯¸ì§€ ìº¡ì…˜ ìë™ ë³´ê°•
â€¢ ì œëª© í•œêµ­ì–´ ë³€í™˜ Â· ì¤‘ë³µ í—¤ë” ì œê±° Â· placeholder ì´ë¯¸ì§€ í•„í„°
"""

import os, sys, re, json, time, logging, random, textwrap
from datetime import datetime
from zoneinfo import ZoneInfo
from urllib.parse import urljoin, urlparse, urlunparse
import xml.etree.ElementTree as ET
import requests
import feedparser
from bs4 import BeautifulSoup
from requests.exceptions import RequestException

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WP_URL      = os.getenv("WP_URL", "https://belatri.info").rstrip("/")
USER        = os.getenv("WP_USERNAME")
APP_PW      = os.getenv("WP_APP_PASSWORD")
OPEN_KEY    = os.getenv("OPENAI_API_KEY")
if not all([USER, APP_PW, OPEN_KEY]):
    sys.exit("âŒ  WP_USERNAME / WP_APP_PASSWORD / OPENAI_API_KEY ëˆ„ë½")

POSTS_API   = f"{WP_URL}/wp-json/wp/v2/posts"
TAGS_API    = f"{WP_URL}/wp-json/wp/v2/tags"
UDF_BASE    = "https://udf.name/news/"
HEADERS     = {"User-Agent": "UDFCrawler/3.8"}
SEEN_FILE   = "seen_urls.json"
TARGET_CAT_ID = 20

norm = lambda u: urlunparse(urlparse(u)._replace(query="", params="", fragment=""))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ seen ê´€ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_seen():
    return set(json.load(open(SEEN_FILE))) if os.path.exists(SEEN_FILE) else set()
def save_seen(s):
    json.dump(list(s), open(SEEN_FILE, "w"), ensure_ascii=False, indent=2)

def wp_exists(u):
    r = requests.get(POSTS_API, params={"search":u,"per_page":1},
                     auth=(USER,APP_PW), timeout=10)
    return r.ok and bool(r.json())

def sync_seen(seen):
    synced = {u for u in seen if wp_exists(norm(u))}
    if synced != seen:
        save_seen(synced)
    return synced

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë§í¬ í¬ë¡¤ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_links():
    try:
        resp = requests.get(UDF_BASE, headers=HEADERS, timeout=15)  # íƒ€ì„ì•„ì›ƒ 15ì´ˆë¡œ ì—°ì¥
        resp.raise_for_status()
        html = resp.text
    except RequestException as e:
        logging.warning("ë§í¬ í¬ë¡¤ë§ ì‹¤íŒ¨: %s", e)
        return []  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    soup = BeautifulSoup(html, "html.parser")
    return list({
        norm(urljoin(UDF_BASE, a["href"]))
        for a in soup.select("div.article1 div.article_title_news a[href]")
    })

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ì‚¬ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse(url):
    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        r.raise_for_status()
    except RequestException as e:
        logging.warning("íŒŒì‹± ì‹¤íŒ¨(%s): %s", url, e)
        return None

    s = BeautifulSoup(r.text, "html.parser")
    t = s.find("h1", class_="newtitle")
    b = s.find("div", id="zooming")
    if not (t and b):
        return None

    img = s.find("img", class_="lazy") or s.find("img")
    src = None
    if img:
        src = img.get("data-src") or img.get("src")
        if src and ("placeholder" in src or "default" in src):
            src = None
    img_url = urljoin(url, src) if src else None
    cat = url.split("/news/")[1].split("/")[0]
    return {
        "title": t.get_text(strip=True),
        "html":  str(b),
        "image": img_url,
        "url":   url,
        "cat":   cat
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_brief(cat: str, headline: str) -> str:
    snippets = []

    # 1) BYN ê¸°ì¤€ìœ¼ë¡œ ê° í†µí™” í•œ ë²ˆì— ë¶ˆëŸ¬ì˜¤ê¸°
    try:
        resp = requests.get(
            "https://api.exchangerate.host/latest",
            params={"base": "BYN", "symbols": "USD,EUR,KRW"},
            timeout=10
        )
        resp.raise_for_status()
        rates = resp.json().get("rates", {})

        usd = rates.get("USD")
        eur = rates.get("EUR")
        krw = rates.get("KRW")

        if usd is not None:
            snippets.append(f" ğŸ‡ºğŸ‡¸ 1ë‹¬ëŸ¬ = {1/usd:.4f} BYN")
        if eur is not None:
            snippets.append(f" ğŸ‡ªğŸ‡º 1ìœ ë¡œ = {1/eur:.4f} BYN")
        if krw is not None:
            snippets.append(f" ğŸ‡°ğŸ‡· 1,000ì› = {1000/krw:.4f} BYN")
    except Exception:
        # í™˜ìœ¨ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ, ì•„ë¬´ í•­ëª©ë„ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        pass

    # 2) BBC World í—¤ë“œë¼ì¸ 1ê±´
    if cat != "economic":
        try:
            dp = feedparser.parse("https://feeds.bbci.co.uk/news/world/rss.xml")
            title = dp.entries[0].title.strip()
            snippets.append(f" ğŸ‡¬ğŸ‡§ BBC í—¤ë“œë¼ì¸: {title}")
        except Exception:
            snippets.append(" ğŸ‡¬ğŸ‡§ BBC í—¤ë“œë¼ì¸: ë°ì´í„° ì—†ìŒ")

    # 3) ì£¼ìš” í‚¤ì›Œë“œ
    snippets.append(f" ğŸŒ ì£¼ìš” í‚¤ì›Œë“œ: {headline.strip()[:60]}")

    # <li> íƒœê·¸ë¡œ ê°ì‹¸ì„œ ë°˜í™˜
    return "\n".join(f"<li>{s}</li>" for s in snippets)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STYLE_GUIDE = textwrap.dedent("""
<h1>{title}</h1>
<small>UDF â€¢ {date} â€¢ ì½ìŒ {views:,}</small>

<h3>ğŸ’¡ ë³¸ë¬¸ ì •ë¦¬</h3>
<p>âŸªRAW_HTMLâŸ«</p>

<h2>âœï¸ í¸ì§‘ì ì£¼ â€” ì´ ê¸°ì‚¬, ì´ë ‡ê²Œ ì½ì–´ìš”</h2>
<!-- ì•„ë˜ í•œ ë‹¨ë½ì— ê¸°ì‚¬ í•µì‹¬ì„ â€˜ê¸´ ë¬¸ì¥â€™ 2ê°œë¡œ ì‘ì„±í•˜ì„¸ìš” -->
<p></p>

<h3>ğŸ“ ê°œìš”</h3>
<p>ì›ë¬¸ì„ 100% ì¬ë°°ì¹˜í•˜ê³ , ì¶”ê°€ ì¡°ì‚¬Â·ë¶„ì„ì„ ë”í•´ 500ì ì´ìƒ í’ë¶€í•˜ê²Œ ê¸°ìˆ í•˜ì„¸ìš”.</p>

[gpt_latest_data]

<h3>ğŸ’¬ ì „ë¬¸ê°€ ì „ë§</h3>
<p>ì²« ë²ˆì§¸ ë‹¨ë½: êµ¬ì²´ì  ê·¼ê±°Â·ìˆ«ì í¬í•¨ 4ë¬¸ì¥ ì´ìƒ</p>
<p>ë‘ ë²ˆì§¸ ë‹¨ë½: ì‹œë‚˜ë¦¬ì˜¤Â·ì „ë§ í¬í•¨ 4ë¬¸ì¥ ì´ìƒ</p>

[gpt_related_qna]

<p>ğŸ·ï¸ íƒœê·¸: {tags}</p>
<p>ì¶œì²˜: UDF.name ì›ë¬¸<br>
   Photo: UDF.name<br>
   by. LEEğŸŒ³<br>
   <em>* ìƒì„±í˜• AIì˜ ë„ì›€ìœ¼ë¡œ ì‘ì„±.</em></p>

<p class="related"></p>
""").strip()

# â”€â”€â”€ GPT ë¦¬ë¼ì´íŒ… (ì •ì±… ì•ˆì „ + ë©”íƒ€ë°ì´í„° ì‚½ì…) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rewrite(article):
    extra            = build_brief(article['cat'], article['title'])
    today            = datetime.now(tz=ZoneInfo("Asia/Seoul")).strftime("%Y.%m.%d")
    views            = random.randint(7_000, 12_000)
    tags_placeholder = ""

    # 1) META_DATA ë¦¬ìŠ¤íŠ¸ í•­ëª© ìƒì„±
    meta_items = "\n".join(f"<li>{line}</li>" for line in extra.split("\n"))

    # 2) STYLE_GUIDEì˜ í”Œë ˆì´ìŠ¤í™€ë”({emoji},{title} ë“±)ë§Œ ë¨¼ì € ì±„ì›Œì„œ 'filled'ì— ë‹´ê¸°
    filled = STYLE_GUIDE.format(
        emoji="ğŸ“°",
        title=article["title"],
        date=today,
        views=views,
        tags=tags_placeholder
    )

    # 3) RAW_HTMLÂ·META_DATA í”Œë ˆì´ìŠ¤í™€ë” ì¹˜í™˜ ë° ì›ë¬¸/extra_context ë§ë¶™ì´ê¸°
    prompt_body = (
        filled
        .replace("âŸªRAW_HTMLâŸ«", article["html"])
        .replace("âŸªMETA_DATAâŸ«", meta_items)
        + f"""

ì›ë¬¸:
{article["html"]}

extra_context:
{extra}
"""
    )

    # â”€â”€â”€ GPT í˜¸ì¶œ ì¤€ë¹„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    messages = [
        {
            "role": "system",
            "content": (
                "ë‹¹ì‹ ì€ â€˜í—¤ë“œë¼ì´íŠ¸â€™ ë‰´ìŠ¤ë ˆí„°ì˜ í†¤ê³¼ ë¬¸ì²´ë¥¼ 100% ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.\n"
                "â€“ ì¹œê·¼í•œ ëŒ€í™”ì²´ë¡œ, ë¬¸ì¥ë§ˆë‹¤ â€˜~ìš”â€™, â€˜~ì£ â€™, â€˜~ë„¤ìš”?â€™ ê°™ì€ ì¢…ê²°ì–´ë¯¸ë¥¼ ê¼­ ë„£ê³ , â€œ?â€ì™€ â€œ!â€ë¥¼ ì„ì–´ ì§ˆë¬¸ê³¼ ê°íƒ„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©í•˜ì„¸ìš”.\n"
                "â€“ ë¬µì§í•œ ì„¤ëª…ë¬¸ì²´ ëŒ€ì‹ , ë…ìì—ê²Œ ë§ì„ ê±´ë„¤ë“¯ ìƒë™ê° ìˆê²Œ ì¨ì•¼ í•©ë‹ˆë‹¤.\n"
                "â€“ ë¬´ë¡€í•˜ê±°ë‚˜ ë¶€ì ì ˆí•œ í‘œí˜„ì€ ì ˆëŒ€ ì“°ì§€ ë§ˆì„¸ìš”.\n"
                "â€“ ì •ì±…ì— ë¯¼ê°í•œ ë‹¨ì–´ë‚˜ ë¶€ì ì ˆí•œ í‘œí˜„ë„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n\n"
                "**ğŸ“Š ìµœì‹  ë°ì´í„° ì„¹ì…˜ì€ ìˆì½”ë“œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.**\n"
                "**â“ Q&A ì„¹ì…˜ì€ ìˆì½”ë“œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.**\n"
                "`[gpt_related_qna]`\n\n"
                "**â€» ë°˜ë“œì‹œ STYLE_GUIDE ìˆœì„œëŒ€ë¡œ ì•„ë˜ í—¤ë” ë¸”ë¡ì„ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.**\n"
                "    - `<h1>â€¦</h1>`\n"
                "    - `<small>â€¦</small>`\n"
                "    - `<h3>ğŸ’¡ ë³¸ë¬¸ ì •ë¦¬</h3>`\n"
                "    - `<h2>âœï¸ í¸ì§‘ì ì£¼ â€¦</h2>`\n"
                "    - `<h3>ğŸ“ ê°œìš”</h3>`\n"
                "    - `<h3>ğŸ“Š ìµœì‹  ë°ì´í„°</h3>` + `<ul>â€¦</ul>`\n"
                "    - `<h3>ğŸ’¬ ì „ë¬¸ê°€ ì „ë§</h3>`\n"
                "    - `[gpt_related_qna]`\n"
                "    - `<p>ğŸ·ï¸ íƒœê·¸: â€¦</p>`\n"
                "    - `<p>ì¶œì²˜: â€¦</p>`\n"
                "    - `<p class=\"related\"></p>`"
            )
        },
        {
            "role": "user",
            "content": prompt_body
        }
    ]

    headers = {
        "Authorization": f"Bearer {OPEN_KEY}",
        "Content-Type":  "application/json"
    }

    data = {
        "model":       "gpt-4o",
        "messages":    messages,
        "temperature": 0.4,
        "max_tokens":  1800
    }

    # 4) ì²« ìš”ì²­
    r = requests.post(
        "https://api.openai.com/v1/chat/completions",
        headers=headers,
        json=data,
        timeout=90
    )
    r.raise_for_status()
    txt = r.json()["choices"][0]["message"]["content"].strip().replace("**", "")

    # 5) ê¸¸ì´ ë³´ê°•
    if len(txt) < 1500:
        logging.info("  â†º ê¸¸ì´ ë³´ê°• ì¬-ìš”ì²­")
        data["temperature"] = 0.6
        r2 = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=90
        )
        r2.raise_for_status()
        txt = r2.json()["choices"][0]["message"]["content"].strip().replace("**", "")

    return txt
    
# â”€â”€â”€ ê¸°íƒ€ ìœ í‹¸ ë° ê²Œì‹œ ë¡œì§ (ë³€ê²½ ì—†ìŒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CYRILLIC = re.compile(r"[Ğ-Ğ¯Ğ°-ÑĞÑ‘]")

def korean_title(src: str, context: str) -> str:
    if not CYRILLIC.search(src):
        return src
    prompt = (
        "ê¸°ì‚¬ ë‚´ìš©ì„ ì°¸ê³ í•´ ì¹œê·¼í•œ ëŒ€í™”ì²´ë¡œ, ë…ìì˜ í˜¸ê¸°ì‹¬ì„ ëŒ "
        "45ì ì´ë‚´ í•œêµ­ì–´ ì œëª©ì„ ë§Œë“¤ê³  ì´ëª¨ì§€ 1â€“3ê°œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•˜ì„¸ìš”.\n\n"
        f"ì›ì œëª©: {src}\nê¸°ì‚¬ ì¼ë¶€: {context[:300]}"
    )
    headers = {"Authorization":f"Bearer {OPEN_KEY}", "Content-Type":"application/json"}
    data = {"model":"gpt-4o-mini","messages":[{"role":"user","content":prompt}],
            "temperature":0.8,"max_tokens":60}
    try:
        r = requests.post("https://api.openai.com/v1/chat/completions",
                          headers=headers, json=data, timeout=20)
        r.raise_for_status()
        return r.json()["choices"][0]["message"]["content"].strip()
    except:
        return src

STOP = {"ë²¨ë¼ë£¨ìŠ¤","ë‰´ìŠ¤","ê¸°ì‚¬"}
def tag_names(txt: str) -> list[str]:
    m = re.search(r"ğŸ·ï¸\s*íƒœê·¸[^:]*[:ï¼š]\s*(.+)", txt)
    if not m: return []
    out=[]
    for t in re.split(r"[,\s]+", m.group(1)):
        t = t.strip("â€“-#â€¢")
        if 1<len(t)<=20 and t not in STOP and t not in out:
            out.append(t)
        if len(out)==6: break
    return out

def tag_id(name: str) -> int|None:
    q = requests.get(TAGS_API, params={"search":name,"per_page":1},
                     auth=(USER,APP_PW), timeout=10)
    if q.ok and q.json(): return q.json()[0]["id"]
    c = requests.post(TAGS_API, json={"name":name}, auth=(USER,APP_PW), timeout=10)
    return c.json().get("id") if c.status_code==201 else None

def ensure_depth(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    modified = False
    for li in soup.find_all("li"):
        txt = li.get_text()
        if "<strong>A." not in txt: continue
        if len(re.findall(r"[.!?]", txt)) < 2:
            prompt = f"ì•„ë˜ ë‹µë³€ì„ ê·¼ê±°Â·ìˆ«ìÂ·ì „ë§ í¬í•¨ 3ë¬¸ì¥ ì´ìƒìœ¼ë¡œ í™•ì¥:\n{txt}"
            headers={"Authorization":f"Bearer {OPEN_KEY}","Content-Type":"application/json"}
            data={"model":"gpt-4o-mini","messages":[{"role":"user","content":prompt}],
                  "temperature":0.7,"max_tokens":100}
            try:
                r = requests.post("https://api.openai.com/v1/chat/completions",
                                  headers=headers, json=data, timeout=20)
                r.raise_for_status()
                li.string = r.json()["choices"][0]["message"]["content"].strip()
                modified = True
            except:
                pass
    return str(soup) if modified else html

# â”€â”€â”€ ê²Œì‹œ ì „ í—¤ë” ë³€í™˜/í•„í„°ë§ & ê²Œì‹œ ë¡œì§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def publish(article: dict, txt: str, tag_ids: list[int]):
    # 1) Q&A ê¹Šì´ ë³´ê°• ìœ ì§€
    txt = ensure_depth(txt)

    # 2) ì›ë³¸ URL ìˆ¨ê¹€ + ëŒ€í‘œ ì´ë¯¸ì§€ íƒœê·¸
    hidden  = f'<a href="{article["url"]}" style="display:none">src</a>\n'
    img_tag = f'<p><img src="{article["image"]}" alt=""></p>\n' if article["image"] else ""

    # 3) Markdown í—¤ë”(#, ##, ###)ë¥¼ HTML <h1>-<h3>ë¡œ ë³€í™˜í•˜ê³ 
    #    ì½”ë“œë¸”ë¡, ê¸°ì¡´ ğŸ“° í—¤ë”, 'ì†Œì œëª©' ì£¼ì„ì€ ì œê±°
    lines = []
    for line in txt.splitlines():
        s = line.lstrip()

        # (ê°€) ì œê±°í•  íŒ¨í„´
        if s.startswith("```") or s.startswith("ğŸ“°") or "ì†Œì œëª©" in s:
            continue

        # (ë‚˜) Markdown í—¤ë” â†’ HTML í—¤ë”
        m = re.match(r'^(#{1,6})\s*(.*)$', s)
        if m:
            level   = min(len(m.group(1)), 3)       # ìµœëŒ€ h3
            content = m.group(2).strip()
            lines.append(f"<h{level}>{content}</h{level}>")
            continue

        # (ë‹¤) ì¼ë°˜ ë¬¸ì¥
        lines.append(line)

    # 4) BeautifulSoupìœ¼ë¡œ ë‹¤ì‹œ íŒŒì‹±
    soup = BeautifulSoup("\n".join(lines), "html.parser")

    # 5) ì œëª© ì¬ì‚½ì… (korean_title ë³€í™˜ í¬í•¨)
    h1   = soup.find("h1")
    orig = h1.get_text(strip=True) if h1 else article["title"]
    title= korean_title(orig, soup.get_text(" ", strip=True))
    if h1:
        h1.decompose()
    new_h1 = soup.new_tag("h1")
    new_h1.string = title
    soup.insert(0, new_h1)

    # 6) ì´ë¯¸ì§€ ìº¡ì…˜
    if img_tag:
        img = soup.find("img")
        if img and not img.find_next_sibling("em"):
            cap = soup.new_tag("em")
            cap.string = "Photo: UDF.name"
            img.insert_after(cap)

    # 7) ë‚´ë¶€ ê´€ë ¨ ê¸°ì‚¬ ë§í¬ ì‚½ì…
    if tag_ids:
        try:
            r = requests.get(
                POSTS_API,
                params={"tags": tag_ids[0], "per_page": 1},
                auth=(USER, APP_PW),
                timeout=10
            )
            if r.ok and r.json():
                link = r.json()[0]["link"]
                more = soup.new_tag("p")
                a    = soup.new_tag("a", href=link)
                a.string = "ğŸ“š ê´€ë ¨ ê¸°ì‚¬ ë” ë³´ê¸°"
                more.append(a)
                soup.append(more)
        except:
            pass

    # 8) ìµœì¢… ê²Œì‹œ (í•œ ë²ˆë§Œ í˜¸ì¶œ)
    body = hidden + img_tag + str(soup)
    payload = {
        "title":      title,
        "content":    body,
        "status":     "publish",
        "categories": [TARGET_CAT_ID],
        "tags":       tag_ids
    }
    r = requests.post(POSTS_API, json=payload, auth=(USER, APP_PW), timeout=30)
    logging.info("  â†³ ê²Œì‹œ %s %s", r.status_code, r.json().get("id"))
    r.raise_for_status()
def main():
    logging.basicConfig(
        level=logging.INFO,
        stream=sys.stdout,                        # â† STDERR ëŒ€ì‹  STDOUTìœ¼ë¡œ
        format="%(asctime)s â”‚ %(levelname)s â”‚ %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )

    seen  = sync_seen(load_seen())
    links = fetch_links()
    todo  = [u for u in links if norm(u) not in seen and not wp_exists(norm(u))]
    logging.info("ğŸ“° ìƒˆ ê¸°ì‚¬ %d / ì´ %d", len(todo), len(links))

    for url in todo:
        logging.info("â–¶ %s", url)
        art = parse(url)
        time.sleep(1)
        if not art:
            continue

        try:
            txt = rewrite(art)
        except Exception as e:
            logging.warning("GPT ì˜¤ë¥˜: %s", e)
            continue

        tag_ids = [tid for n in tag_names(txt) if (tid := tag_id(n))]
        try:
            publish(art, txt, tag_ids)
            seen.add(norm(url))
            save_seen(seen)
        except Exception as e:
            logging.warning("ì—…ë¡œë“œ ì‹¤íŒ¨: %s", e)

        time.sleep(1.5)

if __name__ == "__main__":
    main()
