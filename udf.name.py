#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
udf.name.py â€“ v4.0.1-full  (ë‰´ë‹‰ í†¤ + AdSense ê°€ì´ë“œ + RSS/í™˜ìœ¨ + ì•ˆì • íŒ¨ì¹˜)
"""

import os, sys, re, json, time, logging, html, random
from urllib.parse import urljoin, urlparse, urlunparse

import requests, feedparser          # â† requirements.txt ì— feedparser ì¶”ê°€
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WP_URL   = os.getenv("WP_URL" , "https://belatri.info").rstrip("/")
USER     = os.getenv("WP_USERNAME")
APP_PW   = os.getenv("WP_APP_PASSWORD")
OPEN_KEY = os.getenv("OPENAI_API_KEY")
if not all([USER, APP_PW, OPEN_KEY]):
    sys.exit("âŒ  WP_USERNAME / WP_APP_PASSWORD / OPENAI_API_KEY ëˆ„ë½")

POSTS_API = f"{WP_URL}/wp-json/wp/v2/posts"
TAGS_API  = f"{WP_URL}/wp-json/wp/v2/tags"

UDF_BASE   = "https://udf.name/news/"
HEADERS    = {"User-Agent": "UDFCrawler/4.0.1-full"}
SEEN_FILE  = "seen_urls.json"
TARGET_CAT_ID = 20                      # â€˜ë²¨ë¼ë£¨ìŠ¤ ë‰´ìŠ¤â€™ ì¹´í…Œê³ ë¦¬(ID)

norm = lambda u: urlunparse(urlparse(u)._replace(query="", params="", fragment=""))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ seen íŒŒì¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_seen() -> set[str]:
    return set(json.load(open(SEEN_FILE))) if os.path.exists(SEEN_FILE) else set()

def save_seen(s: set[str]): json.dump(list(s), open(SEEN_FILE, "w"), ensure_ascii=False, indent=2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ WP ì¡´ì¬ ì—¬ë¶€ í™•ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def wp_exists(url_norm: str) -> bool:
    r = requests.get(POSTS_API, params={"search": url_norm, "per_page": 1},
                     auth=(USER, APP_PW), timeout=10)
    return r.ok and bool(r.json())

def sync_seen(seen: set[str]) -> set[str]:
    synced = {u for u in seen if wp_exists(norm(u))}
    if synced != seen: save_seen(synced)
    return synced

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë§í¬ í¬ë¡¤ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_links() -> list[str]:
    soup = BeautifulSoup(requests.get(UDF_BASE, headers=HEADERS, timeout=10).text,
                         "html.parser")
    return list({norm(urljoin(UDF_BASE, a["href"]))
                 for a in soup.select("div.article1 div.article_title_news a[href]")})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ì‚¬ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse(url: str):
    r = requests.get(url, headers=HEADERS, timeout=10)
    if not r.ok: return None
    s = BeautifulSoup(r.text, "html.parser")
    h1 = s.find("h1", class_="newtitle")
    body = s.find("div", id="zooming")
    if not (h1 and body): return None
    img = s.find("img", class_="lazy") or s.find("img")
    return {
        "title": h1.get_text(strip=True),
        "html": str(body),
        "image": urljoin(url, img.get("data-src") or img.get("src")) if img else None,
        "url": url
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì™¸ë¶€ ë°ì´í„° (í™˜ìœ¨Â·ìœ ê°€Â·RSS í—¤ë“œë¼ì¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def nbrb_rate() -> str:
    try:
        j = requests.get("https://api.nbrb.by/exrates/rates/431", timeout=8).json()
        return f"NBRB USD/BYN {j['Cur_OfficialRate']:.4g} ({j['Date'][:10]})"
    except Exception:
        return "NBRB í™˜ìœ¨ ë°ì´í„° N/A"

def oil_price() -> str:
    try:
        # EIA DEMO_KEY ëŠ” í•˜ë£¨ 2ì²œ call í•œê³„
        j = requests.get(
            "https://api.eia.gov/series/?api_key=DEMO_KEY&series_id=PET.RWTC.D",
            timeout=8).json()
        price = j["series"][0]["data"][0][1]
        return f"WTI {price} $/bbl"
    except Exception:
        return "WTI ê°€ê²© N/A"

RSS_SOURCES = {
    "biz": "https://feeds.bbci.co.uk/news/business/rss.xml",
    "world": "https://feeds.bbci.co.uk/news/world/rss.xml",
    "eu": "https://feeds.bbci.co.uk/news/world/europe/rss.xml",
}
def rss_headlines(max_items=3) -> list[str]:
    out = []
    for url in RSS_SOURCES.values():
        try:
            feed = feedparser.parse(url)
            for e in feed.entries[: max_items]:
                out.append(f"â€¢ {e.title}")
        except Exception: pass
    random.shuffle(out)
    return out[:max_items]

def build_brief() -> str:
    s = []
    s.append(f"â€¢ {nbrb_rate()}")
    s.append(f"â€¢ {oil_price()}")
    for h in rss_headlines():
        s.append(h)
    return "<br>".join(s)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STYLE_GUIDE = f"""
â€¢ ì•„ë˜ í…œí”Œë¦¿ì„ â€˜ê·¸ëŒ€ë¡œâ€™ ìœ ì§€í•˜ì„¸ìš”.
  <h1>ğŸ“° (ì´ëª¨ì§€) 45ìâ†“ í•œêµ­ì–´ ì œëª©</h1>
  <h2>âœï¸ í¸ì§‘ì ì£¼ â€” í•µì‹¬ 2ë¬¸ì¥</h2>
  <h3>ğŸ“Š ìµœì‹  ë°ì´í„°</h3>
    {build_brief()}
    ì´ì–´ì„œ (ì´ 550ì ì´ìƒ) â€¦
  <h3>ğŸ’¬ ì „ë¬¸ê°€ ì „ë§</h3>
    (550ì ì´ìƒ, ì‹œë‚˜ë¦¬ì˜¤Â·ì¸ìš© í¬í•¨) â€¦
  <h3>â“ Q&A</h3>
    <ul><li><strong>Q1:</strong> â€¦<br><strong>A:</strong> â€¦</li> â€¦</ul>
  <p>ğŸ·ï¸ íƒœê·¸: ëª…ì‚¬ 3â€“6ê°œ</p>
  <p>ì¶œì²˜: UDF.name ì›ë¬¸<br>by. ì—ë””í„° LEEğŸŒ³</p>

â€¢ ì›ë¬¸ ë¬¸ì¥ âœ–ï¸ì‚­ì œ âœ–ï¸ìš”ì•½ (ê°€ë…ì„± ìœ„í•´ ë¬¸ë‹¨Â·ì–´ìˆœ ì¬ë°°ì¹˜ OK)
â€¢ ì½”ë“œë¸”ë¡Â·ë°±í‹±Â·â€œì†Œì œëª© 1â€ ê°™ì€ í‘œì‹œ ê¸ˆì§€.
â€¢ ì „ì²´ ê¸¸ì´ ìµœì†Œ 1â€Š400ì. 40 %ëŠ” ìƒˆë¡œ ìƒì„±ëœ í…ìŠ¤íŠ¸.
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPT ë¦¬ë¼ì´íŒ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rewrite(article):
    prompt = f"{STYLE_GUIDE}\n\nâ—† ì›ë¬¸:\n{article['html']}"
    headers = {"Authorization": f"Bearer {OPEN_KEY}", "Content-Type": "application/json"}
    data = {"model": "gpt-4o",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.6,
            "max_tokens": 2600}
    r = requests.post("https://api.openai.com/v1/chat/completions",
                      headers=headers, json=data, timeout=150)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"].strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ íƒœê·¸ ì¶”ì¶œ & WP íƒœê·¸ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STOP = {"ë²¨ë¼ë£¨ìŠ¤", "ë‰´ìŠ¤", "ê¸°ì‚¬"}
def tag_names(txt: str) -> list[str]:
    m = re.search(r"ğŸ·ï¸\s*íƒœê·¸[^:ï¼š]*[:ï¼š]\s*(.+)", txt)
    if not m: return []
    out = []
    for t in re.split(r"[,\s]+", m.group(1)):
        t = t.strip("â€“-#â€¢")
        if 1 < len(t) <= 20 and t not in STOP and t not in out:
            out.append(t)
        if len(out) == 6: break
    return out

def tag_id(name: str):
    q = requests.get(TAGS_API, params={"search": name, "per_page": 1},
                     auth=(USER, APP_PW), timeout=10)
    if q.ok and q.json(): return q.json()[0]["id"]
    c = requests.post(TAGS_API, json={"name": name},
                      auth=(USER, APP_PW), timeout=10)
    return c.json()["id"] if c.status_code == 201 else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë‚´ë¶€ ê´€ë ¨ ê¸°ì‚¬ ë§í¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def related_links(tag_ids, exclude_id=0, limit=3):
    if not tag_ids: return ""
    res = requests.get(
        POSTS_API,
        params={"tags": tag_ids[0], "per_page": limit,
                "exclude": exclude_id, "status": "publish"},
        auth=(USER, APP_PW), timeout=10).json()
    if not isinstance(res, list) or not res: return ""
    lis = [f'<li><a href="{p["link"]}">{html.escape(p["title"]["rendered"])}</a></li>'
           for p in res]
    return "<h3>ğŸ“š ê´€ë ¨ ê¸°ì‚¬ ë” ë³´ê¸°</h3><ul>" + "".join(lis) + "</ul>"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²Œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def publish(article, txt, tag_ids):
    hidden  = f'<a href="{article["url"]}" style="display:none">src</a>\n'
    cap     = '<p><em>Photo: UDF.name</em></p>\n' if article["image"] else ""
    img_tag = f'<p><img src="{article["image"]}" alt=""></p>\n' if article["image"] else ""

    soup = BeautifulSoup(txt, "html.parser")
    h1 = soup.find("h1")
    title = h1.get_text(strip=True) if h1 else article["title"]
    if h1: h1.decompose()

    if not soup.find(string=lambda t: isinstance(t, str) and "ì¶œì²˜:" in t):
        soup.append(BeautifulSoup(
            '<p>ì¶œì²˜: UDF.name ì›ë¬¸<br>by. ì—ë””í„° LEEğŸŒ³</p>', "html.parser"))

    body = hidden + cap + img_tag + str(soup) + related_links(tag_ids)

    payload = {"title": title, "content": body, "status": "publish",
               "categories": [TARGET_CAT_ID], "tags": tag_ids}
    r = requests.post(POSTS_API, json=payload, auth=(USER, APP_PW), timeout=30)
    logging.info("  â†³ ê²Œì‹œ %s %s", r.status_code, r.json().get("id"))
    r.raise_for_status()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    logging.basicConfig(level=logging.INFO,
        format="%(asctime)s â”‚ %(levelname)s â”‚ %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S")
    logging.info("ğŸš€ ë²„ì „ 4.0.1-full ì‹¤í–‰")

    seen = sync_seen(load_seen())
    links = fetch_links()
    todo = [u for u in links if norm(u) not in seen and not wp_exists(norm(u))]
    logging.info("ğŸ“° ìƒˆ ê¸°ì‚¬ %d / ì´ %d", len(todo), len(links))

    for url in todo:
        logging.info("â–¶ %s", url)
        art = parse(url); time.sleep(1)
        if not art: continue

        try:
            txt = rewrite(art)
        except Exception as e:
            logging.warning("GPT ì˜¤ë¥˜: %s", e); continue

        tag_ids = [tid for n in tag_names(txt) if (tid := tag_id(n))]
        try:
            publish(art, txt, tag_ids)
            seen.add(norm(url)); save_seen(seen)
        except Exception as e:
            logging.warning("ì—…ë¡œë“œ ì‹¤íŒ¨: %s", e)

        time.sleep(1.5)

if __name__ == "__main__":
    main()
