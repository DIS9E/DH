#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
udf.name.py â€“ v4.0.1  (AdSenseÂ·ë‰´ë‹‰ í†¤Â·ì™¸ë¶€ë°ì´í„°Â·ìë™ íƒœê·¸Â·ê´€ë ¨ ê¸°ì‚¬Â·ë””ë²„ê·¸ í¬í•¨)
"""

import os, sys, re, json, time, logging, random, html
from datetime import datetime
from urllib.parse import urljoin, urlparse, urlunparse

import requests, feedparser                # â† requirements.txt ì— feedparser ì¶”ê°€
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WP_URL   = os.getenv("WP_URL", "https://belatri.info").rstrip("/")
USER     = os.getenv("WP_USERNAME")
APP_PW   = os.getenv("WP_APP_PASSWORD")
OPEN_KEY = os.getenv("OPENAI_API_KEY")
if not all([USER, APP_PW, OPEN_KEY]):
    sys.exit("âŒ  WP_USERNAME / WP_APP_PASSWORD / OPENAI_API_KEY ëˆ„ë½")

POSTS_API = f"{WP_URL}/wp-json/wp/v2/posts"
TAGS_API  = f"{WP_URL}/wp-json/wp/v2/tags"

UDF_BASE   = "https://udf.name/news/"
HEADERS    = {"User-Agent": "UDFCrawler/4.0.1"}
SEEN_FILE  = "seen_urls.json"
TARGET_CAT_ID = 20                         # â€˜ë²¨ë¼ë£¨ìŠ¤ ë‰´ìŠ¤â€™

norm = lambda u: urlunparse(urlparse(u)._replace(query="", params="", fragment=""))

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s â”‚ %(levelname)s â”‚ %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_seen() -> set[str]:
    if os.path.exists(SEEN_FILE):
        with open(SEEN_FILE, encoding="utf-8") as f:
            return set(json.load(f))
    return set()

def save_seen(s: set[str]):  json.dump(list(s), open(SEEN_FILE, "w"), ensure_ascii=False, indent=2)

def wp_exists(u):
    r = requests.get(POSTS_API, params={"search": u, "per_page": 1},
                     auth=(USER, APP_PW), timeout=10)
    return r.ok and bool(r.json())

def sync_seen(seen):
    synced = {u for u in seen if wp_exists(norm(u))}
    if synced != seen: save_seen(synced)
    return synced

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë§í¬ í¬ë¡¤ëŸ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_links():
    soup = BeautifulSoup(requests.get(UDF_BASE, headers=HEADERS, timeout=10).text,
                         "html.parser")
    return list({norm(urljoin(UDF_BASE, a["href"]))
                 for a in soup.select("div.article1 div.article_title_news a[href]")})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ì‚¬ íŒŒì„œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse(url):
    r = requests.get(url, headers=HEADERS, timeout=10)
    if not r.ok: return None
    s = BeautifulSoup(r.text, "html.parser")
    t = s.find("h1", class_="newtitle"); b = s.find("div", id="zooming")
    if not (t and b): return None
    img = s.find("img", class_="lazy") or s.find("img")
    return {"title": t.get_text(strip=True),
            "html": str(b),
            "image": urljoin(url, img.get("data-src") or img.get("src")) if img else None,
            "url": url}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì™¸ë¶€ ë°ì´í„° (í™˜ìœ¨Â·ìœ ê°€Â·í—¤ë“œë¼ì¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_usd_rate():
    try:
        js = requests.get("https://api.nbrb.by/exrates/rates/431", timeout=8).json()
        return f"{js['Cur_OfficialRate']:.4g} BYN (NBRB Â· {js['Date'][:10]})"
    except Exception:  return "ë°ì´í„° ì—†ìŒ"

def get_wti_price():
    try:
        # ë¬´ë£Œ DEMO_KEY ëŠ” 24h ì§€ì—°Â·404ì¼ ìˆ˜ ìˆìŒ â†’ ì‹¤íŒ¨ ì‹œ None
        js = requests.get("https://api.eia.gov/series/?api_key=DEMO_KEY&series_id=PET.RWTC.D",
                          timeout=8).json()
        v, d = js["series"][0]["data"][-1]
        return f"{float(v):.2f}$/bbl (WTI Â· {d})"
    except Exception:  return None

def get_rss_headlines(category):
    feeds = {
        "econ": ["https://feeds.bbci.co.uk/news/business/rss.xml",
                 "https://www.reuters.com/rssFeed/businessNews"],
        "world": ["https://feeds.bbci.co.uk/news/world/rss.xml"],
    }.get(category, [])
    items = []
    for url in feeds:
        try:
            fp = feedparser.parse(url)
            items += [ent.title for ent in fp.entries[:5]]
        except Exception:
            continue
    return random.sample(items, k=min(3, len(items)))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¸Œë¦¬í”„(ğŸ“ŠÂ·ğŸ’¬) ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_brief(cat):
    s = []
    usd = get_usd_rate(); s.append(f"â€¢ NBRB í™˜ìœ¨ USD/BYN {usd}")
    wti = get_wti_price();  wti and s.append(f"â€¢ êµ­ì œìœ ê°€ WTI {wti}")
    heads = get_rss_headlines(cat)
    for h in heads: s.append(f"â€¢ í—¤ë“œë¼ì¸ â€• {h}")
    return "<br>".join(s)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STYLE_GUIDE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STYLE_GUIDE = """
â€¢ êµ¬ì¡°
  <h1>ğŸ“° (ì´ëª¨ì§€) ë‰´ë‹‰ ëŠë‚Œì˜ í•œêµ­ì–´ ì œëª©</h1>
  <h2>âœï¸ í¸ì§‘ì ì£¼ â€” 2ë¬¸ì¥ ê°œìš”</h2>
  <h3>ğŸ“Š ìµœì‹  ë°ì´í„°</h3>
    ì™¸ë¶€ APIÂ·RSS ìˆ«ì ìµœì†Œ 4ì¤„
  <h3>ğŸ’¬ ì „ë¬¸ê°€ ì „ë§</h3>
    ìµœì†Œ 500ì, ì§ˆë¬¸Â·ê°íƒ„ í¬í•¨
  <h3>â“ Q&A</h3>
    Q3ê°œ + ê° ë‹µë³€ 2ë¬¸ì¥â†‘
  <h3>ğŸ’¡ ë³¸ë¬¸ í•´ì„¤</h3>
    ì›ë¬¸ 100% ë³´ì¡´Â·ë¶€ê°€ í•´ì„¤
  <p>ğŸ·ï¸ íƒœê·¸: ëª…ì‚¬ 3~6ê°œ</p>
  <p>ì¶œì²˜: UDF.name ì›ë¬¸<br>Photo: UDF.name<br>by. ì—ë””í„° LEEğŸŒ³</p>

â€¢ ìš”ì•½Â·ì‚­ì œ ê¸ˆì§€ â€” ì›ë¬¸ ë¬¸ì¥ ìœ ì§€ + ë¶€ê°€ ì„¤ëª…ìœ¼ë¡œ â€˜ìƒˆ í…ìŠ¤íŠ¸ 40%â†‘â€™
â€¢ ì œëª©ì€ 45ìâ†“, ì´ëª¨ì§€ 1~3ê°œ, ëŸ¬ì‹œì•„ì–´Â·ì˜ì–´ ê¸ˆì§€
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPT ë¦¬ë¼ì´íŒ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CYRILLIC = re.compile(r"[Ğ-Ğ¯Ğ°-ÑĞÑ‘]")

def korean_title(src:str, context:str)->str:
    if not CYRILLIC.search(src): return src  # ì´ë¯¸ í•œê¸€
    prompt=( "ë‹¤ìŒ ê¸°ì‚¬ ë‚´ìš©ì— ì–´ìš¸ë¦¬ëŠ” ì¹´í”¼ë¼ì´íŒ… í•œêµ­ì–´ ì œëª©ì„ 45ì ì´ë‚´ë¡œ ì‘ì„±í•˜ê³ , "
             "ê´€ë ¨ ì´ëª¨ì§€ 1â€“3ê°œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•˜ì„¸ìš”.\n\n"
             f"ì›ì œëª©: {src}\nê¸°ì‚¬ ì¼ë¶€: {context[:300]}" )
    data={"model":"gpt-4o-mini","messages":[{"role":"user","content":prompt}],
          "temperature":0.8,"max_tokens":60}
    try:
        r=requests.post("https://api.openai.com/v1/chat/completions",
                        headers={"Authorization":f"Bearer {OPEN_KEY}","Content-Type":"application/json"},
                        json=data,timeout=20)
        r.raise_for_status()
        return r.json()["choices"][0]["message"]["content"].strip()
    except Exception:
        return src

def rewrite(article, brief_html):
    prompt = f"{STYLE_GUIDE}\n\nâ—† ì›ë¬¸:\n{article['html']}\n\nâ—† ì™¸ë¶€ë°ì´í„° HTML:\n{brief_html}"
    data={"model":"gpt-4o","messages":[{"role":"user","content":prompt}],
          "temperature":0.55,"max_tokens":2200}
    r=requests.post("https://api.openai.com/v1/chat/completions",
                    headers={"Authorization":f"Bearer {OPEN_KEY}",
                             "Content-Type":"application/json"},
                    json=data,timeout=120)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"].strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ íƒœê·¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STOP = {"ë²¨ë¼ë£¨ìŠ¤","ë‰´ìŠ¤","ê¸°ì‚¬"}
def extract_tags(txt):
    m = re.search(r"ğŸ·ï¸\s*íƒœê·¸[^:ï¼š]*[:ï¼š]\s*(.+)", txt)
    if not m: return []
    out=[]
    for t in re.split(r"[,\s]+", m.group(1)):
        t=t.strip("â€“-#â€¢")
        if 1<len(t)<=20 and t not in STOP and t not in out:
            out.append(t)
        if len(out)==6: break
    return out

def ensure_tag(name):
    q=requests.get(TAGS_API, params={"search":name,"per_page":1},
                   auth=(USER,APP_PW),timeout=10)
    if q.ok and q.json(): return q.json()[0]["id"]
    c=requests.post(TAGS_API, json={"name":name},
                    auth=(USER,APP_PW),timeout=10)
    return c.json()["id"] if c.status_code==201 else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê´€ë ¨ ê¸°ì‚¬ ë§í¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def related_posts(tag_ids):
    if not tag_ids: return ""
    r=requests.get(POSTS_API, params={"tags":",".join(map(str,tag_ids)),
                                      "per_page":3,"exclude":0,"status":"publish"},
                   auth=(USER,APP_PW),timeout=10)
    if not (r.ok and r.json()): return ""
    li = '\n'.join(f"<li><a href='{p['link']}'>{html.escape(p['title']['rendered'])}</a></li>"
                   for p in r.json())
    return f"<h3>ğŸ“š ê´€ë ¨ ê¸°ì‚¬ ë” ë³´ê¸°</h3><ul>{li}</ul>"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²Œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def publish(article, gpt_html, tag_ids):
    hidden  = f'<a href="{article["url"]}" style="display:none">src</a>\n'
    img_tag = f'<p><img src="{article["image"]}" alt=""></p>\n' if article["image"] else ""

    soup = BeautifulSoup(gpt_html, "html.parser")
    h1 = soup.find("h1")
    title_txt = korean_title(h1.get_text(" ",strip=True) if h1 else article["title"],
                             soup.get_text(" ",strip=True))
    if h1: h1.decompose()                   # ë³¸ë¬¸ì— H1 ì œê±°

    body = hidden + img_tag + str(soup) + related_posts(tag_ids)

    payload = {"title": title_txt, "content": body, "status":"publish",
               "categories":[TARGET_CAT_ID], "tags": tag_ids}
    r=requests.post(POSTS_API, json=payload, auth=(USER,APP_PW), timeout=30)
    r.raise_for_status()
    logging.info("  â†³ ê²Œì‹œ %s %s", r.status_code, r.json().get("id"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    logging.info("ğŸš€ ë²„ì „ 4.0.1 ì‹¤í–‰")
    seen = sync_seen(load_seen())
    links = fetch_links()
    todo = [u for u in links if norm(u) not in seen and not wp_exists(norm(u))]
    logging.info("ğŸ“° ìƒˆ ê¸°ì‚¬ %d / ì´ %d", len(todo), len(links))

    for url in todo:
        logging.info("â–¶ %s", url)
        art = parse(url); time.sleep(1)
        if not art: continue

        brief = build_brief("econ" if "/economic/" in url else "world")
        try:
            txt = rewrite(art, brief)
        except Exception as e:
            logging.warning("GPT ì˜¤ë¥˜: %s", e); continue

        tag_ids=[ensure_tag(n) for n in extract_tags(txt)]
        tag_ids = [t for t in tag_ids if t]

        try:
            publish(art, txt, tag_ids)
            seen.add(norm(url)); save_seen(seen)
        except Exception as e:
            logging.warning("ì—…ë¡œë“œ ì‹¤íŒ¨: %s", e)

        time.sleep(2)

if __name__ == "__main__":
    main()
