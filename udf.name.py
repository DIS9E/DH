#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
udf.name.py â€“ v3.6.2  (í•œêµ­ì–´ ì œëª© ë³€í™˜ íŒ¨ì¹˜)
â€¢ WPâ†”seen ë™ê¸°í™”  â€¢ ì´ë¯¸ì§€ ì‚½ì… â€¢ ì¤‘ë³µ ë°©ì§€ â€¢ ìë™ íƒœê·¸ â€¢ í—¤ë“œë¼ì´íŠ¸ í†¤
â€¢ ëŸ¬ì‹œì•„ì–´ ì œëª©ì„ í•œêµ­ì–´ ì¹´í”¼ë¼ì´í„° ìŠ¤íƒ€ì¼+ì´ëª¨ì§€ë¡œ ìë™ ë³€í™˜
"""

import os, sys, re, json, time, logging
from urllib.parse import urljoin, urlparse, urlunparse
import requests
from bs4 import BeautifulSoup
from requests.exceptions import RequestException

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WP_URL   = os.getenv("WP_URL", "https://belatri.info").rstrip("/")
USER     = os.getenv("WP_USERNAME")
APP_PW   = os.getenv("WP_APP_PASSWORD")
OPEN_KEY = os.getenv("OPENAI_API_KEY")           # â† ë°˜ë“œì‹œ ì„¤ì •
if not all([USER, APP_PW, OPEN_KEY]):
    sys.exit("âŒ  WP_USERNAME / WP_APP_PASSWORD / OPENAI_API_KEY ëˆ„ë½")

POSTS_API = f"{WP_URL}/wp-json/wp/v2/posts"
TAGS_API  = f"{WP_URL}/wp-json/wp/v2/tags"

UDF_BASE   = "https://udf.name/news/"
HEADERS    = {"User-Agent": "UDFCrawler/3.6.2"}
SEEN_FILE  = "seen_urls.json"
TARGET_CAT_ID = 20                      # â€˜ë²¨ë¼ë£¨ìŠ¤ ë‰´ìŠ¤â€™ ì¹´í…Œê³ ë¦¬(ID)

norm = lambda u: urlunparse(urlparse(u)._replace(query="", params="", fragment=""))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ seen íŒŒì¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_seen() -> set[str]:
    if os.path.exists(SEEN_FILE):
        with open(SEEN_FILE, encoding="utf-8") as f:
            return set(json.load(f))
    return set()

def save_seen(s: set[str]):
    with open(SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump(list(s), f, ensure_ascii=False, indent=2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ WP ì¡´ì¬ ì—¬ë¶€ í™•ì¸ & ë™ê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def wp_exists(url_norm: str) -> bool:
    r = requests.get(POSTS_API,
                     params={"search": url_norm, "per_page": 1},
                     auth=(USER, APP_PW), timeout=10)
    return r.ok and bool(r.json())

def sync_seen(seen: set[str]) -> set[str]:
    synced = {u for u in seen if wp_exists(norm(u))}
    if len(synced) != len(seen):
        save_seen(synced)
    return synced

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë§í¬ í¬ë¡¤ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_links() -> list[str]:
    soup = BeautifulSoup(requests.get(UDF_BASE, headers=HEADERS, timeout=10).text,
                         "html.parser")
    return list({norm(urljoin(UDF_BASE, a["href"]))
                 for a in soup.select("div.article1 div.article_title_news a[href]")})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ì‚¬ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse(url: str):
    r = requests.get(url, headers=HEADERS, timeout=10)
    if not r.ok:
        return None
    s = BeautifulSoup(r.text, "html.parser")
    title = s.find("h1", class_="newtitle")
    body  = s.find("div", id="zooming")
    if not (title and body):
        return None
    img = s.find("img", class_="lazy") or s.find("img")
    img_url = urljoin(url, img.get("data-src") or img.get("src")) if img else None
    return {"title": title.get_text(strip=True),
            "html": str(body),
            "image": img_url,
            "url": url}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‘ì„± ê°€ì´ë“œ & í”„ë¡¬í”„íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STYLE_GUIDE = """
â€¢ êµ¬ì¡° (ë°˜ë“œì‹œ HTML íƒœê·¸ ì‚¬ìš©)
  <h1>ğŸ“° ğŸ¯ í¥ë¯¸ë¡œìš´ ì œëª© ğŸ˜®</h1>
  <h2>âœï¸ í¸ì§‘ì ì£¼ â€” ì›ë¬¸ ë§¥ë½ 2ë¬¸ì¥</h2>
  <h3>ì†Œì œëª© 1</h3>
    ë³¸ë¬¸ â€¦
  <h3>ì†Œì œëª© 2</h3>
    ë³¸ë¬¸ â€¦
  <p>ğŸ·ï¸ íƒœê·¸: ëª…ì‚¬ 3~6ê°œ</p>
  <p>ì´ ê¸°ì‚¬ëŠ” ë²¨ë¼ë£¨ìŠ¤ í˜„ì§€ ë³´ë„ ë‚´ìš©ì„ ì¬êµ¬ì„±í•œ ì½˜í…ì¸ ì…ë‹ˆë‹¤.<br>
     by. ì—ë””í„° LEEğŸŒ³</p>

â€¢ âš ï¸ **ì ˆëŒ€ ìš”ì•½Â·ì‚­ì œ ê¸ˆì§€** â€” ì›ë¬¸ ê¸¸ì´ë¥¼ 100 % ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.
â€¢ ì œëª©ì€ **ì „ë¶€ í•œêµ­ì–´**ë¡œ, ì¹´í”¼ë¼ì´í„°ì²˜ëŸ¼ ëˆˆê¸¸ì„ ëŒë˜ ê´€ë ¨ ì´ëª¨ì§€ 1â€“3ê°œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë„£ìœ¼ì„¸ìš”. ğŸ‡°ğŸ‡·  <â”€â€» ëŸ¬ì‹œì•„ì–´Â·ì˜ì–´ ê¸ˆì§€!
â€¢ í†¤: ì¹œê·¼í•œ ëŒ€í™”ì²´, ì§ˆë¬¸Â·ê°íƒ„ ì„ê¸°.
"""

# â”€â”€ GPT í˜¸ì¶œ (ë³¸ë¬¸ ë¦¬ë¼ì´íŒ…) â”€â”€
def rewrite(article: dict) -> str:
    prompt = f"""{STYLE_GUIDE}

ì•„ë˜ ì›ë¬¸ì„ ê·œì¹™ì— ë§ì¶° ì¬ì‘ì„±í•˜ì„¸ìš”.

â—† ì›ë¬¸:
{article['html']}
"""
    headers = {
        "Authorization": f"Bearer {OPEN_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "gpt-4o",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.4,
        "max_tokens": 1800
    }
    r = requests.post("https://api.openai.com/v1/chat/completions",
                      headers=headers, json=data, timeout=90)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"].strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ëŸ¬ì‹œì•„ì–´ ì œëª© â†’ í•œêµ­ì–´ ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CYRILLIC = re.compile(r"[Ğ-Ğ¯Ğ°-ÑĞÑ‘]")

def korean_title(src: str) -> str:
    """
    í‚¤ë¦´ ë¬¸ìê°€ í¬í•¨ëœ ì œëª©ì„ í•œêµ­ì–´ ì¹´í”¼ë¼ì´í„° ìŠ¤íƒ€ì¼(ì´ëª¨ì§€ í¬í•¨)ë¡œ ë³€í™˜.
    ì´ë¯¸ í•œê¸€ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜.
    """
    if not CYRILLIC.search(src):
        return src  # ë³€í™˜ ë¶ˆí•„ìš”

    prompt = (
        "ë‹¤ìŒ ì œëª©ì„ í•œêµ­ì–´ ì¹´í”¼ë¼ì´í„° ìŠ¤íƒ€ì¼ë¡œ 45ì ì´ë‚´ë¡œ ì¬ì°½ì‘í•´ ì£¼ì„¸ìš”. "
        "ê´€ë ¨ ì´ëª¨ì§€ 1â€“3ê°œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë„£ì–´ ì£¼ì„¸ìš”.\n\n"
        f"Â«{src}Â»"
    )
    headers = {"Authorization": f"Bearer {OPEN_KEY}", "Content-Type": "application/json"}
    data = {
        "model": "gpt-4o-mini",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.8,
        "max_tokens": 60,
    }
    try:
        r = requests.post("https://api.openai.com/v1/chat/completions",
                          headers=headers, json=data, timeout=20)
        r.raise_for_status()
        return r.json()["choices"][0]["message"]["content"].strip()
    except Exception as e:
        logging.warning("ì œëª© ë³€í™˜ ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: %s", e)
        return src

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ íƒœê·¸ ì¶”ì¶œ & WP íƒœê·¸ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STOP = {"ë²¨ë¼ë£¨ìŠ¤", "ë‰´ìŠ¤", "ê¸°ì‚¬"}
def tag_names(txt: str) -> list[str]:
    m = re.search(r"ğŸ·ï¸\s*íƒœê·¸[^:ï¼š]*[:ï¼š]\s*(.+)", txt)
    if not m:
        return []
    out = []
    for t in re.split(r"[,\s]+", m.group(1)):
        t = t.strip("â€“-#â€¢")
        if 1 < len(t) <= 20 and t not in STOP and t not in out:
            out.append(t)
        if len(out) == 6:
            break
    return out

def tag_id(name: str):
    q = requests.get(TAGS_API, params={"search": name, "per_page": 1},
                     auth=(USER, APP_PW), timeout=10)
    if q.ok and q.json():
        return q.json()[0]["id"]
    c = requests.post(TAGS_API, json={"name": name},
                      auth=(USER, APP_PW), timeout=10)
    if c.status_code == 201:
        return c.json()["id"]
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²Œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def publish(article: dict, txt: str, tag_ids: list[int]):
    hidden = f'<a href="{article["url"]}" style="display:none">src</a>\n'
    img_tag = f'<p><img src="{article["image"]}" alt=""></p>\n' if article["image"] else ""

    # â€•â€•â€• ì œëª© ì¶”ì¶œ & í•œêµ­ì–´ ë³€í™˜ â€•â€•â€•
    soup       = BeautifulSoup(txt, "html.parser")
    h1_tag     = soup.find("h1")
    orig_title = h1_tag.get_text(strip=True) if h1_tag else article["title"]
    title      = korean_title(orig_title)

    if h1_tag:
        h1_tag.string = title
    else:
        # ë³¸ë¬¸ì— <h1>ì´ ì—†ìœ¼ë©´ ë§¨ ì•ì— ì‚½ì…
        new_h1 = soup.new_tag("h1")
        new_h1.string = title
        soup.insert(0, new_h1)

    txt = str(soup)
    body = hidden + img_tag + txt

    payload = {
        "title": title,
        "content": body,
        "status": "publish",
        "categories": [TARGET_CAT_ID],
        "tags": tag_ids
    }
    r = requests.post(POSTS_API, json=payload, auth=(USER, APP_PW), timeout=30)
    logging.info("  â†³ ê²Œì‹œ %s %s", r.status_code, r.json().get("id"))
    r.raise_for_status()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s â”‚ %(levelname)s â”‚ %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S")

    seen = sync_seen(load_seen())
    links = fetch_links()

    todo = [u for u in links if norm(u) not in seen and not wp_exists(norm(u))]
    logging.info("ğŸ“° ìƒˆ ê¸°ì‚¬ %d / ì´ %d", len(todo), len(links))

    for url in todo:
        logging.info("â–¶ %s", url)
        art = parse(url)
        if not art:
            continue

        try:
            txt = rewrite(art)
        except Exception as e:
            logging.warning("GPT ì˜¤ë¥˜: %s", e)
            continue

        tag_ids = [tid for name in tag_names(txt) if (tid := tag_id(name))]
        try:
            publish(art, txt, tag_ids)
            seen.add(norm(url)); save_seen(seen)
        except Exception as e:
            logging.warning("ì—…ë¡œë“œ ì‹¤íŒ¨: %s", e)

        time.sleep(2)

if __name__ == "__main__":
    main()
