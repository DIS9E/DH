#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
udf.name.py â€“ v3.6.2 (3.6 ê¸°ë°˜ ìµœì†Œ íŒ¨ì¹˜)
 â€¢ WPâ†”seen ë™ê¸°í™” / dup-safe / no-media
 â€¢ í—¤ë“œë¼ì´íŠ¸ í†¤ ë¦¬ë¼ì´íŠ¸ + GPT íƒœê·¸ ìë™í™”
 â€¢ Yoast ë©”íƒ€ í•„ë“œ ì™„ì „ ì œê±°
"""

import os, sys, re, json, time, logging
from urllib.parse import urljoin, urlparse, urlunparse
import requests
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜
WP_URL   = os.getenv("WP_URL", "https://belatri.info").rstrip("/")
USER     = os.getenv("WP_USERNAME")      # WP Application-Password user
APP_PW   = os.getenv("WP_APP_PASSWORD")  # â€¦password
OPEN_KEY = os.getenv("OPENAI_API_KEY")   # OpenAI key
if not all([USER, APP_PW, OPEN_KEY]):
    sys.exit("âŒ  WP_USERNAME / WP_APP_PASSWORD / OPENAI_API_KEY ëˆ„ë½")

POSTS = f"{WP_URL}/wp-json/wp/v2/posts"
TAGS  = f"{WP_URL}/wp-json/wp/v2/tags"

UDF_BASE   = "https://udf.name/news/"
HEADERS    = {"User-Agent": "UDFCrawler/3.6.2"}
SEEN_FILE  = "seen_urls.json"
TARGET_CAT_ID = 20            # ê³ ì • ì¹´í…Œê³ ë¦¬(â€˜ë²¨ë¼ë£¨ìŠ¤ ë‰´ìŠ¤â€™) ID

norm = lambda u: urlunparse(urlparse(u)._replace(query="", params="", fragment=""))

# â”€â”€â”€â”€â”€â”€â”€ seen íŒŒì¼
def load_seen() -> set[str]:
    if os.path.exists(SEEN_FILE):
        with open(SEEN_FILE, encoding="utf-8") as f:
            return set(json.load(f))
    return set()

def save_seen(s: set[str]):
    with open(SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump(list(s), f, ensure_ascii=False, indent=2)

# â”€â”€â”€â”€â”€â”€â”€ WPì— í•´ë‹¹ URL ê¸€ì´ ì‚´ì•„ ìˆëŠ”ê°€
def wp_exists(url_norm: str) -> bool:
    r = requests.get(POSTS, params={"search": url_norm, "per_page": 1},
                     auth=(USER, APP_PW), timeout=10)
    return r.ok and bool(r.json())

# â”€â”€â”€â”€â”€â”€â”€ WP ì‹¤ê¸€ê³¼ seen.json ë™ê¸°í™”
def sync_seen(seen: set[str]) -> set[str]:
    synced = {u for u in seen if wp_exists(norm(u))}
    if len(synced) != len(seen):
        save_seen(synced)
    return synced

# â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
def fetch_links() -> list[str]:
    soup = BeautifulSoup(
        requests.get(UDF_BASE, headers=HEADERS, timeout=10).text, "html.parser"
    )
    # UDFëŠ” ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ .article1 ì˜ a íƒœê·¸ë¡œ ë…¸ì¶œ
    return list(
        {
            norm(urljoin(UDF_BASE, a["href"]))
            for a in soup.select("div.article1 div.article_title_news a[href]")
        }
    )

# â”€â”€â”€â”€â”€â”€â”€ ê°œë³„ ê¸°ì‚¬ íŒŒì‹±
def parse(url: str):
    r = requests.get(url, headers=HEADERS, timeout=10)
    if not r.ok:
        return None
    s = BeautifulSoup(r.text, "html.parser")
    title = s.find("h1", class_="newtitle")
    body  = s.find("div", id="zooming")
    if not (title and body):
        return None

    # ëŒ€í‘œ ì´ë¯¸ì§€ (ë³¸ë¬¸ ë§¨ ì•ì— ë„£ê¸°ë§Œ - WP ë¯¸ë””ì–´ ì—…ë¡œë“œ X)
    img = s.find("img", class_="lazy") or s.find("img")
    img_url = urljoin(url, img.get("data-src") or img.get("src")) if img else None

    return {"title": title.get_text(strip=True),
            "html":  str(body),
            "image": img_url,
            "url":   url}

# â”€â”€â”€â”€â”€â”€â”€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ / GPT í”„ë¡¬í”„íŠ¸
STYLE_GUIDE = """
â€¢ í†¤: ì¹œê·¼í•œ ì¡´ëŒ“ë§, ì§ˆë¬¸Â·ê°íƒ„ ì‚¬ìš©
â€¢ êµ¬ì¡°
  ğŸ“° ì œëª©
  âœï¸ í¸ì§‘ì ì£¼ â€” í•µì‹¬ 2ë¬¸ì¥
  ğŸ—ï¸ ë³¸ë¬¸
    â€£ ì†Œì œëª©1: â€¦
    â€£ ì†Œì œëª©2: â€¦
  ğŸ”¦ í—¤ë“œë¼ì´íŠ¸'s ì½”ë©˜íŠ¸ (300ì ë‚´ì™¸)
  ğŸ·ï¸ íƒœê·¸: ëª…ì‚¬ 3~6ê°œ
â€¢ #, ##, ### ê°™ì€ ë§ˆí¬ë‹¤ìš´ í—¤ë” ì‚¬ìš© ê¸ˆì§€
â€¢ ì›ë¬¸ ë‚´ìš© 90Â±10 % ë¶„ëŸ‰ ìœ ì§€ (ë‚­ë¹„ë˜ëŠ” ìš”ì•½ X)
"""

def rewrite(article: dict) -> str:
    prompt = f"""{STYLE_GUIDE}

ì•„ë˜ ì›ë¬¸ì„ ê·œì¹™ì— ë§ì¶° ì¬ì‘ì„±í•˜ì„¸ìš”.

â—† ì›ë¬¸
{article['html']}
"""
    res = requests.post(
        "https://api.openai.com/v1/chat/completions",
        headers={
            "Authorization": f"Bearer {OPEN_KEY}",
            "Content-Type": "application/json",
        },
        json={
            "model": "gpt-4o",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.4,
        },
        timeout=90,
    )
    res.raise_for_status()
    text = res.json()["choices"][0]["message"]["content"]

    # GPTê°€ í˜¹ì‹œ í—¤ë” ê¸°í˜¸ë¥¼ ì¨ë„ ì¹˜í™˜
    fixed = []
    for line in text.splitlines():
        if line.startswith("###"):
            fixed.append("â€£ " + line.lstrip("# ").strip())
        elif line.startswith("##"):
            fixed.append("âœï¸ " + line.lstrip("# ").strip())
        elif line.startswith("#"):
            fixed.append("ğŸ“° " + line.lstrip("# ").strip())
        else:
            fixed.append(line)
    return "\n".join(fixed)

# â”€â”€â”€â”€â”€â”€â”€ GPTê°€ ì¶œë ¥í•œ íƒœê·¸ ë¬¸ìì—´ â†’ ë¦¬ìŠ¤íŠ¸  (â˜… 3.6.2 íŒ¨ì¹˜)
PARTICLE = re.compile(r"^(ì€|ëŠ”|ì´|ê°€|ì˜|ê³¼|ì™€|ì—ì„œ|ìœ¼ë¡œ)$")
STOP     = {"ë²¨ë¼ë£¨ìŠ¤", "ë‰´ìŠ¤", "ê¸°ì‚¬"}

def tag_names(txt: str) -> list[str]:
    m = re.search(r"ğŸ·ï¸\s*íƒœê·¸[^:ï¼š]*[:ï¼š]\s*(.+)", txt)
    if not m:
        return []

    out: list[str] = []
    for raw in re.split(r"[,\s]+", m.group(1)):
        t = raw.strip("â€“â€”-â€¢#.,")
        if (
            1 < len(t) <= 20
            and not PARTICLE.match(t)
            and t not in STOP
            and t not in out
        ):
            out.append(t)
        if len(out) == 6:
            break
    return out

def tag_id(name: str):
    q = requests.get(
        TAGS, params={"search": name, "per_page": 1},
        auth=(USER, APP_PW), timeout=10
    )
    if q.ok and q.json():
        return q.json()[0]["id"]
    c = requests.post(
        TAGS, json={"name": name},
        auth=(USER, APP_PW), timeout=10
    )
    return c.json().get("id") if c.status_code == 201 else None

# â”€â”€â”€â”€â”€â”€â”€ ì›Œë“œí”„ë ˆìŠ¤ ë°œí–‰  (â˜… Yoast í•„ë“œ ì œê±° ë²„ì „)
def publish(article, txt: str, tag_ids: list[int]):
    # ì›ë³¸â€†URL ì€ SEO ë…¸ì´ì¦ˆ ì£¼ì§€ ì•Šê²Œ ìˆ¨ê¹€ ë§í¬ ì²˜ë¦¬
    hidden = f'<a href="{article["url"]}" style="display:none">src</a>\n'
    img_tag = f'<p><img src="{article["image"]}" alt=""></p>\n' if article["image"] else ""
    body = hidden + img_tag + txt

    # ì œëª© ì¶”ì¶œ (ğŸ“° ë¼ì¸ â†’ WP title)
    title_line = next((l for l in txt.splitlines() if l.startswith("ğŸ“°")), article["title"])
    title = title_line.lstrip("ğŸ“°").strip()

    payload = {
        "title":      title,
        "content":    body,
        "status":     "publish",
        "categories": [TARGET_CAT_ID],
        "tags":       tag_ids,        # Yoast í•„ë“œ ì œê±° â†’ WP ê¸°ë³¸ íƒœê·¸ë§Œ
    }

    r = requests.post(POSTS, json=payload, auth=(USER, APP_PW), timeout=30)
    print("  â†³ ê²Œì‹œ", r.status_code, r.json().get("id"))
    r.raise_for_status()

# â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸
def main():
    logging.basicConfig(level=logging.WARNING)
    seen = sync_seen(load_seen())
    links = fetch_links()

    todo = [u for u in links if norm(u) not in seen and not wp_exists(norm(u))]
    print(f"ğŸ“° ìƒˆ ê¸°ì‚¬ {len(todo)} / ì´ {len(links)}")

    for url in todo:
        print("===", url)
        art = parse(url)
        if not art:
            continue

        try:
            txt = rewrite(art)
        except Exception as e:
            print("  GPT ì˜¤ë¥˜:", e)
            continue

        tag_ids = [tid for n in tag_names(txt) if (tid := tag_id(n))]
        try:
            publish(art, txt, tag_ids)
            seen.add(norm(url))
            save_seen(seen)
        except Exception as e:
            print("  ì—…ë¡œë“œ ì‹¤íŒ¨:", e)

        time.sleep(2)  # UDF ì„œë²„Â·WP ì„œë²„ ë³´í˜¸ìš© ë”œë ˆì´

if __name__ == "__main__":
    main()
