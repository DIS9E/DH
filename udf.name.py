import requests
import json
import os
from bs4 import BeautifulSoup, NavigableString, Tag
from urllib.parse import urljoin

# âœ… í™˜ê²½ë³€ìˆ˜ë¡œë¶€í„° ì¸ì¦ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°
WP_USERNAME = os.getenv("WP_USERNAME")
WP_APP_PASSWORD = os.getenv("WP_APP_PASSWORD")
WP_API_URL = "https://belatri.info/wp-json/wp/v2/posts"
TAG_API_URL = "https://belatri.info/wp-json/wp/v2/tags"

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
UDF_BASE_URL = "https://udf.name/news/"
HEADERS = {"User-Agent": "Mozilla/5.0"}
SEEN_FILE = "seen_urls.json"

# âœ… ì´ì „ì— ë³¸ URL ë¡œë”©
def load_seen_urls():
    if os.path.exists(SEEN_FILE):
        with open(SEEN_FILE, "r", encoding="utf-8") as f:
            return set(json.load(f))
    return set()

def save_seen_urls(urls):
    with open(SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump(list(urls), f, ensure_ascii=False, indent=2)

# âœ… ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
def get_article_links():
    response = requests.get(UDF_BASE_URL, headers=HEADERS)
    if response.status_code != 200:
        print(f"âŒ ë©”ì¸ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    links = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if (
            href.startswith("https://udf.name/news/") and
            href.endswith(".html") and
            href.count("/") >= 5
        ):
            links.append(href)
    return list(set(links))

# âœ… ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ
def extract_article(url):
    response = requests.get(url, headers=HEADERS)
    if response.status_code != 200:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {url} | {response.status_code}")
        return None

    soup = BeautifulSoup(response.text, "html.parser")

    title = soup.find("h1", class_="newtitle")
    title_text = title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ"

    author = soup.find("div", class_="author")
    author_text = author.get_text(strip=True) if author else "ì¶œì²˜ ì—†ìŒ"

    image = soup.find("img", class_="lazy")
    image_url = "https://udf.name" + image["data-src"] if image else None

    content_block = soup.find("div", id="zooming")
    content_lines = []
    if content_block:
        for el in content_block.descendants:
            if isinstance(el, NavigableString):
                txt = el.strip()
                if txt:
                    content_lines.append(txt)
            elif isinstance(el, Tag) and el.name in ["p", "br"]:
                content_lines.append("\n")

    content = "\n".join(line for line in content_lines if line.strip())
    content = content.replace("dle_leech_begin", "").replace("dle_leech_end", "").strip()

    return {
        "title": title_text,
        "author": author_text,
        "image": image_url,
        "url": url,
        "content": content
    }

# âœ… ChatGPT ë¦¬ë¼ì´íŒ… + íƒœê·¸ ì¶”ì¶œ
def rewrite_with_chatgpt(article):
    prompt = f"""
ë‹¤ìŒì€ ë²¨ë¼ë£¨ìŠ¤ ê´€ë ¨ ì™¸ì‹  ê¸°ì‚¬ì…ë‹ˆë‹¤. ì•„ë˜ ì–‘ì‹ì— ë§ì¶° í•œêµ­ ë…ìë¥¼ ìœ„í•œ ë¸”ë¡œê·¸ ê²Œì‹œê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ğŸ¯ ì‘ì„± ì¡°ê±´:
- ê¸°ì‚¬ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ìš”ì•½í•˜ê±°ë‚˜ í•´ì„í•˜ì§€ ë§ê³ **, **ë¬¸ì²´ì™€ êµ¬ì¡°ë§Œ ë°”ê¿”ì„œ ì¬ì‘ì„±**í•´ì£¼ì„¸ìš”.
- **ê¸°ì‚¬ì˜ ì •ë³´ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€**í•˜ê³ , **í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ê°€ë…ì„± ë†’ê²Œ** ì‘ì„±í•´ì£¼ì„¸ìš”.
- **ì œëª©(H1), ë¶€ì œ(H2), ë‚´ìš© ë¬¸ë‹¨(H3)** ë“±ìœ¼ë¡œ êµ¬ë¶„í•´ ë¸”ë¡œê·¸ì— ìµœì í™”ëœ êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
- **ë³¸ë¬¸ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ íƒœê·¸ìš© í‚¤ì›Œë“œë¡œ í•¨ê»˜ ì œê³µ**í•´ì£¼ì„¸ìš”.

ğŸ“° ê¸°ì‚¬ ì›ë¬¸:
{article["content"]}
    """
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "gpt-4o",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.4
    }
    res = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload)
    result = res.json()["choices"][0]["message"]["content"]
    return result

# âœ… íƒœê·¸ ìë™ ë“±ë¡ (ì¤‘ë³µ í™•ì¸ í¬í•¨)
def create_or_get_tag_id(tag_name):
    response = requests.get(TAG_API_URL, params={"search": tag_name})
    if response.status_code == 200 and response.json():
        return response.json()[0]["id"]
    res = requests.post(TAG_API_URL,
        auth=(WP_USERNAME, WP_APP_PASSWORD),
        json={"name": tag_name})
    if res.status_code == 201:
        return res.json()["id"]
    return None

# âœ… WordPress ì—…ë¡œë“œ
def post_to_wordpress(title, content, tags):
    data = {
        "title": title,
        "content": content,
        "status": "publish",
        "tags": tags  # âœ… íƒœê·¸ í¬í•¨
    }
    res = requests.post(WORDPRESS_API_URL, headers=HEADERS, json=data)

    print(f"ğŸ“¡ [ì‘ë‹µ ì½”ë“œ] {res.status_code}")
    print(f"ğŸ“¨ [ì‘ë‹µ ë³¸ë¬¸] {res.text[:500]}")

    return res.status_code == 201

# âœ… ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸ” í¬ë¡¤ë§ ì‹œì‘")
    seen = load_seen_urls()
    all_links = get_article_links()
    new_links = [link for link in all_links if link not in seen]
    print(f"ğŸ“° ìƒˆ ê¸°ì‚¬ ìˆ˜: {len(new_links)}")

    for url in new_links:
        article = extract_article(url)
        if not article or not article["content"]:
            continue
        rewritten = rewrite_with_chatgpt(article)

        # ğŸ¯ GPT ì¶œë ¥ì—ì„œ íƒœê·¸ ì¶”ì¶œ (ì˜ˆ: "- ì´ˆì  í‚¤í”„ë ˆì´ì¦ˆ: ë²¨ë¼ë£¨ìŠ¤ ê²½ì œ ìœ„ê¸°")
        tag_lines = [line for line in rewritten.splitlines() if "ì´ˆì  í‚¤í”„ë ˆì´ì¦ˆ:" in line]
        tags = tag_lines[0].split(":", 1)[1].strip().split() if tag_lines else []

        success = post_to_wordpress(article["title"], rewritten, tags)
        if success:
            print(f"âœ… ì—…ë¡œë“œ ì„±ê³µ: {article['title']}")
            seen.add(url)
        else:
            print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {article['title']}")

    save_seen_urls(seen)
    print("âœ… ì‘ì—… ì™„ë£Œ")
