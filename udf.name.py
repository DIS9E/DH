#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
udf.name.py â€“ v3.9-debug2  (ë¡±í¼Â·ì²´ë¥˜ì‹œê°„ ê°•í™” + í•‘ë°± ì°¨ë‹¨ + ë””ë²„ê·¸)
"""

import os, sys, re, json, time, logging
from urllib.parse import urljoin, urlparse, urlunparse
import requests
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WP_URL   = os.getenv("WP_URL", "https://belatri.info").rstrip("/")
USER     = os.getenv("WP_USERNAME")
APP_PW   = os.getenv("WP_APP_PASSWORD")
OPEN_KEY = os.getenv("OPENAI_API_KEY")
if not all([USER, APP_PW, OPEN_KEY]):
    sys.exit("âŒ  WP_USERNAME / WP_APP_PASSWORD / OPENAI_API_KEY ëˆ„ë½")

POSTS_API = f"{WP_URL}/wp-json/wp/v2/posts"
TAGS_API  = f"{WP_URL}/wp-json/wp/v2/tags"

UDF_BASE  = "https://udf.name/news/"
HEADERS   = {"User-Agent": "UDFCrawler/3.9-debug2"}
SEEN_FILE = "seen_urls.json"
TARGET_CAT_ID = 20
norm = lambda u: urlunparse(urlparse(u)._replace(query="", params="", fragment=""))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ seen â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_seen(): return set(json.load(open(SEEN_FILE))) if os.path.exists(SEEN_FILE) else set()
def save_seen(s): json.dump(list(s), open(SEEN_FILE, "w"), ensure_ascii=False, indent=2)
def wp_exists(u): return requests.get(POSTS_API, params={"search":u,"per_page":1},
                                      auth=(USER,APP_PW),timeout=10).json()
def sync_seen(seen): 
    synced={u for u in seen if wp_exists(norm(u))}
    if synced!=seen: save_seen(synced)
    return synced

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë§í¬ í¬ë¡¤ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_links():
    html=requests.get(UDF_BASE,headers=HEADERS,timeout=10).text
    soup=BeautifulSoup(html,"html.parser")
    return list({norm(urljoin(UDF_BASE,a["href"])) 
                 for a in soup.select("div.article1 div.article_title_news a[href]")})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse(url):
    r=requests.get(url,headers=HEADERS,timeout=10)
    if not r.ok: return None
    s=BeautifulSoup(r.text,"html.parser")
    t=s.find("h1",class_="newtitle"); b=s.find("div",id="zooming")
    if not(t and b): return None
    img=s.find("img",class_="lazy") or s.find("img")
    src=None
    if img:
        src=img.get("data-src") or img.get("src")
        if src and ("placeholder" in src or "default" in src): src=None
    cat=url.split("/news/")[1].split("/")[0]
    return {"title":t.get_text(strip=True),"html":str(b),"image":urljoin(url,src) if src else None,
            "url":url,"cat":cat}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì™¸ë¶€ ë¸Œë¦¬í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_brief(cat,headline):
    s=[]
    try:
        rss=requests.get("https://www.reuters.com/rssFeed/ru/businessNews",timeout=10).text
        s+=[f"â€¢ Reuters: {t}" for t in re.findall(r"<title>(.*?)</title>",rss)[1:3]]
    except: pass
    if cat=="economic":
        try:r=requests.get("https://www.nbrb.by/api/exrates/rates/usd?parammode=2",timeout=10).json()
        ;s.append(f"â€¢ NBRB <a href='https://www.nbrb.by'>USD/BLR</a> {r['Cur_OfficialRate']} ({r['Date'][:10]})")
        except: pass
    else:
        try:
            bbc=requests.get("https://feeds.bbci.co.uk/news/world/rss.xml",timeout=10).text
            s.append("â€¢ BBC: "+re.search(r"<title>(.*?)</title>",bbc).group(1))
        except: pass
        try:
            eia=requests.get("https://api.eia.gov/series/?api_key=DEMO_KEY&series_id=PET.RWTC.D",timeout=10).json()
            s.append(f"â€¢ <a href='https://www.eia.gov'>WTI</a> ${eia['series'][0]['data'][0][1]}")
        except: pass
    s.append(f"â€¢ í—¤ë“œë¼ì¸ í‚¤ì›Œë“œ: {headline[:60]}")
    return "\n".join(s)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STYLE_GUIDE (ìš”ì•½) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STYLE_GUIDE="""<h1>ğŸ“° (ì´ëª¨ì§€) í¥ë¯¸ë¡œìš´ í•œêµ­ì–´ ì œëª©</h1>
<h2>âœï¸ í¸ì§‘ì ì£¼ â€” ê¸°ì‚¬ í•µì‹¬ì„ 2ë¬¸ì¥</h2>
<h3>ğŸ“Š ìµœì‹  ë°ì´í„°</h3><p>(extra_context ìˆ«ìÂ·ë§í¬, <strong>500ìâ†‘</strong>)</p>
<h3>ğŸ’¬ ì „ë¬¸ê°€ ì „ë§</h3><p>(ì‹œë‚˜ë¦¬ì˜¤Â·ìˆ«ìÂ·ê¸°ê´€ ì¸ìš©, <strong>500ìâ†‘</strong>)</p>
<h3>â“ Q&A</h3><ul><li>Q1â€¦?<br><strong>A.</strong> â€¦</li><li>Q2â€¦?<br><strong>A.</strong> â€¦</li><li>Q3â€¦?<br><strong>A.</strong> â€¦</li></ul>
<h3>(ë³¸ë¬¸ í•´ì„¤)</h3><p>ì›ë¬¸ ë¬¸ì¥ ëª¨ë‘ ìì—°ìŠ¤ëŸ½ê²Œ ì¬ë°°ì¹˜â€¦</p>
<p>ğŸ·ï¸ íƒœê·¸: ëª…ì‚¬ 3â€“6ê°œ</p>
<p>ì´ ê¸°ì‚¬ëŠ” ë²¨ë¼ë£¨ìŠ¤ í˜„ì§€ ë³´ë„ë¥¼ ì¬êµ¬ì„±í•œ ì½˜í…ì¸ ì…ë‹ˆë‹¤.<br>by. ì—ë””í„° LEEğŸŒ³</p>"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPT í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chat(prompt,max_tok=1800,temp=0.5,model="gpt-4o"):
    h={"Authorization":f"Bearer {OPEN_KEY}","Content-Type":"application/json"}
    d={"model":model,"messages":[{"role":"user","content":prompt}],
       "temperature":temp,"max_tokens":max_tok}
    r=requests.post("https://api.openai.com/v1/chat/completions",
                    headers=h,json=d,timeout=120)
    r.raise_for_status(); return r.json()["choices"][0]["message"]["content"].strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¦¬ë¼ì´íŒ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rewrite(art):
    extra=build_brief(art['cat'],art['title'])
    prompt=f"{STYLE_GUIDE}\n\nâ—† ì›ë¬¸:\n{art['html']}\n\nâ—† extra_context:\n{extra}"
    txt=chat(prompt); logging.debug("GPT raw >>> %s â€¦",txt[:300].replace('\n',' '))
    return txt

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì œëª© ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CYRILLIC=re.compile(r"[Ğ-Ğ¯Ğ°-ÑĞÑ‘]")
def korean_title(src,ctx):
    if not CYRILLIC.search(src): return src
    return chat(f"ë‹¤ìŒ ì œëª©ì„ í•œêµ­ì–´ ì¹´í”¼ë¼ì´í„° ìŠ¤íƒ€ì¼(45ìâ†“, ì´ëª¨ì§€ 1â€“3ê°œ)ë¡œ:\nÂ«{src}Â»\në¬¸ë§¥:{ctx[:200]}",
                max_tok=60,temp=0.9,model="gpt-4o-mini")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ íƒœê·¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STOP={"ë²¨ë¼ë£¨ìŠ¤","ë‰´ìŠ¤","ê¸°ì‚¬"}
def tag_names(txt):
    m=re.search(r"ğŸ·ï¸.*[:ï¼š]\s*(.+)",txt); out=[]
    if m:
        for t in re.split(r"[,\s]+",m.group(1)):
            t=t.strip("â€“-#â€¢"); 
            if 1<len(t)<=20 and t not in STOP and t not in out: out.append(t)
    return out[:6]
def tag_id(name):
    q=requests.get(TAGS_API,params={"search":name,"per_page":1},
                   auth=(USER,APP_PW),timeout=10).json()
    if q: return q[0]["id"]
    c=requests.post(TAGS_API,json={"name":name},
                    auth=(USER,APP_PW),timeout=10)
    return c.json()["id"] if c.status_code==201 else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸¸ì´Â·í—¤ë” ê°€ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ensure_long(html,title):
    soup=BeautifulSoup(html,"html.parser")
    for t in soup.find_all(string=True):
        if title.strip() in t.strip(): t.extract()
    for blk in soup.find_all(["p","ul"]):
        if len(blk.get_text())<500:
            try:
                blk.clear()
                blk.append(BeautifulSoup(
                    chat(f"ë¬¸ë‹¨ì„ ê·¼ê±°Â·ìˆ«ìÂ·ì „ë§ í¬í•¨ 500ìâ†‘ í™•ì¥:\n{blk}",
                         max_tok=200,temp=0.7,model="gpt-4o-mini"),"html.parser"))
            except: pass
    return str(soup)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²Œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def publish(art,txt,tags):
    logging.debug("before guard len=%d",len(txt))
    txt=ensure_long(txt,art["title"])
    logging.debug("after guard len=%d",len(txt))

    hidden=f'<a href="{art["url"]}" style="display:none">src</a>\n'
    img_tag=f'<p><img src="{art["image"]}" alt=""></p>\n' if art["image"] else ""
    soup=BeautifulSoup(txt,"html.parser")

    h1=soup.find("h1"); orig=h1.get_text(strip=True) if h1 else art["title"]
    title=korean_title(orig,soup.get_text())
    if h1:h1.decompose()
    new_h1=soup.new_tag("h1"); new_h1.string=title; soup.insert(0,new_h1)
    logging.debug("after header has_h1=%s",bool(soup.find("h1")))

    if img_tag:
        img=soup.find("img"); cap=soup.new_tag("em"); cap.string="Photo: UDF.name"
        img.insert_after(cap)

    body=hidden+img_tag+str(soup)
    payload={"title":title,"content":body,"status":"publish",
             "categories":[TARGET_CAT_ID],"tags":tags,"ping_status":"closed"}  # ğŸ”’ í•‘ë°± ì°¨ë‹¨
    r=requests.post(POSTS_API,json=payload,auth=(USER,APP_PW),timeout=30)
    logging.info("â†³ ê²Œì‹œ %s %s",r.status_code,r.json().get("id"));
    r.raise_for_status()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    logging.basicConfig(level=logging.DEBUG,
        format="%(asctime)s â”‚ %(levelname)s â”‚ %(message)s",datefmt="%Y-%m-%d %H:%M:%S")
    seen=sync_seen(load_seen()); links=fetch_links()
    todo=[u for u in links if norm(u) not in seen and not wp_exists(norm(u))]
    logging.info("ğŸ“° ìƒˆ ê¸°ì‚¬ %d / ì´ %d",len(todo),len(links))
    for url in todo:
        logging.info("â–¶ %s",url); art=parse(url); time.sleep(1)
        if not art: continue
        try: txt=rewrite(art)
        except Exception as e: logging.warning("GPT ì˜¤ë¥˜:%s",e); continue
        tag_ids=[tid for n in tag_names(txt) if (tid:=tag_id(n))]
        try: publish(art,txt,tag_ids); seen.add(norm(url)); save_seen(seen)
        except Exception as e: logging.warning("ì—…ë¡œë“œ ì‹¤íŒ¨:%s",e)
        time.sleep(1.5)

if __name__=="__main__": main()
