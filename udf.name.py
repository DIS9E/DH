#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
udf.name.py â€“ v4.1.0  (í—¤ë“œë¼ì´íŠ¸Â·AdSense ëŒ€ì‘)
â€¢ WPâ†”seen ë™ê¸°í™”  â€¢ ì´ë¯¸ì§€ ì‚½ì… â€¢ ìë™ íƒœê·¸ â€¢ ì™¸ë¶€ ë°ì´í„° ì£¼ì… â€¢ ê¸¸ì´ Guard
"""

import os, sys, re, json, time, logging, random, textwrap
from datetime import datetime
from zoneinfo import ZoneInfo
from urllib.parse import urljoin, urlparse, urlunparse

import requests
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WP_URL   = os.getenv("WP_URL", "https://belatri.info").rstrip("/")
USER     = os.getenv("WP_USERNAME")
APP_PW   = os.getenv("WP_APP_PASSWORD")
OPEN_KEY = os.getenv("OPENAI_API_KEY")
if not all([USER, APP_PW, OPEN_KEY]):
    sys.exit("âŒ  WP_USERNAME / WP_APP_PASSWORD / OPENAI_API_KEY ëˆ„ë½")

POSTS_API = f"{WP_URL}/wp-json/wp/v2/posts"
TAGS_API  = f"{WP_URL}/wp-json/wp/v2/tags"

UDF_BASE  = "https://udf.name/news/"
HEADERS   = {"User-Agent": "UDFCrawler/4.1.0"}
SEEN_FILE = "seen_urls.json"
TARGET_CAT_ID = 20

norm = lambda u: urlunparse(urlparse(u)._replace(query="", params="", fragment=""))

# â”€â”€ Helper: ë©”íƒ€ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_meta():
    today = datetime.now(tz=ZoneInfo("Asia/Seoul")).strftime("%Y.%m.%d")
    reads = f"{random.randint(7_000, 12_000):,}"
    return f"<p class='meta'>í—¤ë“œë¼ì´íŠ¸ â€¢ {today} â€¢ ì½ìŒ {reads}</p>"

# â”€â”€ seen íŒŒì¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_seen():  # set[str]
    return set(json.load(open(SEEN_FILE))) if os.path.exists(SEEN_FILE) else set()

def save_seen(s: set[str]):
    json.dump(list(s), open(SEEN_FILE, "w"), ensure_ascii=False, indent=2)

# â”€â”€ WP ì¤‘ë³µ ê²€ì‚¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def wp_exists(u_norm):
    r = requests.get(POSTS_API, params={"search": u_norm, "per_page": 1},
                     auth=(USER, APP_PW), timeout=10)
    return r.ok and bool(r.json())

def sync_seen(seen):
    synced = {u for u in seen if wp_exists(norm(u))}
    if synced != seen:
        save_seen(synced)
    return synced

# â”€â”€ ë§í¬ í¬ë¡¤ëŸ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_links():
    soup = BeautifulSoup(requests.get(UDF_BASE, headers=HEADERS, timeout=10).text,
                         "html.parser")
    return list({norm(urljoin(UDF_BASE, a["href"]))
                 for a in soup.select("div.article1 div.article_title_news a[href]")})

# â”€â”€ ê¸°ì‚¬ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse(url):
    r = requests.get(url, headers=HEADERS, timeout=10)
    if not r.ok: return None
    s = BeautifulSoup(r.text, "html.parser")
    title_tag = s.find("h1", class_="newtitle")
    body_tag  = s.find("div", id="zooming")
    if not (title_tag and body_tag): return None
    img = s.find("img", class_="lazy") or s.find("img")
    return {
        "title": title_tag.get_text(strip=True),
        "html":  str(body_tag),
        "image": urljoin(url, img.get("data-src") or img.get("src")) if img else None,
        "url":   url
    }

# â”€â”€ STYLE_GUIDE (âŸªMETAâŸ«, âŸªRAW_HTMLâŸ« í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš©) â”€â”€â”€â”€â”€â”€â”€â”€
STYLE_GUIDE = textwrap.dedent("""
<h1>(ì´ëª¨ì§€ 1â€“3ê°œ) í¥ë¯¸ë¡œìš´ í•œêµ­ì–´ ì œëª©</h1>

<h2>âœï¸ í¸ì§‘ì ì£¼ â€” ê¸°ì‚¬ í•µì‹¬ 2ë¬¸ì¥</h2>
<h3>ğŸ“Š ìµœì‹  ë°ì´í„°</h3><p><ul>
  <li>í™˜ìœ¨Â·ìœ ê°€Â·í—¤ë“œë¼ì¸ ë“± ìµœì†Œ 4~6ì¤„</li>
</ul></p>
<h3>ğŸ’¬ ì „ë¬¸ê°€ ì „ë§</h3><p>ê·¼ê±°Â·ìˆ«ì í¬í•¨ ë¶„ì„ 2ë¬¸ë‹¨(500ìâ†‘)</p>
<h3>â“ Q&A</h3>
<ul><li><strong>Q1.</strong> â€¦?<br><strong>A.</strong> â€¦</li>
<li><strong>Q2.</strong> â€¦?<br><strong>A.</strong> â€¦</li>
<li><strong>Q3.</strong> â€¦?<br><strong>A.</strong> â€¦</li></ul>
<h3>(ë³¸ë¬¸ í•´ì„¤)</h3><p>ì›ë¬¸ 100 % ì¬ë°°ì¹˜</p>
<p>ğŸ·ï¸ íƒœê·¸: ëª…ì‚¬ 3â€“6ê°œ</p>
<p>ì¶œì²˜: UDF.name ì›ë¬¸<br>Photo: UDF.name<br>
by. ì—ë””í„° LEEğŸŒ³<br><em>* ìƒì„±í˜• AIì˜ ë„ì›€ìœ¼ë¡œ ì‘ì„±.</em></p>

<p class="related">ğŸ“š ê´€ë ¨ ê¸°ì‚¬ ë” ë³´ê¸°</p>
""").strip()

# â”€â”€ OpenAI ë¦¬ë¼ì´í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rewrite(article):
    prompt = STYLE_GUIDE.replace("âŸªMETAâŸ«", build_meta())\
                        .replace("âŸªRAW_HTMLâŸ«", article["html"])
    headers={"Authorization":f"Bearer {OPEN_KEY}",
             "Content-Type":"application/json"}
    data = {
        "model": "gpt-4o",
        "messages":[{"role":"user", "content": prompt}],
        "temperature":0.4,
        "max_tokens": 2300
    }
    r = requests.post("https://api.openai.com/v1/chat/completions",
                      headers=headers, json=data, timeout=90)
    r.raise_for_status()
    txt = r.json()["choices"][0]["message"]["content"].strip()

    # ê¸¸ì´ guard: 1 500ì ë¯¸ë§Œì´ë©´ í•œ ë²ˆ ë” ìš”ì²­
    if len(txt) < 1500:
        logging.info("  â†º ê¸¸ì´ ë³´ê°• ì¬-ìš”ì²­")
        data["temperature"] = 0.6
        r2 = requests.post("https://api.openai.com/v1/chat/completions",
                           headers=headers, json=data, timeout=90)
        r2.raise_for_status()
        txt = r2.json()["choices"][0]["message"]["content"].strip()
    return txt

# â”€â”€ íƒœê·¸ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STOP = {"ë²¨ë¼ë£¨ìŠ¤", "ë‰´ìŠ¤", "ê¸°ì‚¬"}
def tag_names(txt):
    m = re.search(r"ğŸ·ï¸\s*íƒœê·¸[^:ï¼š]*[:ï¼š]\s*(.+)", txt)
    if not m: return []
    out=[]
    for t in re.split(r"[,\s]+", m.group(1)):
        t=t.strip("â€“-#â€¢")
        if 1<len(t)<=20 and t not in STOP and t not in out:
            out.append(t)
        if len(out)==6: break
    return out

def tag_id(name):
    q = requests.get(TAGS_API, params={"search":name,"per_page":1},
                     auth=(USER,APP_PW), timeout=10)
    if q.ok and q.json(): return q.json()[0]["id"]
    r = requests.post(TAGS_API, json={"name":name},
                      auth=(USER,APP_PW), timeout=10)
    return r.json()["id"] if r.status_code==201 else None

# â”€â”€ ê²Œì‹œ (ì œëª© ì¤‘ë³µ H1 ì œê±° & ê´€ë ¨ ë§í¬) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def publish(article, txt, tag_ids):
    soup = BeautifulSoup(txt, "html.parser")

    h1 = soup.find("h1")
    title = (h1.get_text(" ", strip=True) if h1 else article["title"]).lstrip("ğŸ“°").strip()
    if h1: h1.decompose()

    # ê´€ë ¨ ê¸°ì‚¬(ë™ì¼ íƒœê·¸ ìµœì‹  3) â€“ íƒœê·¸ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
    related_html = ""
    if tag_ids:
        r = requests.get(POSTS_API,
                         params={"tags": ",".join(map(str,tag_ids)),
                                 "per_page": 3, "exclude":0, "status":"publish"},
                         auth=(USER,APP_PW), timeout=10)
        if r.ok and r.json():
            lst = [f"<li><a href='{p['link']}'>{p['title']['rendered']}</a></li>"
                   for p in r.json()]
            related_html = "<ul>" + "".join(lst) + "</ul>"
    if related_html:
        soup.find("p", class_="related").append(BeautifulSoup(related_html,"html.parser"))

    hidden = f'<a href="{article["url"]}" style="display:none">src</a>\n'
    img_tag = f'<p><img src="{article["image"]}" alt=""></p>\n' if article["image"] else ""
    body = hidden + img_tag + str(soup)

    payload = {
        "title": title,
        "content": body,
        "status": "publish",
        "categories": [TARGET_CAT_ID],
        "tags": tag_ids
    }
    r = requests.post(POSTS_API, json=payload, auth=(USER,APP_PW), timeout=30)
    if r.status_code == 201:
        logging.info("  â†³ ê²Œì‹œ 201 %s", r.json()["id"])
    else:
        logging.warning("  â†³ ì—…ë¡œë“œ ì‹¤íŒ¨ %s %s", r.status_code, r.text)
    r.raise_for_status()

# â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    logging.basicConfig(level=logging.INFO,
        format="%(asctime)s â”‚ %(levelname)s â”‚ %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S")
    logging.info("ğŸš€ ë²„ì „ 4.1.0 ì‹¤í–‰")

    seen = sync_seen(load_seen())
    links = fetch_links()

    todo = [u for u in links if norm(u) not in seen and not wp_exists(norm(u))]
    logging.info("ğŸ“° ìƒˆ ê¸°ì‚¬ %d / ì´ %d", len(todo), len(links))

    for url in todo:
        logging.info("â–¶ %s", url)
        art = parse(url)
        if not art: continue

        try:
            txt = rewrite(art)
        except Exception as e:
            logging.warning("GPT ì˜¤ë¥˜: %s", e); continue

        tag_ids=[tid for n in tag_names(txt) if (tid:=tag_id(n))]
        try:
            publish(art, txt, tag_ids)
            seen.add(norm(url)); save_seen(seen)
        except Exception as e:
            logging.warning("ì—…ë¡œë“œ ì‹¤íŒ¨: %s", e)

        time.sleep(1.2)

if __name__ == "__main__":
    main()
