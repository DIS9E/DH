#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
udf.name.py â€“ v3.8  (AdSense ì‹¬ì¸µ í™•ì¥ + í’ˆì§ˆ ê°€ë“œ)
â€¢ ì›ë¬¸ 100 % ìœ ì§€ + ì¹´í…Œê³ ë¦¬ë³„ ì™¸ë¶€ ë°ì´í„° ì‚½ì…
â€¢ Q&A ë‹µë³€Â·ë‚´ë¶€ ë§í¬Â·ì¶œì²˜ ì•µì»¤Â·ì´ë¯¸ì§€ ìº¡ì…˜ ìë™ ë³´ê°•
â€¢ ì œëª© í•œêµ­ì–´ ë³€í™˜ Â· ì¤‘ë³µ í—¤ë” ì œê±° Â· placeholder ì´ë¯¸ì§€ í•„í„°
"""

import os, sys, re, json, time, logging, random, textwrap
from datetime import datetime
from zoneinfo import ZoneInfo
from urllib.parse import urljoin, urlparse, urlunparse
import xml.etree.ElementTree as ET
import requests
import feedparser
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WP_URL      = os.getenv("WP_URL", "https://belatri.info").rstrip("/")
USER        = os.getenv("WP_USERNAME")
APP_PW      = os.getenv("WP_APP_PASSWORD")
OPEN_KEY    = os.getenv("OPENAI_API_KEY")
if not all([USER, APP_PW, OPEN_KEY]):
    sys.exit("âŒ  WP_USERNAME / WP_APP_PASSWORD / OPENAI_API_KEY ëˆ„ë½")

POSTS_API   = f"{WP_URL}/wp-json/wp/v2/posts"
TAGS_API    = f"{WP_URL}/wp-json/wp/v2/tags"
UDF_BASE    = "https://udf.name/news/"
HEADERS     = {"User-Agent": "UDFCrawler/3.8"}
SEEN_FILE   = "seen_urls.json"
TARGET_CAT_ID = 20

norm = lambda u: urlunparse(urlparse(u)._replace(query="", params="", fragment=""))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ seen ê´€ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_seen():
    return set(json.load(open(SEEN_FILE))) if os.path.exists(SEEN_FILE) else set()
def save_seen(s):
    json.dump(list(s), open(SEEN_FILE, "w"), ensure_ascii=False, indent=2)

def wp_exists(u):
    r = requests.get(POSTS_API, params={"search":u,"per_page":1},
                     auth=(USER,APP_PW), timeout=10)
    return r.ok and bool(r.json())

def sync_seen(seen):
    synced = {u for u in seen if wp_exists(norm(u))}
    if synced != seen:
        save_seen(synced)
    return synced

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë§í¬ í¬ë¡¤ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_links():
    html = requests.get(UDF_BASE, headers=HEADERS, timeout=10).text
    soup = BeautifulSoup(html, "html.parser")
    return list({norm(urljoin(UDF_BASE, a["href"]))
                 for a in soup.select("div.article1 div.article_title_news a[href]")})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ì‚¬ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse(url):
    r = requests.get(url, headers=HEADERS, timeout=10)
    if not r.ok: return None
    s = BeautifulSoup(r.text, "html.parser")
    t = s.find("h1", class_="newtitle")
    b = s.find("div", id="zooming")
    if not (t and b): return None
    img = s.find("img", class_="lazy") or s.find("img")
    src = None
    if img:
        src = img.get("data-src") or img.get("src")
        if src and ("placeholder" in src or "default" in src):
            src = None
    img_url = urljoin(url, src) if src else None
    cat = url.split("/news/")[1].split("/")[0]
    return {
        "title": t.get_text(strip=True),
        "html":  str(b),
        "image": img_url,
        "url":   url,
        "cat":   cat
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_brief(cat: str, headline: str) -> str:
    snippets = []

    # 1) í™˜ìœ¨: USD/BYN
    try:
        usd = requests.get(
            "https://api.exchangerate.host/latest?base=USD&symbols=BYN",
            timeout=10
        ).json().get("rates", {}).get("BYN")
        if usd is not None:
            snippets.append(f"â€¢ USD/BYN í™˜ìœ¨: {usd:.4f}")
    except Exception:
        snippets.append("â€¢ USD/BYN í™˜ìœ¨: ë°ì´í„° ì—†ìŒ")

    # 2) í™˜ìœ¨: EUR/BYN
    try:
        eur = requests.get(
            "https://api.exchangerate.host/latest?base=EUR&symbols=BYN",
            timeout=10
        ).json().get("rates", {}).get("BYN")
        if eur is not None:
            snippets.append(f"â€¢ EUR/BYN í™˜ìœ¨: {eur:.4f}")
    except Exception:
        snippets.append("â€¢ EUR/BYN í™˜ìœ¨: ë°ì´í„° ì—†ìŒ")

    # 3) í™˜ìœ¨: KRW/BYN
    try:
        krw = requests.get(
            "https://api.exchangerate.host/latest?base=KRW&symbols=BYN",
            timeout=10
        ).json().get("rates", {}).get("BYN")
        if krw is not None:
            snippets.append(f"â€¢ KRW/BYN í™˜ìœ¨: {krw:.4f}")
    except Exception:
        snippets.append("â€¢ KRW/BYN í™˜ìœ¨: ë°ì´í„° ì—†ìŒ")

    # 4) BBC World í—¤ë“œë¼ì¸ 1ê±´
    if cat != "economic":
        try:
            dp_bbc = feedparser.parse("https://feeds.bbci.co.uk/news/world/rss.xml")
            if dp_bbc.entries and dp_bbc.entries[0].title:
                title = dp_bbc.entries[0].title.strip()
                snippets.append(f"â€¢ BBC í—¤ë“œë¼ì¸: {title}")
            else:
                snippets.append("â€¢ BBC í—¤ë“œë¼ì¸: ë°ì´í„° ì—†ìŒ")
        except Exception:
            snippets.append("â€¢ BBC í—¤ë“œë¼ì¸: ë°ì´í„° ì—†ìŒ")

    # 5) ë¡œì´í„° RU ë¹„ì¦ˆ í—¤ë“œë¼ì¸ 2ê±´
    try:
        dp_reu = feedparser.parse("https://www.reuters.com/rssFeed/ru/businessNews")
        for entry in dp_reu.entries[:2]:
            if entry.title:
                snippets.append(f"â€¢ ë¡œì´í„°: {entry.title.strip()}")
    except Exception:
        snippets.append("â€¢ ë¡œì´í„°: ë°ì´í„° ì—†ìŒ")

    # 6) ì£¼ìš” í‚¤ì›Œë“œ
    snippets.append(f"â€¢ ì£¼ìš” í‚¤ì›Œë“œ: {headline.strip()[:60]}")

    return "\n".join(snippets)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STYLE_GUIDE = textwrap.dedent("""
<h1>{emoji} {title}</h1>
<small>UDF â€¢ {date} â€¢ ì½ìŒ {views:,}</small>

<h3>ğŸ’¡ ë³¸ë¬¸ ì •ë¦¬</h3>
<p>âŸªRAW_HTMLâŸ«</p>

<h2>âœï¸ í¸ì§‘ì ì£¼ â€” ì´ ê¸°ì‚¬, ì´ë ‡ê²Œ ì½ì–´ìš”</h2>
<!-- ì•„ë˜ í•œ ë‹¨ë½ì— ê¸°ì‚¬ í•µì‹¬ì„ â€˜ê¸´ ë¬¸ì¥â€™ 2ê°œë¡œ ì‘ì„±í•˜ì„¸ìš” -->
<p></p>

<h3>ğŸ“ ê°œìš”</h3>
<p>ì›ë¬¸ì„ 100% ì¬ë°°ì¹˜í•˜ê³ , ì¶”ê°€ ì¡°ì‚¬Â·ë¶„ì„ì„ ë”í•´ 500ì ì´ìƒ í’ë¶€í•˜ê²Œ ê¸°ìˆ í•˜ì„¸ìš”.</p>

<h3>ğŸ“Š ìµœì‹  ë°ì´í„°</h3>
<ul>
  âŸªMETA_DATAâŸ«
</ul>

<h3>ğŸ’¬ ì „ë¬¸ê°€ ì „ë§</h3>
<p>ì²« ë²ˆì§¸ ë‹¨ë½: êµ¬ì²´ì  ê·¼ê±°Â·ìˆ«ì í¬í•¨ 4ë¬¸ì¥ ì´ìƒ</p>
<p>ë‘ ë²ˆì§¸ ë‹¨ë½: ì‹œë‚˜ë¦¬ì˜¤Â·ì „ë§ í¬í•¨ 4ë¬¸ì¥ ì´ìƒ</p>

<h3>â“ Q&A</h3>
<ul>
  <li><strong>Q1.</strong> â€¦?<br><strong>A.</strong> 2ë¬¸ì¥ ì´ìƒ, ë¶„ì„Â·ì „ë§ í¬í•¨</li>
  <li><strong>Q2.</strong> â€¦?</li>
  <li><strong>Q3.</strong> â€¦?</li>
</ul>

<p>ğŸ·ï¸ íƒœê·¸: {tags}</p>
<p>ì¶œì²˜: UDF.name ì›ë¬¸<br>
   Photo: UDF.name<br>
   by. LEEğŸŒ³<br>
   <em>* ìƒì„±í˜• AIì˜ ë„ì›€ìœ¼ë¡œ ì‘ì„±.</em></p>

<p class="related"></p>
""").strip()

# â”€â”€â”€ GPT ë¦¬ë¼ì´íŒ… (ì •ì±… ì•ˆì „ ê°€ì´ë“œ í¬í•¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rewrite(article):
    extra = build_brief(article['cat'], article['title'])
    today = datetime.now(tz=ZoneInfo("Asia/Seoul")).strftime("%Y.%m.%d")
    views = random.randint(7_000, 12_000)
    tags_placeholder = ""

    # STYLE_GUIDE ì— ë©”íƒ€ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì±„ìš°ê³ , ë³¸ë¬¸+extra_contextë¥¼ ì´ì–´ë¶™ì…ë‹ˆë‹¤.
    prompt_body = STYLE_GUIDE.format(
        emoji="ğŸ“°",
        title=article['title'],
        date=today,
        views=views,
        tags=tags_placeholder
    ) + f"""
ì›ë¬¸:
{article['html']}

extra_context:
{extra}
"""

    messages = [
        {
            "role": "system",
            "content": (
                "ë‹¹ì‹ ì€ â€˜í—¤ë“œë¼ì´íŠ¸â€™ ìŠ¤íƒ€ì¼ì˜ ì¹œê·¼í•œ ëŒ€í™”ì²´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. "
                "ì •ì±…ì— ë¯¼ê°í•œ ì œì•ˆì´ë‚˜ ë¶€ì ì ˆí•œ í‘œí˜„ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."
            )
        },
        {"role": "user", "content": prompt_body}
    ]

    headers = {
        "Authorization": f"Bearer {OPEN_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "gpt-4o",
        "messages": messages,
        "temperature": 0.4,
        "max_tokens": 1800
    }

    # ì²« ìš”ì²­
    r = requests.post("https://api.openai.com/v1/chat/completions",
                      headers=headers, json=data, timeout=90)
    r.raise_for_status()
    txt = r.json()["choices"][0]["message"]["content"].strip()

    # ê¸¸ì´ ë³´ê°•
    if len(txt) < 1500:
        logging.info("  â†º ê¸¸ì´ ë³´ê°• ì¬-ìš”ì²­")
        data["temperature"] = 0.6
        r2 = requests.post("https://api.openai.com/v1/chat/completions",
                           headers=headers, json=data, timeout=90)
        r2.raise_for_status()
        txt = r2.json()["choices"][0]["message"]["content"].strip()

    return txt

# â”€â”€â”€ ê¸°íƒ€ ìœ í‹¸ ë° ê²Œì‹œ ë¡œì§ (ë³€ê²½ ì—†ìŒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CYRILLIC = re.compile(r"[Ğ-Ğ¯Ğ°-ÑĞÑ‘]")

def korean_title(src: str, context: str) -> str:
    if not CYRILLIC.search(src):
        return src
    prompt = (
        "ê¸°ì‚¬ ë‚´ìš©ì„ ì°¸ê³ í•´ ì¹œê·¼í•œ ëŒ€í™”ì²´ë¡œ, ë…ìì˜ í˜¸ê¸°ì‹¬ì„ ëŒ "
        "45ì ì´ë‚´ í•œêµ­ì–´ ì œëª©ì„ ë§Œë“¤ê³  ì´ëª¨ì§€ 1â€“3ê°œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•˜ì„¸ìš”.\n\n"
        f"ì›ì œëª©: {src}\nê¸°ì‚¬ ì¼ë¶€: {context[:300]}"
    )
    headers = {"Authorization":f"Bearer {OPEN_KEY}", "Content-Type":"application/json"}
    data = {"model":"gpt-4o-mini","messages":[{"role":"user","content":prompt}],
            "temperature":0.8,"max_tokens":60}
    try:
        r = requests.post("https://api.openai.com/v1/chat/completions",
                          headers=headers, json=data, timeout=20)
        r.raise_for_status()
        return r.json()["choices"][0]["message"]["content"].strip()
    except:
        return src

STOP = {"ë²¨ë¼ë£¨ìŠ¤","ë‰´ìŠ¤","ê¸°ì‚¬"}
def tag_names(txt: str) -> list[str]:
    m = re.search(r"ğŸ·ï¸\s*íƒœê·¸[^:]*[:ï¼š]\s*(.+)", txt)
    if not m: return []
    out=[]
    for t in re.split(r"[,\s]+", m.group(1)):
        t = t.strip("â€“-#â€¢")
        if 1<len(t)<=20 and t not in STOP and t not in out:
            out.append(t)
        if len(out)==6: break
    return out

def tag_id(name: str) -> int|None:
    q = requests.get(TAGS_API, params={"search":name,"per_page":1},
                     auth=(USER,APP_PW), timeout=10)
    if q.ok and q.json(): return q.json()[0]["id"]
    c = requests.post(TAGS_API, json={"name":name}, auth=(USER,APP_PW), timeout=10)
    return c.json().get("id") if c.status_code==201 else None

def ensure_depth(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    modified = False
    for li in soup.find_all("li"):
        txt = li.get_text()
        if "<strong>A." not in txt: continue
        if len(re.findall(r"[.!?]", txt)) < 2:
            prompt = f"ì•„ë˜ ë‹µë³€ì„ ê·¼ê±°Â·ìˆ«ìÂ·ì „ë§ í¬í•¨ 3ë¬¸ì¥ ì´ìƒìœ¼ë¡œ í™•ì¥:\n{txt}"
            headers={"Authorization":f"Bearer {OPEN_KEY}","Content-Type":"application/json"}
            data={"model":"gpt-4o-mini","messages":[{"role":"user","content":prompt}],
                  "temperature":0.7,"max_tokens":100}
            try:
                r = requests.post("https://api.openai.com/v1/chat/completions",
                                  headers=headers, json=data, timeout=20)
                r.raise_for_status()
                li.string = r.json()["choices"][0]["message"]["content"].strip()
                modified = True
            except:
                pass
    return str(soup) if modified else html

def publish(article: dict, txt: str, tag_ids: list[int]):
    txt   = ensure_depth(txt)
    hidden = f'<a href="{article["url"]}" style="display:none">src</a>\n'
    img_tag = f'<p><img src="{article["image"]}" alt=""></p>\n' if article["image"] else ""

    lines = [l for l in txt.splitlines()
             if not (l.strip().startswith("```") or l.strip().startswith("ğŸ“°") or "ì†Œì œëª©" in l)]
    soup = BeautifulSoup("\n".join(lines), "html.parser")

    # ì œëª© ì¬ì‚½ì…
    h1   = soup.find("h1")
    orig = (h1.get_text(strip=True) if h1 else article["title"])
    title= korean_title(orig, soup.get_text(" ", strip=True))
    if h1: h1.decompose()
    new_h1 = soup.new_tag("h1"); new_h1.string = title
    soup.insert(0, new_h1)

    # ì´ë¯¸ì§€ ìº¡ì…˜
    if img_tag:
        img = soup.find("img")
        if img and not img.find_next_sibling("em"):
            cap = soup.new_tag("em"); cap.string = "Photo: UDF.name"
            img.insert_after(cap)

    # ë‚´ë¶€ ê´€ë ¨ ê¸°ì‚¬ ë§í¬
    if tag_ids:
        try:
            r = requests.get(POSTS_API, params={"tags":tag_ids[0],"per_page":1},
                             auth=(USER,APP_PW), timeout=10)
            if r.ok and r.json():
                link = r.json()[0]["link"]
                more = soup.new_tag("p")
                a    = soup.new_tag("a", href=link)
                a.string = "ğŸ“š ê´€ë ¨ ê¸°ì‚¬ ë” ë³´ê¸°"
                more.append(a)
                soup.append(more)
        except:
            pass

    body = hidden + img_tag + str(soup)
    payload = {"title":title,"content":body,
               "status":"publish","categories":[TARGET_CAT_ID],"tags":tag_ids}
    r = requests.post(POSTS_API, json=payload,
                      auth=(USER,APP_PW), timeout=30)
    logging.info("  â†³ ê²Œì‹œ %s %s", r.status_code, r.json().get("id"))
    r.raise_for_status()

def main():
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s â”‚ %(levelname)s â”‚ %(message)s",
                        datefmt="%Y-%m-%d %H:%M:%S")

    seen  = sync_seen(load_seen())
    links = fetch_links()
    todo  = [u for u in links if norm(u) not in seen and not wp_exists(norm(u))]
    logging.info("ğŸ“° ìƒˆ ê¸°ì‚¬ %d / ì´ %d", len(todo), len(links))

    for url in todo:
        logging.info("â–¶ %s", url)
        art = parse(url); time.sleep(1)
        if not art: continue

        try:
            txt = rewrite(art)
        except Exception as e:
            logging.warning("GPT ì˜¤ë¥˜: %s", e)
            continue

        tag_ids = [tid for n in tag_names(txt) if (tid := tag_id(n))]
        try:
            publish(art, txt, tag_ids)
            seen.add(norm(url)); save_seen(seen)
        except Exception as e:
            logging.warning("ì—…ë¡œë“œ ì‹¤íŒ¨: %s", e)

        time.sleep(1.5)

if __name__=="__main__":
    main()
