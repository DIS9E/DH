#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
udf.name.py â€“ v4.0.0-e  (ë‰´ë‹‰-ìŠ¤íƒ€ì¼ Â· AdSense ëŒ€ì‘ Â· ë””ë²„ê·¸ í¬í•¨)
â€¢ ë£¨íŠ¸ ê¸°ì‚¬ + ì™¸ë¶€ ë°ì´í„° Â· GPT í•´ì„¤
â€¢ ì¤‘ë³µ ë°©ì§€ Â· ìë™ íƒœê·¸ Â· ê´€ë ¨ ê¸°ì‚¬ ë‚´ë¶€ë§í¬
"""

__version__ = "4.0.0-e"

import os, sys, re, json, time, logging
from datetime import datetime as dt
from urllib.parse import urljoin, urlparse, urlunparse
import requests, feedparser
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WP_URL = os.getenv("WP_URL", "https://belatri.info").rstrip("/")
USER   = os.getenv("WP_USERNAME")
APP_PW = os.getenv("WP_APP_PASSWORD")
OPENAI = os.getenv("OPENAI_API_KEY")
if not all([USER, APP_PW, OPENAI]):
    sys.exit("âŒ  WP_USERNAME / WP_APP_PASSWORD / OPENAI_API_KEY ëˆ„ë½")

POSTS_API = f"{WP_URL}/wp-json/wp/v2/posts"
TAGS_API  = f"{WP_URL}/wp-json/wp/v2/tags"

UDF_BASE = "https://udf.name/news/"
HEADERS  = {"User-Agent": "UDFCrawler/4.0.0-e"}
SEEN_FILE = "seen_urls.json"
TARGET_CAT_ID = 20

norm = lambda u: urlunparse(urlparse(u)._replace(query="", params="", fragment=""))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ seen â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_seen():  return set(json.load(open(SEEN_FILE))) if os.path.exists(SEEN_FILE) else set()
def save_seen(s): json.dump(list(s), open(SEEN_FILE, "w"), ensure_ascii=False, indent=2)

def wp_exists(u):
    r = requests.get(POSTS_API, params={"search": u, "per_page": 1},
                     auth=(USER, APP_PW), timeout=10)
    return r.ok and bool(r.json())

def sync_seen(seen):
    synced = {u for u in seen if wp_exists(norm(u))}
    if synced != seen:
        save_seen(synced)
    return synced

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë§í¬ í¬ë¡¤ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_links():
    html = requests.get(UDF_BASE, headers=HEADERS, timeout=10).text
    soup = BeautifulSoup(html, "html.parser")
    return list({norm(urljoin(UDF_BASE, a["href"]))
                 for a in soup.select("div.article1 div.article_title_news a[href]")})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ì‚¬ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse(url):
    r = requests.get(url, headers=HEADERS, timeout=10)
    if not r.ok:
        return None
    s = BeautifulSoup(r.text, "html.parser")
    t = s.find("h1", class_="newtitle")
    b = s.find("div", id="zooming")
    if not (t and b):
        return None
    img = s.find("img", class_="lazy") or s.find("img")
    return {
        "title": t.get_text(strip=True),
        "html": str(b),
        "image": urljoin(url, img.get("data-src") or img.get("src")) if img else None,
        "url": url,
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì™¸ë¶€ ë°ì´í„° (í™˜ìœ¨Â·ìœ ê°€Â·RSS) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_brief():
    s = []
    # â‘  ë²¨ë¼ë£¨ìŠ¤ ë£¨ë¸”/USD
    try:
        r = requests.get("https://api.nbrb.by/exrates/rates/431", timeout=6).json()
        s.append(f"â€¢ NBRB USD/BLR {r['Cur_OfficialRate']} ({r['Date'][:10]})")
    except Exception:
        pass
    # â‘¡ êµ­ì œ ìœ ê°€(WTI) â€“ EIA DEMO
    try:
        r = requests.get("https://api.eia.gov/series/?api_key=DEMO_KEY&series_id=PET.RWTC.D",
                         timeout=6).json()["series"][0]["data"][0]
        s.append(f"â€¢ WTI ì›ìœ  ${r[1]} (EIA {r[0]})")
    except Exception:
        pass
    # â‘¢ BBC World RSS í—¤ë“œë¼ì¸
    try:
        feed = feedparser.parse("https://feeds.bbci.co.uk/news/world/rss.xml")
        for ent in feed.entries[:2]:
            s.append(f"â€¢ BBC: <a href='{ent.link}'>{ent.title}</a>")
    except Exception:
        pass
    return "<br>".join(s)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STYLE_GUIDE = """
â€¢ HTML íƒœê·¸ë§Œ ì‚¬ìš©, ì½”ë“œë¸”ë¡Â·ë°±í‹± ê¸ˆì§€
â€¢ êµ¬ì¡°: <h1>(ì œëª©)</h1> â†’ <h2>(ê°œìš”)</h2> â†’
        <h3>ğŸ“Š ìµœì‹  ë°ì´í„°</h3> â†’ <h3>ğŸ’¬ ì „ë¬¸ê°€ ì „ë§</h3> â†’
        <h3>â“ Q&A</h3> â†’ (ë³¸ë¬¸ í•´ì„¤) â†’ íƒœê·¸Â·ì¶œì²˜
â€¢ ì›ë¬¸ ë¬¸ì¥ì€ 100 % ìœ ì§€, ì¶”ê°€ í•´ì„¤Â·ë°ì´í„°ë¡œ ì „ì²´ ê¸¸ì´ì˜ 40 %â†‘ ìƒˆ í…ìŠ¤íŠ¸
â€¢ ì œëª© 45ìâ†“ í•œêµ­ì–´ + ì´ëª¨ì§€ 1â€“3ê°œ
â€¢ Q&A ê° ë‹µë³€ 2ë¬¸ì¥ ì´ìƒ, ìˆ«ìÂ·ì‹œë‚˜ë¦¬ì˜¤ í¬í•¨
"""

GPT_URL = "https://api.openai.com/v1/chat/completions"
def gpt_chat(messages, model="gpt-4o-mini", temperature=0.7, max_tokens=1024):
    r = requests.post(
        GPT_URL,
        headers={"Authorization": f"Bearer {OPENAI}", "Content-Type": "application/json"},
        json={
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        },
        timeout=90,
    )
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"].strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì œëª© í•œêµ­ì–´ ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CYRILLIC = re.compile(r"[Ğ-Ğ¯Ğ°-ÑĞÑ‘]")
def korean_title(src, context):
    if not CYRILLIC.search(src):
        return src
    prompt = ("ê¸°ì‚¬ ë‚´ìš©ì„ ì°¸ê³ í•´ ë…ìì˜ í˜¸ê¸°ì‹¬ì„ ëŒë©´ì„œë„ ë§¥ë½ì— ì–´ìš¸ë¦¬ëŠ” "
              "í•œêµ­ì–´ ì œëª©(45ìâ†“)ì„ ì‘ì„±í•˜ê³ , ê´€ë ¨ ì´ëª¨ì§€ 1â€“3ê°œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë„£ìœ¼ì„¸ìš”.\n\n"
              f"ì›ì œëª©: {src}\nê¸°ì‚¬ ì¼ë¶€: {context[:360]}")
    return gpt_chat([{"role": "user", "content": prompt}], temperature=0.9, max_tokens=60)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPT ë¦¬ë¼ì´íŒ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rewrite(article, brief):
    user_prompt = f"{STYLE_GUIDE}\n\nâ—† ì™¸ë¶€ ë°ì´í„°:\n{brief}\n\nâ—† ì›ë¬¸:\n{article['html']}"
    return gpt_chat([{"role": "user", "content": user_prompt}], model="gpt-4o")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ íƒœê·¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STOP = {"ë²¨ë¼ë£¨ìŠ¤", "ë‰´ìŠ¤", "ê¸°ì‚¬"}
def tag_names(txt):
    m = re.search(r"ğŸ·ï¸\s*íƒœê·¸[^:ï¼š]*[:ï¼š]\s*(.+)", txt)
    if not m:
        return []
    out = []
    for t in re.split(r"[,\s]+", m.group(1)):
        t = t.strip("â€“-#â€¢")
        if 1 < len(t) <= 20 and t not in STOP and t not in out:
            out.append(t)
        if len(out) == 6:
            break
    return out

def tag_id(name):
    q = requests.get(TAGS_API, params={"search": name, "per_page": 1},
                     auth=(USER, APP_PW), timeout=10)
    if q.ok and q.json():
        return q.json()[0]["id"]
    c = requests.post(TAGS_API, json={"name": name},
                      auth=(USER, APP_PW), timeout=10)
    return c.json()["id"] if c.status_code == 201 else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê´€ë ¨ ê¸€ ë§í¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def related_links(tag_ids, exclude_id=0, limit=3):
    if not tag_ids:
        return ""
    res = requests.get(POSTS_API,
        params={"tags": tag_ids[0], "per_page": limit,
                "exclude": exclude_id, "status": "publish"},
        auth=(USER, APP_PW), timeout=10).json()
    if not isinstance(res, list) or not res:       # â†â˜… ë¹ˆ ë¦¬ìŠ¤íŠ¸ ê°€ë“œ
        return ""
    lis = [f'<li><a href="{p["link"]}">{p["title"]["rendered"]}</a></li>' for p in res]
    return f"<h3>ğŸ“š ê´€ë ¨ ê¸°ì‚¬ ë” ë³´ê¸°</h3><ul>{''.join(lis)}</ul>"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²Œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def publish(article, txt, tag_ids):
    hidden  = f'<a href="{article["url"]}" style="display:none">src</a>\n'
    img_tag = f'<p><img src="{article["image"]}" alt="Photo: UDF.name"></p>\n' if article["image"] else ""

    # ì½”ë“œë¸”ë¡/í”Œë ˆì´ìŠ¤í™€ë” ì œê±°
    lines = []
    for l in txt.splitlines():
        s = l.strip()
        if s.startswith("```") or s.startswith("(ë³¸ë¬¸") or "ê¸°ì‚¬ í•µì‹¬" in s:
            continue
        lines.append(l)
    txt_clean = "\n".join(lines)

    soup = BeautifulSoup(txt_clean, "html.parser")
    h1 = soup.find("h1")
    orig = h1.get_text(strip=True) if h1 else article["title"]
    title = korean_title(orig, soup.get_text(" ", strip=True))
    if h1:
        h1.decompose()       # ë³¸ë¬¸ ì¤‘ë³µ ì œê±°

    body_extra = related_links(tag_ids)
    body = hidden + img_tag + str(soup) + body_extra

    payload = {
        "title": title,
        "content": body,
        "status": "publish",
        "categories": [TARGET_CAT_ID],
        "tags": tag_ids,
    }
    r = requests.post(POSTS_API, json=payload, auth=(USER, APP_PW), timeout=30)
    r.raise_for_status()
    logging.info("  â†³ ê²Œì‹œ %s %s", r.status_code, r.json().get("id"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s â”‚ %(levelname)s â”‚ %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    logging.info("ğŸš€ ë²„ì „ %s ì‹¤í–‰", __version__)

    seen  = sync_seen(load_seen())
    links = fetch_links()
    todo  = [u for u in links if norm(u) not in seen and not wp_exists(norm(u))]
    logging.info("ğŸ“° ìƒˆ ê¸°ì‚¬ %d / ì´ %d", len(todo), len(links))

    brief = build_brief()

    for url in todo:
        logging.info("â–¶ %s", url)
        art = parse(url)
        if not art:
            continue

        try:
            txt = rewrite(art, brief)
        except Exception as e:
            logging.warning("GPT ì˜¤ë¥˜: %s", e)
            continue

        tag_ids = [tid for n in tag_names(txt) if (tid := tag_id(n))]
        try:
            publish(art, txt, tag_ids)
            seen.add(norm(url)); save_seen(seen)
        except Exception as e:
            logging.warning("ì—…ë¡œë“œ ì‹¤íŒ¨: %s", e)

        time.sleep(1.5)

if __name__ == "__main__":
    main()
