import requests
import json
import os
from bs4 import BeautifulSoup, NavigableString, Tag

BASE_URL = "https://udf.name/news/"
HEADERS = {"User-Agent": "Mozilla/5.0"}
SEEN_FILE = "seen_urls.json"

# âœ… ì´ì „ì— ë³¸ URL ë¡œë”©
def load_seen_urls():
    if os.path.exists(SEEN_FILE):
        with open(SEEN_FILE, "r", encoding="utf-8") as f:
            return set(json.load(f))
    return set()

# âœ… í¬ë¡¤ë§ ì™„ë£Œ í›„ URL ì €ì¥
def save_seen_urls(urls):
    with open(SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump(list(urls), f, ensure_ascii=False, indent=2)

# âœ… ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
def get_article_links():
    response = requests.get(BASE_URL, headers=HEADERS)
    if response.status_code != 200:
        print(f"âŒ ë©”ì¸ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    links = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if (
            href.startswith("https://udf.name/news/") and
            href.endswith(".html") and
            href.count("/") >= 5
        ):
            links.append(href)
    return list(set(links))

# âœ… ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜
def extract_article(url):
    response = requests.get(url, headers=HEADERS)
    if response.status_code != 200:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {url} | {response.status_code}")
        return None

    soup = BeautifulSoup(response.text, "html.parser")

    title = soup.find("h1", class_="newtitle")
    title_text = title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ"

    author = soup.find("div", class_="author")
    author_text = author.get_text(strip=True) if author else "ì¶œì²˜ ì—†ìŒ"

    image = soup.find("img", class_="lazy")
    image_url = "https://udf.name" + image["data-src"] if image else None

    content_block = soup.find("div", id="zooming")
    content_lines = []
    if content_block:
        for el in content_block.descendants:
            if isinstance(el, NavigableString):
                txt = el.strip()
                if txt:
                    content_lines.append(txt)
            elif isinstance(el, Tag) and el.name in ["p", "br"]:
                content_lines.append("\n")

    content = "\n".join(line for line in content_lines if line.strip())
    content = content.replace("dle_leech_begin", "").replace("dle_leech_end", "")
    content = content.strip()

    return {
        "title": title_text,
        "author": author_text,
        "image": image_url,
        "url": url,
        "content": content
    }

# âœ… ì‹¤í–‰ ë©”ì¸
if __name__ == "__main__":
    print("ğŸ” í¬ë¡¤ë§ ì‹œì‘")

    seen = load_seen_urls()
    all_links = get_article_links()

    new_links = [link for link in all_links if link not in seen]
    print(f"ğŸ“° ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(new_links)}")

    for link in new_links:
        article = extract_article(link)
        if article and article["content"]:
            print("\nğŸ“° [ê¸°ì‚¬ ì œëª©]", article["title"])
            print("ğŸ‘¤ [ì¶œì²˜]", article["author"])
            print("ğŸ–¼ [ì´ë¯¸ì§€]", article["image"])
            print("ğŸ”— [URL]", article["url"])
            print("\nğŸ“„ [ë³¸ë¬¸ ë‚´ìš©]")
            print(article["content"][:500] + "..." if len(article["content"]) > 500 else article["content"])

            # URL ì €ì¥
            seen.add(link)

    save_seen_urls(seen)
    print("\nâœ… í¬ë¡¤ë§ ì™„ë£Œ")