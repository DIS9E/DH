#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
v4-adsense-nyunik-d.py
â€¢ ë‰´ë‹‰ ìŠ¤íƒ€ì¼ + AdSense í’ˆì§ˆ ìš”ê±´ ì™„ì „ ëŒ€ì‘
â€¢ tag_ids ë¹ˆ ë°°ì—´ â†’ ì¸ë±ìŠ¤ ì—ëŸ¬ í•´ê²° (safe_tags ë°±ì—… & ê°€ë“œ)
â€¢ related_links() WP ê²°ê³¼ 0ê°œÂ·ì˜¤ë¥˜ JSON ì•ˆì „ ì²˜ë¦¬
"""

import os, sys, re, json, time, logging, random
from urllib.parse import urljoin, urlparse, urlunparse
import requests
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. ê³µí†µ ìƒìˆ˜ & í™˜ê²½ ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WP_URL = os.getenv("WP_URL", "https://belatri.info").rstrip("/")
USER, APP_PW, OPEN_KEY = map(os.getenv,
    ["WP_USERNAME", "WP_APP_PASSWORD", "OPENAI_API_KEY"])
if not all([USER, APP_PW, OPEN_KEY]):
    sys.exit("âŒ í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ëˆ„ë½")

POSTS_API = f"{WP_URL}/wp-json/wp/v2/posts"
TAGS_API  = f"{WP_URL}/wp-json/wp/v2/tags"
UDF_BASE  = "https://udf.name/news/"
HEADERS   = {"User-Agent": "UDFCrawler/4-nyunik-d"}
SEEN_FILE = "seen_urls.json"
TARGET_CAT_ID = 20
LOCK      = "/tmp/udf_crawler.lock"
norm = lambda u: urlunparse(urlparse(u)._replace(query="", params="", fragment=""))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. ë½íŒŒì¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if os.path.exists(LOCK):
    print("âš ï¸ ì´ë¯¸ ì‹¤í–‰ ì¤‘, ì¢…ë£Œ"); sys.exit(0)
open(LOCK, "w").close()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. seen & WP util â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_seen():
    return set(json.load(open(SEEN_FILE))) if os.path.exists(SEEN_FILE) else set()
def save_seen(s):
    json.dump(list(s), open(SEEN_FILE, "w"), ensure_ascii=False, indent=2)
def wp_exists(u):
    return bool(requests.get(POSTS_API,
        params={"search": u, "per_page": 1},
        auth=(USER, APP_PW), timeout=10).json())
def sync_seen(seen):
    kept = {u for u in seen if wp_exists(norm(u))}
    if kept != seen: save_seen(kept)
    return kept

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. í¬ë¡¤ & íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_links():
    soup = BeautifulSoup(requests.get(UDF_BASE, headers=HEADERS,
                    timeout=10).text, "html.parser")
    return list({norm(urljoin(UDF_BASE, a["href"]))
        for a in soup.select("div.article1 div.article_title_news a[href]")})

def parse(url):
    r = requests.get(url, headers=HEADERS, timeout=10); r.raise_for_status()
    s = BeautifulSoup(r.text, "html.parser")
    h = s.find("h1", class_="newtitle"); b = s.find("div", id="zooming")
    if not (h and b): return None
    img = s.find("img", class_="lazy") or s.find("img")
    src = (img.get("data-src") or img.get("src")) if img else None
    if src and any(x in src for x in ("placeholder", "default")): src = None
    cat = url.split("/news/")[1].split("/")[0]
    return {"title": h.get_text(strip=True), "html": str(b),
            "image": urljoin(url, src) if src else None,
            "url": url, "cat": cat}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. ì™¸ë¶€ ë°ì´í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RSS_FEEDS = {
    "economic":[
        "https://feeds.bbci.co.uk/news/business/rss.xml",
        "https://www.rbc.ru/v10/ajax/channel.jsp?channel=biz&limit=10"],
    "sport":[
        "https://www.espn.com/espn/rss/news",
        "https://sports.yahoo.com/rss/"],
    "politic":[
        "https://feeds.bbci.co.uk/news/world/rss.xml",
        "https://www.rferl.org/rss"]
}

def pick_headlines(cat, n=2):
    for feed in RSS_FEEDS.get(cat, []):
        try:
            xml = requests.get(feed, timeout=6).text
            items = BeautifulSoup(xml, "xml").find_all("item", limit=n)
            if items:
                return [f"â€¢ {i.title.get_text(strip=True)} (ì¶œì²˜: {urlparse(feed).hostname})"
                        for i in items]
        except Exception: pass
    return []

def nbrb_rate():
    try:
        j = requests.get("https://api.nbrb.by/exrates/rates/431",
                         timeout=6).json()
        return f"â€¢ NBRB USD/BYN {j['Cur_OfficialRate']:.2f} (ì¶œì²˜: NBRB)"
    except Exception: return ""

def oil_price():
    try:
        j = requests.get("https://api.eia.gov/series/?api_key=DEMO_KEY&series_id=PET.RWTC.D",
                         timeout=6).json()
        p = j["series"][0]["data"][0][1]
        return f"â€¢ WTI ìœ ê°€ {p}$/bbl (ì¶œì²˜: EIA)"
    except Exception: return ""

def build_brief(cat):
    data = [nbrb_rate(), oil_price()] + pick_headlines(cat)
    return "\n".join([d for d in data if d])[:400]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. GPT ë˜í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chat(sys_p, user_p, max_tok=1800, temp=0.45, model="gpt-4o"):
    hdr = {"Authorization": f"Bearer {OPEN_KEY}",
           "Content-Type":  "application/json"}
    msgs = [{"role":"system","content":sys_p},
            {"role":"user",  "content":user_p}]
    r = requests.post("https://api.openai.com/v1/chat/completions",
        headers=hdr, json={"model":model,"messages":msgs,
                           "temperature":temp,"max_tokens":max_tok},
        timeout=120)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"].strip()

SYS = ("ë‹¹ì‹ ì€ â€˜ë‰´ë‹‰â€™ í•œêµ­ì–´ ê¸°ì. ì œëª©â†’ê°œìš”â†’ğŸ“Šâ†’ğŸ’¬â†’Q&Aâ†’í•´ì„¤â†’íƒœê·¸â†’footer "
       "êµ¬ì¡°ë¥¼ ì§€í‚¤ê³ , ì¹œê·¼í•œ ì¡´ëŒ“ë§Â·ì§ˆë¬¸Â·ê°íƒ„Â·ì´ëª¨ì§€ë¥¼ í™œìš©í•œë‹¤.")

STYLE_GUIDE = """
<h1>(ì´ëª¨ì§€ 1â€“3ê°œ) í¥ë¯¸ë¡œìš´ í•œêµ­ì–´ ì œëª©</h1>
<small>ë‰´ë‹‰ â€¢ {date} â€¢ ì½ìŒ {views:,}</small>

<h2>âœï¸ í¸ì§‘ì ì£¼ â€” ê¸°ì‚¬ í•µì‹¬ì„ 2ë¬¸ì¥</h2>

<h3>ğŸ“Š ìµœì‹  ë°ì´í„°</h3>
<p>(extra_context ìˆ«ìÂ·ë§í¬ ì‚¬ìš©, 4â€“6ì¤„Â·ê° ì¤„ 60ìÂ±10)</p>

<h3>ğŸ’¬ ì „ë¬¸ê°€ ì „ë§</h3>
<p>(ì‹œë‚˜ë¦¬ì˜¤Â·ê·¼ê±°Â·ìˆ«ì í¬í•¨ 500ìâ†‘)</p>

<h3>â“ Q&A</h3>
<ul><li><strong>Q1.</strong> â€¦?<br><strong>A.</strong> 2ë¬¸ì¥â†‘ ë¶„ì„</li>
<li><strong>Q2.</strong> â€¦?<br><strong>A.</strong> â€¦</li>
<li><strong>Q3.</strong> â€¦?<br><strong>A.</strong> â€¦</li></ul>

<h3>(ë³¸ë¬¸ í•´ì„¤)</h3>
<p>ì›ë¬¸ ë¬¸ì¥ 100 % ì¬ë°°ì¹˜Â·ì˜ì—­ ì—†ì´ ìœ ì§€.</p>

<p>ğŸ·ï¸ íƒœê·¸: ëª…ì‚¬ 3â€“6ê°œ</p>
<p>ì¶œì²˜: UDF.name ì›ë¬¸<br>Photo: UDF.name<br>
by. ì—ë””í„° LEEğŸŒ³<br><em>* ìƒì„±í˜• AIì˜ ë„ì›€ìœ¼ë¡œ ì‘ì„±í•œ ì•„í‹°í´ì…ë‹ˆë‹¤.</em></p>
"""

TAG_RGX = re.compile(r"ğŸ·ï¸[^:ï¼š]{0,20}[:ï¼š]\s*(.+)", re.S)

def rewrite(art):
    prompt = STYLE_GUIDE.format(date=time.strftime("%Y.%m.%d"),
                                views=random.randrange(7800, 12000))
    prompt += f"\n\nâ—† ì›ë¬¸:\n{art['html']}\n\n"
    prompt += f"â—† extra_context:\n{build_brief(art['cat'])}"
    txt = chat(SYS, prompt, max_tok=2300)
    txt = re.sub(r"<h1>.*?</h1>", "", txt, flags=re.S)  # GPT h1 ì œê±°
    return txt

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. íƒœê·¸ ì¶”ì¶œ & ì•ˆì „ ë°±ì—… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STOP = {"ë²¨ë¼ë£¨ìŠ¤","ë‰´ìŠ¤","ê¸°ì‚¬"}

def tag_names(txt):
    m = TAG_RGX.search(txt)
    if not m: return []
    plain = re.sub(r"<[^>]+>", " ", m.group(1))
    out=[]
    for t in re.split(r"[,\s]+", plain):
        t=t.strip("â€“-#â€¢")
        if 1<len(t)<=20 and t not in STOP and t not in out:
            out.append(t)
    return out[:6]

def tag_id(name):
    q = requests.get(TAGS_API, params={"search":name,"per_page":1},
                     auth=(USER,APP_PW), timeout=10).json()
    if q: return q[0]["id"]
    c = requests.post(TAGS_API, json={"name":name},
                      auth=(USER,APP_PW), timeout=10)
    return c.json()["id"] if c.status_code==201 else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. ë¬¸ë‹¨ í™•ì¥(ê¸¸ì´ë³´ì¥) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PLACE_RGX = re.compile(
    r"(ê¸°ì‚¬ í•µì‹¬.*?2ë¬¸ì¥|í¥ë¯¸ë¡œìš´ í•œêµ­ì–´ ì œëª©:|extra_context.+?strong>|"
    r"ë¬¸ë‹¨ì„.+?í™•ì¥.*?|ì–´ë–¤ ì£¼ì œì—.*ì•Œë ¤ì£¼ì„¸ìš”)!?", re.I)

def ensure_long(html, title):
    soup = BeautifulSoup(html,"html.parser")
    for t in soup.find_all(string=True):
        if title.strip()==t.strip(): t.extract()
    for blk in soup.find_all(["p","ul"]):
        if len(blk.get_text())<500:
            try:
                expanded = chat(SYS,
                    f"<ë¬¸ë‹¨>{blk.get_text()}</ë¬¸ë‹¨>\n\n500ìâ†‘ í™•ì¥.",
                    max_tok=500, model="gpt-4o-mini")
                blk.clear(); blk.append(BeautifulSoup(expanded,"html.parser"))
            except Exception as e:
                logging.debug("í™•ì¥ ì‹¤íŒ¨ %s", e)
    html = PLACE_RGX.sub("", str(soup))
    return re.sub(r"\s{2,}", " ", html)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. ê´€ë ¨ ê¸€ ë§í¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def related_links(tag_ids, exclude_id=0, limit=3):
    if not tag_ids: return ""
    t_id = tag_ids[0]
    res = requests.get(POSTS_API,
        params={"tags":t_id,"per_page":limit,"exclude":exclude_id,
                "status":"publish"},
        auth=(USER,APP_PW), timeout=10).json()
    if not isinstance(res, list) or not res: return ""
    lis = [f'<li><a href="{p["link"]}">{p["title"]["rendered"]}</a></li>'
           for p in res]
    return "<h3>ğŸ“š ê´€ë ¨ ê¸°ì‚¬ ë” ë³´ê¸°</h3><ul>"+"".join(lis)+"</ul>"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 9. ì œëª© ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CYRILLIC = re.compile(r"[Ğ-Ğ¯Ğ°-ÑĞÑ‘]")
def korean_title(src, ctx):
    if not CYRILLIC.search(src): return src
    return chat(SYS,
        f"ë‹¤ìŒ ëŸ¬ì‹œì•„ì–´ ì œëª©ì„ 45ìâ†“ í•œêµ­ì–´ ì¹´í”¼ë¼ì´í„°+ì´ëª¨ì§€ë¡œ:\nÂ«{src}Â»\në¬¸ë§¥:{ctx[:200]}",
        max_tok=60, temp=0.9, model="gpt-4o-mini")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10. ê²Œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def publish(art, raw_txt, safe_tags):
    # 1) ìµœì¢… ë³¸ë¬¸ í™•ì¥ & ê¸¸ì´ ë³´ì¥
    kor_title = korean_title(art["title"], raw_txt)
    body = ensure_long(raw_txt, kor_title)
    soup = BeautifulSoup(body,"html.parser")

    # 2) ìƒˆ h1 ì‚½ì…
    h1 = soup.new_tag("h1"); h1.string=kor_title; soup.insert(0,h1)

    # 3) íƒœê·¸ ì¶”ì¶œ (í™•ì¥ í›„ + ë°±ì—…) â†’ ì¤‘ë³µ ì œê±°
    names = list(dict.fromkeys(tag_names(str(soup))+safe_tags))
    tag_ids=[tid for n in names if (tid:=tag_id(n))]

    # 4) footer ë³´ê°•
    if not soup.find(string=re.compile("by\\. ì—ë””í„°")):
        foo = (f'<p>ì¶œì²˜: <a href="{art["url"]}">UDF.name ì›ë¬¸</a><br>'
               'Photo: UDF.name<br>by. ì—ë””í„° LEEğŸŒ³<br>'
               '<em>* ìƒì„±í˜• AIì˜ ë„ì›€ìœ¼ë¡œ ì‘ì„±í•œ ì•„í‹°í´ì…ë‹ˆë‹¤.</em></p>')
        soup.append(BeautifulSoup(foo,"html.parser"))

    # 5) ê´€ë ¨ ë‚´ë¶€ ë§í¬
    soup.append(BeautifulSoup(related_links(tag_ids), "html.parser"))

    hidden = f"<a href='{art['url']}' style='display:none'>src</a>\n"
    img = (f"<p><img src='{art['image']}' alt=''><br>"
           f"<em>Photo: UDF.name</em></p>\n") if art["image"] else ""

    payload={"title":kor_title,"content":hidden+img+str(soup),
             "status":"publish","categories":[TARGET_CAT_ID],
             "tags":tag_ids,"ping_status":"closed"}
    r=requests.post(POSTS_API, json=payload,
                    auth=(USER,APP_PW), timeout=30); r.raise_for_status()
    logging.info("â†³ ê²Œì‹œ %s %s", r.status_code, r.json().get("id"))
    return r.json()["id"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 11. main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    logging.basicConfig(level=logging.DEBUG,
        format="%(asctime)s â”‚ %(levelname)s â”‚ %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S")
    seen=sync_seen(load_seen())
    links=fetch_links()
    todo=[u for u in links if norm(u) not in seen and not wp_exists(norm(u))]
    logging.info("ğŸ“° ìƒˆ ê¸°ì‚¬ %d / ì´ %d", len(todo), len(links))

    for url in todo:
        logging.info("â–¶ %s", url)
        art=parse(url); time.sleep(1)
        if not art: continue
        try:
            raw_txt=rewrite(art)          # ì´ˆì•ˆ
            safe_tags=tag_names(raw_txt)  # ë°±ì—…
        except Exception as e:
            logging.warning("GPT ì˜¤ë¥˜: %s", e); continue

        try:
            new_id = publish(art, raw_txt, safe_tags)
            seen.add(norm(url)); save_seen(seen)
        except Exception as e:
            logging.warning("ì—…ë¡œë“œ ì‹¤íŒ¨: %s", e)
        time.sleep(1.5)

if __name__ == "__main__":
    try:
        main()
    finally:
        if os.path.exists(LOCK): os.remove(LOCK)
