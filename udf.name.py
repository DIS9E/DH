#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
udf.name.py â€“ v3.6
â€¢ GPT-4o í—¤ë“œë¼ì´íŠ¸ í†¤ (80~110 % ê¸¸ì´)
â€¢ HTML <h1>/<h2>/<h3> í—¤ë” ë³€í™˜ & <strong> ê°•ì¡°
â€¢ Yoast SEO 3í•„ë“œ ìë™
â€¢ ì¹´í…Œê³ ë¦¬ 20 ê³ ì •, WPâ†”seen ë™ê¸°í™”, dup-safe
â€¢ ë¯¸ë””ì–´ ì—…ë¡œë“œ ì—†ì´ ë³¸ë¬¸ img ì‚½ì…
"""

import os, sys, re, json, time, logging
from urllib.parse import urljoin, urlparse, urlunparse
import requests
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ë³€ìˆ˜ & WP ì—”ë“œí¬ì¸íŠ¸
WP_URL = os.getenv("WP_URL", "https://belatri.info").rstrip("/")
USER   = os.getenv("WP_USERNAME")
APP_PW = os.getenv("WP_APP_PASSWORD")
OPENAI = os.getenv("OPENAI_API_KEY")
if not all([USER, APP_PW, OPENAI]):
    sys.exit("âŒ  WP_USERNAME / WP_APP_PASSWORD / OPENAI_API_KEY ëˆ„ë½")

POSTS = f"{WP_URL}/wp-json/wp/v2/posts"
TAGS  = f"{WP_URL}/wp-json/wp/v2/tags"

UDF_BASE   = "https://udf.name/news/"
HEADERS    = {"User-Agent": "UDFCrawler/3.6"}
SEEN_FILE  = "seen_urls.json"
CAT_ID     = 20            # â€˜ë²¨ë¼ë£¨ìŠ¤ ë‰´ìŠ¤â€™

norm = lambda u: urlunparse(urlparse(u)._replace(query="", params="", fragment=""))

# â”€â”€â”€â”€â”€â”€â”€ seen ê´€ë¦¬
def load_seen():
    if os.path.exists(SEEN_FILE):
        with open(SEEN_FILE, encoding="utf-8") as f:
            return set(json.load(f))
    return set()

def save_seen(s):
    with open(SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump(list(s), f, ensure_ascii=False, indent=2)

def wp_exists(url_):
    r = requests.get(POSTS, params={"search": url_, "per_page": 1},
                     auth=(USER, APP_PW), timeout=10)
    return r.ok and bool(r.json())

def sync_seen(seen):
    synced = {u for u in seen if wp_exists(norm(u))}
    if len(synced) != len(seen):
        save_seen(synced)
    return synced

# â”€â”€â”€â”€â”€â”€â”€ ë§í¬ ìˆ˜ì§‘
def fetch_links():
    soup = BeautifulSoup(requests.get(UDF_BASE, headers=HEADERS, timeout=10).text, "html.parser")
    return list({norm(urljoin(UDF_BASE, a["href"]))
                 for a in soup.select("div.article1 div.article_title_news a[href]")})

# â”€â”€â”€â”€â”€â”€â”€ ê¸°ì‚¬ íŒŒì‹±
def parse(url):
    r = requests.get(url, headers=HEADERS, timeout=10)
    if not r.ok: return None
    s = BeautifulSoup(r.text, "html.parser")
    h1 = s.find("h1", class_="newtitle")
    body = s.find("div", id="zooming")
    if not (h1 and body): return None
    img = s.find("img", class_="lazy") or s.find("img")
    img_url = urljoin(url, (img.get("data-src") or img.get("src"))) if img else None
    return {"title": h1.get_text(strip=True),
            "html": str(body),
            "image": img_url,
            "url": url}

# â”€â”€â”€â”€â”€â”€â”€ ìŠ¤íƒ€ì¼ & GPT
STYLE_GUIDE = """
**í—¤ë“œë¼ì´íŠ¸ ë¸”ë¡œê·¸ ë¦¬ë¼ì´íŠ¸ ê·œì¹™ (v3.6)**  
1. í†¤: ì¹œê·¼í•œ ì¡´ëŒ“ë§, ì§ˆë¬¸Â·ê°íƒ„Â·ì´ëª¨ì§€ ì ì ˆíˆ ì‚½ì…  
2. ê¸¸ì´: ì›ë¬¸ ëŒ€ë¹„ ìµœì†Œ 80 %, ìµœëŒ€ 110 % (ìš”ì•½Â·ìƒëµ ê¸ˆì§€)  
3. êµ¬ì¡°  
   <h1>ğŸ“° ì œëª©</h1>  
   <h2>âœï¸ í¸ì§‘ì ì£¼</h2> â€“ 2ë¬¸ì¥ í•µì‹¬ ìš”ì•½  
   <h2>ğŸ—ï¸ ë³¸ë¬¸</h2>  
   <h3>â€£ ì†Œì œëª© 1</h3> ë³¸ë¬¸ ìœ ì§€Â·ì¬êµ¬ì„±  
   <h3>â€£ ì†Œì œëª© 2</h3> â€¦  
   <h2>ğŸ”¦ í—¤ë“œë¼ì´íŠ¸â€™s ì½”ë©˜íŠ¸</h2> 300ì ë‚´ì™¸ í†µì°°  
   <p>ğŸ·ï¸ íƒœê·¸: â€¦(ëª…ì‚¬ 3~6ê°œ)</p>  
4. ë§ˆí¬ë‹¤ìš´ # ê¸°í˜¸ ì‚¬ìš© ê¸ˆì§€ (ë°˜ë“œì‹œ HTML <h1>/<h2>/<h3>)  
5. êµµê²Œ ê°•ì¡°í•  í‚¤ì›Œë“œì— <strong>â€¦</strong> ì‚¬ìš©  
6. ì›ë¬¸ ë§í¬Â·ë‚ ì§œÂ·ë¶ˆí•„ìš”í•œ ëŸ¬ì‹œì•„ì–´ ê·¸ëŒ€ë¡œ ë‚¨ê¸°ì§€ ë§ ê²ƒ  
7. ì œëª©ì€ â€˜ë…ìê°€ í´ë¦­í•˜ê³  ì‹¶ì„â€™ í•œêµ­ì–´ ìƒˆ ì œëª© (ì§ì—­ X)  
"""

GPT_URL = "https://api.openai.com/v1/chat/completions"
GPT_HDR = {"Authorization": f"Bearer {OPENAI}", "Content-Type": "application/json"}

def rewrite(article):
    prompt = f"{STYLE_GUIDE}\n\nâ—† ì›ë¬¸ HTML\n{article['html']}"
    def ask():
        r = requests.post(GPT_URL,
            headers=GPT_HDR,
            json={"model":"gpt-4o",
                  "messages":[{"role":"user","content":prompt}],
                  "temperature":0.4,
                  "top_p":0.95}, timeout=120)
        r.raise_for_status()
        return r.json()["choices"][0]["message"]["content"].strip()

    text = ask()
    if len(text) < 2000:                 # ë„ˆë¬´ ì§§ìœ¼ë©´ 1íšŒ ì¬ì‹œë„
        text = ask()

    return text

# â”€â”€â”€â”€â”€â”€â”€ íƒœê·¸
STOP = {"ë²¨ë¼ë£¨ìŠ¤", "ë‰´ìŠ¤", "ê¸°ì‚¬"}
def tag_names(txt):
    m = re.search(r"ğŸ·ï¸\s*íƒœê·¸[^:ï¼š]*[:ï¼š]\s*(.+)", txt)
    if not m: return []
    out = []
    for t in re.split(r"[,\s]+", m.group(1)):
        t = t.strip("â€“-#â€¢")
        if 1 < len(t) <= 20 and t not in STOP and t not in out:
            out.append(t)
        if len(out) == 6: break
    return out

def tag_id(name):
    r = requests.get(TAGS, params={"search": name, "per_page":1},
                     auth=(USER, APP_PW), timeout=10)
    if r.ok and r.json():
        return r.json()[0]["id"]
    c = requests.post(TAGS, json={"name": name},
                      auth=(USER, APP_PW), timeout=10)
    return c.json().get("id") if c.status_code == 201 else None

# â”€â”€â”€â”€â”€â”€â”€ ë°œí–‰
def publish(a, txt, tag_ids):
    # ë³¸ë¬¸: ìˆ¨ì€ src ë§í¬ + ì™¸ë¶€ ì´ë¯¸ì§€ + GPT ê²°ê³¼
    hidden = f'<a href="{a["url"]}" style="display:none">src</a>\n'
    img_tag = f'<p><img src="{a["image"]}" alt=""></p>\n' if a["image"] else ""
    content = hidden + img_tag + txt

    # ì œëª© (ì²« <h1>)
    m = re.search(r"<h1[^>]*>(.+?)</h1>", txt)
    title = m.group(1).strip() if m else a["title"]

    # Yoast
    focus_kw = (tag_ids and tag_ids[0]) or ""
    meta = ""
    m2 = re.search(r"âœï¸\s*í¸ì§‘ì ì£¼.*?\n(.+)", txt)
    if m2: meta = re.sub(r"<[^>]+>", "", m2.group(1)).strip()[:140]

    payload = {
        "title": title,
        "content": content,
        "status": "publish",
        "categories": [CAT_ID],
        "tags": tag_ids,
        "meta": {
            "yoast_wpseo_title": title,
            "yoast_wpseo_focuskw": focus_kw,
            "yoast_wpseo_metadesc": meta
        }
    }
    r = requests.post(POSTS, json=payload, auth=(USER, APP_PW), timeout=30)
    print("  â†³ ê²Œì‹œ", r.status_code, r.json().get("id"))
    r.raise_for_status()

# â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸
def main():
    logging.basicConfig(level=logging.WARNING)
    seen = sync_seen(load_seen())
    links = fetch_links()

    todo = [u for u in links if norm(u) not in seen and not wp_exists(norm(u))]
    print(f"ğŸ“° ìƒˆ ê¸°ì‚¬ {len(todo)} / ì´ {len(links)}")

    for url in todo:
        print("===", url)
        art = parse(url)
        if not art: continue
        try:
            txt = rewrite(art)
        except Exception as e:
            print("  GPT ì˜¤ë¥˜:", e); continue

        tag_ids = [tid for n in tag_names(txt) if (tid := tag_id(n))]
        try:
            publish(art, txt, tag_ids)
            seen.add(norm(url)); save_seen(seen)
        except Exception as e:
            print("  ì—…ë¡œë“œ ì‹¤íŒ¨:", e)
        time.sleep(2)

if __name__ == "__main__":
    main()
