#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
udf.name.py  â€“  UDF.name â†’ ChatGPT â†’ WordPress ìë™ ì—…ë¡œë“œ íŒŒì´í”„ë¼ì¸
 - ì¤‘ë³µ í¬ìŠ¤íŠ¸ ë°©ì§€: WP /posts?search= ë¡œ ì„œë²„-ì¸¡ ê²€ì‚¬
 - ì´ë¯¸ì§€ ì—…ë¡œë“œ 401/404 í•´ê²°: Basic Auth + multipart + 404 graceful skip
 - ìµœì‹  HTML ì…€ë ‰í„°: div.article1 div.article_title_news a
 - 'ë²¨ë¼ë£¨ìŠ¤ ë‰´ìŠ¤' ì¹´í…Œê³ ë¦¬(ID 136) ìë™ ì§€ì •
"""
import os, sys, json, time, logging, re
from datetime import datetime
from urllib.parse import urljoin, urlparse, urlunparse
import requests
from bs4 import BeautifulSoup, NavigableString, Tag
from requests.auth import HTTPBasicAuth

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”§ í™˜ê²½ ë³€ìˆ˜
WP_URL         = os.getenv("WP_URL", "https://belatri.info").rstrip("/")
WP_USERNAME    = os.getenv("WP_USERNAME")
WP_APP_PASSWORD= os.getenv("WP_APP_PASSWORD")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
CATEGORY_ID    = 136                       # 'ë²¨ë¼ë£¨ìŠ¤ ë‰´ìŠ¤' ì¹´í…Œê³ ë¦¬

if not all([WP_USERNAME, WP_APP_PASSWORD, OPENAI_API_KEY]):
    sys.exit("âŒ  WP_USERNAME / WP_APP_PASSWORD / OPENAI_API_KEY ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

WP_API_URL    = f"{WP_URL}/wp-json/wp/v2/posts"
TAG_API_URL   = f"{WP_URL}/wp-json/wp/v2/tags"
MEDIA_API_URL = f"{WP_URL}/wp-json/wp/v2/media"

UDF_BASE_URL  = "https://udf.name/news/"
HEADERS_HTML  = {"User-Agent": "UDFCrawler/1.0 (+https://belatri.info)"}
SEEN_FILE     = "seen_urls.json"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ› ï¸ ìœ í‹¸
def normalize_url(u: str) -> str:
    """ì¿¼ë¦¬ìŠ¤íŠ¸ë§ ì œê±°(ì¤‘ë³µ ë°©ì§€)"""
    p = urlparse(u)
    return urlunparse((p.scheme, p.netloc, p.path, "", "", ""))

def load_seen_urls() -> set[str]:
    if os.path.exists(SEEN_FILE):
        with open(SEEN_FILE, encoding="utf-8") as f:
            return set(json.load(f))
    return set()

def save_seen_urls(urls: set[str]) -> None:
    with open(SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump(list(urls), f, ensure_ascii=False, indent=2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸŒ ì„¸ì…˜ (WP ì¸ì¦ í¬í•¨)
session = requests.Session()
session.auth = (WP_USERNAME, WP_APP_PASSWORD)
session.headers.update({"User-Agent": "UDFCrawler/1.0 (+https://belatri.info)"})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“‘ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
def fetch_article_links() -> list[str]:
    res = requests.get(UDF_BASE_URL, headers=HEADERS_HTML, timeout=10)
    res.raise_for_status()
    soup = BeautifulSoup(res.text, "html.parser")
    anchors = soup.select("div.article1 div.article_title_news a[href]")
    links = [normalize_url(urljoin(UDF_BASE_URL, a["href"])) for a in anchors]
    return list(dict.fromkeys(links))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“° ê¸°ì‚¬ íŒŒì‹±
def extract_article(url: str) -> dict | None:
    res = requests.get(url, headers=HEADERS_HTML, timeout=10)
    if res.status_code != 200:
        logging.error("âŒ§ ìš”ì²­ ì‹¤íŒ¨ %s | %s", url, res.status_code); return None
    soup = BeautifulSoup(res.text, "html.parser")
    title  = soup.find("h1", class_="newtitle")
    author = soup.find("div", class_="author")
    body   = soup.find("div", id="zooming")
    if not (title and body):
        logging.warning("ë³¸ë¬¸ ëˆ„ë½: %s", url); return None

    # ëŒ€í‘œ ì´ë¯¸ì§€ (lazy-load ì§€ì›)
    img_tag = soup.find("img", class_="lazy") or soup.find("img")
    img_url = None
    if img_tag:
        img_url = img_tag.get("data-src") or img_tag.get("src")
        if img_url:
            img_url = urljoin(url, img_url)

    # ë³¸ë¬¸(HTML) ê·¸ëŒ€ë¡œ
    content_html = str(body)

    return {
        "title": title.get_text(strip=True),
        "author": author.get_text(strip=True) if author else "",
        "image_url": img_url,
        "source_url": url,
        "content_html": content_html
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”„ ì¤‘ë³µ ê²€ì‚¬
def already_posted(source_url: str) -> bool:
    q = {"search": source_url, "per_page": 1}
    r = session.get(WP_API_URL, params=q, timeout=10)
    return r.status_code == 200 and bool(r.json())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ–¼ï¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ
def upload_media(image_url: str | None) -> int | None:
    if not image_url:
        return None
    img_resp = requests.get(image_url, headers=HEADERS_HTML, timeout=10, stream=True)
    if img_resp.status_code == 404:
        logging.warning("ğŸš« ì´ë¯¸ì§€ 404: %s", image_url); return None
    img_resp.raise_for_status()

    filename = os.path.basename(urlparse(image_url).path) or "featured.jpg"
    files = {"file": (filename, img_resp.content, img_resp.headers.get("Content-Type", "image/jpeg"))}
    up = session.post(MEDIA_API_URL, files=files, timeout=30)
    if up.status_code == 201:
        media_id = up.json()["id"]
        logging.info("ğŸ“¸ ì´ë¯¸ì§€ ì—…ë¡œë“œ ì„±ê³µ ID %s", media_id)
        return media_id
    logging.error("ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨ %s | %s", up.status_code, up.text[:120])
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœï¸ ChatGPT ë¦¬ë¼ì´íŒ…
def rewrite_with_chatgpt(article: dict) -> str:
    prompt = f"""
ë‹¤ìŒì€ ë²¨ë¼ë£¨ìŠ¤ ê´€ë ¨ ì™¸ì‹  ê¸°ì‚¬ì…ë‹ˆë‹¤. ì•„ë˜ ì–‘ì‹ì— ë§ì¶° í•œêµ­ ë…ìë¥¼ ìœ„í•œ ë¸”ë¡œê·¸ ê²Œì‹œê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ğŸ¯ ì‘ì„± ì¡°ê±´:
- ê¸°ì‚¬ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ìš”ì•½í•˜ê±°ë‚˜ í•´ì„í•˜ì§€ ë§ê³ **, **ë¬¸ì²´ì™€ êµ¬ì¡°ë§Œ ë°”ê¿”ì„œ ì¬ì‘ì„±**í•´ì£¼ì„¸ìš”.
- **ê¸°ì‚¬ì˜ ì •ë³´ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€**í•˜ê³ , **í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ê°€ë…ì„± ë†’ê²Œ** ì‘ì„±í•´ì£¼ì„¸ìš”.
- **ì œëª©(H1), ë¶€ì œ(H2), ë‚´ìš© ë¬¸ë‹¨(H3)** ë“±ìœ¼ë¡œ êµ¬ë¶„í•´ ë¸”ë¡œê·¸ì— ìµœì í™”ëœ êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
- ë§ˆì§€ë§‰ì— "ì´ ê¸°ì‚¬ëŠ” ë²¨ë¼ë£¨ìŠ¤ í˜„ì§€ ë³´ë„ ë‚´ìš©ì„ ì¬êµ¬ì„±í•œ ì½˜í…ì¸ ì…ë‹ˆë‹¤." ë¬¸êµ¬ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.

ğŸ§¾ ì¶œë ¥ í˜•ì‹:

# [ğŸ“° ì œëª©]
> ë¸”ë¡œê·¸ ê²Œì‹œê¸€ì˜ í•µì‹¬ì„ ë°˜ì˜í•œ ëª…í™•í•˜ê³  ê°„ê²°í•œ ì œëª©

## âœï¸ í¸ì§‘ì ì£¼
- ì „ì²´ ê¸°ì‚¬ ë§¥ë½ì„ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•œ í¸ì§‘ì ì½”ë©˜íŠ¸

## ğŸ“Œ í•µì‹¬ ë‚´ìš©
### H3 ìš”ì•½ 1
### H3 ìš”ì•½ 2

## ğŸ—ï¸ ì›ë¬¸ ì¬ì‘ì„±
### [ì†Œì œëª© H3 - ì£¼ì œ1]
- ê¸°ì‚¬ ë‚´ìš© ê·¸ëŒ€ë¡œ ë¬¸ì¥ êµ¬ì¡°ë§Œ ë³€ê²½
### [ì†Œì œëª© H3 - ì£¼ì œ2]
- ì´ì–´ì§€ëŠ” ë‚´ìš© ê³„ì† ì„œìˆ 

## ğŸŒ ì‹œì‚¬ì 
- í•œêµ­ í˜¹ì€ ì„¸ê³„ì— ë¯¸ì¹  ì˜í–¥ ì •ë¦¬

## ğŸ”— ì¶œì²˜
- ì›ë¬¸ ë§í¬: {article['source_url']}

---

ğŸ“° ê¸°ì‚¬ ì›ë¬¸:
{article['content_html']}
"""
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": "gpt-4o", "messages": [{"role": "user", "content": prompt}], "temperature": 0.3}
    r = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload, timeout=60)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ·ï¸ íƒœê·¸
def create_or_get_tag_id(tag_name: str) -> int | None:
    q = session.get(TAG_API_URL, params={"search": tag_name, "per_page": 1}, timeout=10)
    if q.status_code == 200 and q.json():
        return q.json()[0]["id"]
    c = session.post(TAG_API_URL, json={"name": tag_name}, timeout=10)
    if c.status_code == 201:
        return c.json()["id"]
    return None

def extract_tags_from_output(output: str) -> list[str]:
    """'# íƒœê·¸:' ê°™ì€ ë¼ì¸ì„ ì°¾ì•„ ë‹¨ì–´ ì¶”ì¶œ (ì›í•˜ëŠ” ì–‘ì‹ëŒ€ë¡œ ìˆ˜ì • ê°€ëŠ¥)"""
    lines = [l.strip() for l in output.splitlines() if l.strip()]
    tag_section = [l for l in lines if l.lower().startswith("ğŸ·") or "íƒœê·¸" in l]
    if not tag_section:
        return []
    # ë¼ì¸ ëì— ì‰¼í‘œ/ê³µë°± êµ¬ë¶„
    raw = re.sub(r"^.*?:", "", tag_section[0])
    return [t.strip("â€“- ,#") for t in raw.split() if t.strip()]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“ í¬ìŠ¤íŠ¸ ì—…ë¡œë“œ
def publish_post(title: str, content: str, tag_ids: list[int], media_id: int | None, source_url: str):
    payload = {
        "title": title,
        "content": content,
        "status": "publish",
        "categories": [CATEGORY_ID],
        "tags": tag_ids,
        "meta": {"_source_url": source_url}
    }
    if media_id:
        payload["featured_media"] = media_id
    r = session.post(WP_API_URL, json=payload, timeout=30)
    r.raise_for_status()
    logging.info("ğŸ“ ê²Œì‹œ ì„±ê³µ (ID %s)", r.json()["id"])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")

    seen_urls = load_seen_urls()
    article_links = fetch_article_links()
    targets = [u for u in article_links if normalize_url(u) not in seen_urls and not already_posted(u)]
    logging.info("ğŸ“° ìƒˆ ê¸°ì‚¬ %dê±´", len(targets))

    for url in targets:
        art = extract_article(url)
        if art is None:
            continue

        try:
            rewritten = rewrite_with_chatgpt(art)
        except Exception as e:
            logging.error("ChatGPT ì‹¤íŒ¨: %s", e); continue

        # ì œëª© ì¶”ì¶œ
        title_line = next((l for l in rewritten.splitlines() if l.startswith("# ")), art["title"])
        title_clean = title_line.replace("# ", "").strip()

        # íƒœê·¸
        tag_names = extract_tags_from_output(rewritten)
        tag_ids = [tid for tag in tag_names if (tid := create_or_get_tag_id(tag))]

        # ì´ë¯¸ì§€
        media_id = upload_media(art["image_url"])

        # ì—…ë¡œë“œ
        try:
            publish_post(title_clean, rewritten, tag_ids, media_id, art["source_url"])
            seen_urls.add(normalize_url(url))
            save_seen_urls(seen_urls)
            time.sleep(3)           # ì„œë²„ ë¶€í•˜ ì™„í™”
        except Exception as e:
            logging.error("ê²Œì‹œ ì‹¤íŒ¨: %s", e)

    logging.info("âœ… ì „ì²´ ì‘ì—… ì™„ë£Œ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    main()
