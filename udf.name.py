#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
UDF.name â†’ WordPress ìë™ í¬ìŠ¤íŒ… ìŠ¤í¬ë¦½íŠ¸
(1ì¼ 1íšŒ Render CronJob)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–  ê°œì„  ìš”ì•½
1. **ì¤‘ë³µ ë°©ì§€ 3-ë‹¨ê³„** (seen.json Â· WP ë©”íƒ€ `_source_url` Â· ìµœê·¼ ì‚­ì œ í—ˆìš©)
2. **ëŒ€í‘œ ì´ë¯¸ì§€ ì—…ë¡œë“œ ì˜µì…˜í™”** (image_id í•„ìš” ì—†ìœ¼ë©´ None)
3. **ì œëª© ì¹´í”¼ë¼ì´íŒ… ê°•í™” + í•œì ì œê±°**
4. **Yoast SEO í•„ë“œ ìë™ ì±„ìš°ê¸°**
5. **ì§ì—­ ê¸ˆì§€ í”„ë¡¬í”„íŠ¸ / ëŸ¬ì‹œì•„ì–´ ê²€ì¶œ ë³´ì •**
6. **ë¡œê¹… ê°€ë…ì„± ê°œì„ **
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os, re, json, random, logging, unicodedata
from datetime import datetime
from urllib.parse import urlparse
from typing import List, Dict, Optional

# ì„œë“œíŒŒí‹°
import requests
from bs4 import BeautifulSoup, NavigableString, Tag
from requests.auth import HTTPBasicAuth

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜
WP_USERNAME     = os.getenv("WP_USERNAME")
WP_APP_PASSWORD = os.getenv("WP_APP_PASSWORD")
WP_API_URL      = "https://belatri.info/wp-json/wp/v2/posts"
TAG_API_URL     = "https://belatri.info/wp-json/wp/v2/tags"
OPENAI_API_KEY  = os.getenv("OPENAI_API_KEY")
UDF_BASE_URL    = "https://udf.name/news/"

HEADERS = {"User-Agent":"Mozilla/5.0 (UDF-crawler)"}
SEEN_FILE = "seen_urls.json"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s â”‚ %(levelname)s â”‚ %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
log = logging.getLogger("udf")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸
def normalize_url(url: str) -> str:
    p = urlparse(url)
    return f"{p.scheme}://{p.netloc}{p.path}"

def load_seen_urls() -> set:
    if os.path.exists(SEEN_FILE):
        with open(SEEN_FILE, "r", encoding="utf-8") as f:
            return set(json.load(f))
    return set()

def save_seen_urls(urls: set):
    with open(SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump(sorted(list(urls)), f, ensure_ascii=False, indent=2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# WordPress ìª½ ì¤‘ë³µ ê²€ì‚¬
def get_existing_source_urls(pages:int=50) -> set:
    page, existing = 1, set()
    while page <= pages:
        r = requests.get(
            WP_API_URL,
            params={"per_page":100,"page":page,"_fields":"meta"},
            auth=(WP_USERNAME, WP_APP_PASSWORD)
        )
        if r.status_code != 200 or not r.json():
            break
        for post in r.json():
            src = post.get("meta",{}).get("_source_url")
            if src:
                existing.add(normalize_url(src))
        page+=1
    log.info("WP ì €ì¥ _source_url %dê±´", len(existing))
    return existing

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
def get_article_links()->List[str]:
    r = requests.get(UDF_BASE_URL, headers=HEADERS, timeout=15)
    r.raise_for_status()
    soup = BeautifulSoup(r.text,"html.parser")
    links=set()
    for a in soup.find_all("a",href=True):
        href=a["href"]
        if href.startswith("https://udf.name/news/") and href.endswith(".html"):
            links.add(normalize_url(href))
    return list(links)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë³¸ë¬¸Â·ë©”íƒ€ ì¶”ì¶œ
def extract_article(url:str)->Optional[Dict]:
    try:
        r=requests.get(url,headers=HEADERS,timeout=15)
        r.raise_for_status()
    except Exception as e:
        log.warning("ìš”ì²­ ì‹¤íŒ¨ %s | %s", url,e); return None

    soup = BeautifulSoup(r.text,"html.parser")
    title  = soup.find("h1", class_="newtitle")
    author = soup.find("div", class_="author")
    content_block = soup.find("div", id="zooming")

    # ë³¸ë¬¸ í…ìŠ¤íŠ¸
    lines=[]
    if content_block:
        for el in content_block.descendants:
            if isinstance(el,NavigableString):
                t=el.strip()
                if t: lines.append(t)
            elif isinstance(el,Tag) and el.name in ("p","br"): lines.append("\n")
    content="\n".join(l for l in lines if l.strip())
    content=re.sub(r"dle_leech_(begin|end)","",content).strip()

    return {
        "title": title.get_text(strip=True) if title else "",
        "author": author.get_text(strip=True) if author else "",
        "url":   url,
        "content": content
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GPT ë¦¬ë¼ì´íŒ…
PROMPT_BASE = """
ë„ˆëŠ” 20â€“40ëŒ€ í•œêµ­ì¸ì„ ìœ„í•œ ë²¨ë¼ë£¨ìŠ¤ ë‰´ìŠ¤ ì¹¼ëŸ¼ë‹ˆìŠ¤íŠ¸ì•¼.

âœï¸ [ì œëª© ê·œì¹™]
â€¢ ëŸ¬ì‹œì•„ì–´Â·ì§ì—­ NO, **êµ­ë‚´ ë…ìê°€ í´ë¦­í•  25~35ì ì¹´í”¼**  
â€¢ ìˆ«ìÂ·ì§ˆë¬¸Â·ëŒ€ì¡° í‘œí˜„ í™œìš©, ëì— ê´€ë ¨ ì´ëª¨ì§€ 1ê°œ ë¶™ì´ê¸°  
â€¢ í•œì ì‚¬ìš© ê¸ˆì§€

âœï¸ [ë³¸ë¬¸Â·ë ˆì´ì•„ì›ƒ]
<ê¸€ í˜•ì‹ ì˜ˆì‹œ>
<h1>ì œëª© ğŸ“°</h1>
<blockquote>í•œ ì¤„ í¸ì§‘ì ì£¼ (1ë¬¸ì¥)</blockquote>

<h2>í¬ì¸íŠ¸ ìš”ì•½ âœï¸</h2>
<ul>
<li>í•µì‹¬ 1</li><li>í•µì‹¬ 2</li></ul>

<h2>í˜„ì§€ ìƒí™© ğŸ”</h2>
<h3>ì†Œì œëª©</h3>
<p>â€¦</p>

<h2>ì‹œì‚¬ì  ğŸ’¡</h2>
<p>â€¦</p>

<em>by. ì—ë””í„° LEEğŸŒ³</em>

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ëŸ¬ì‹œì•„ì–´ ì›ë¬¸ â†“
{article_body}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âš ï¸ ì§€ì‹œ
â€¢ â€˜##â€™, â€˜###â€™ ê°™ì€ Markdown ëŒ€ì‹  html íƒœê·¸ ì‚¬ìš©  
â€¢ ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ í™œìš©  
â€¢ ìš”ì•½Â·í•´ì„ì€ ììœ ë¡­ê²Œ, **ì •ë³´ ì™œê³¡ì€ ê¸ˆì§€**
â€¢ í•œì(æ¼¢å­—) ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€
"""

def rewrite_with_chatgpt(article:dict)->str:
    prompt = PROMPT_BASE.format(article_body=article["content"][:2500])
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type":  "application/json"
    }
    payload = {
        "model":"gpt-4o",
        "messages":[{"role":"user","content":prompt}],
        "temperature":0.4
    }
    r=requests.post("https://api.openai.com/v1/chat/completions",
                    headers=headers,json=payload,timeout=60)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í•œì ì œê±°Â·ì œëª© ë³´ì •
HANJA_R = re.compile(r"[\u3400-\u4DBF\u4E00-\u9FFF]+")
RUEN_R  = re.compile(r"[A-Za-zĞ-Ğ¯Ğ°-ÑĞÑ‘]")

TEMPLATES = [
    "â—‹â—‹, ì§„ì§œ ë…¸ë¦¼ìˆ˜ëŠ”? {e}",
    "ë²¨ë¼ë£¨ìŠ¤ â—â— íŒŒì¥ {e}",
    "ì™œ ì§€ê¸ˆ â—‹â—‹? {e}",
    "í˜„ì§€ì„œ í„°ì§„ â—â— {e}"
]
EMOJIS = ["ğŸš¨","ğŸŒ","ğŸ’¡","ğŸ¤”","ğŸ‡§ğŸ‡¾","ğŸ“°","âœˆï¸","âš¡"]

def strip_hanja(txt:str)->str:              # í•œì ì œê±°
    return HANJA_R.sub("", txt)

def quick_ko_title(src:str)->str:           # fallback
    t = strip_hanja(src)
    t = RUEN_R.sub("", t)                   # ëŸ¬Â·ì˜ ì‚­ì œ
    t = re.sub(r"\s+"," ",t).strip()
    return (t[:30] or "ë²¨ë¼ë£¨ìŠ¤ í˜„ì§€ ì†Œì‹") + " ğŸ“°"

def ensure_catchy(title:str, kw:str)->str:
    title = strip_hanja(title or "")
    if RUEN_R.search(title) or len(title)<15 or title.endswith("."):
        title=""
    if title: return title
    tpl=random.choice(TEMPLATES)
    return tpl.replace("â—‹â—‹",kw[:10]).replace("â—â—",kw[:8]).format(e=random.choice(EMOJIS))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SEO ìœ í‹¸
def pick_focus_kw(text:str)->str:           # ê°„ë‹¨ í‚¤í”„ë ˆì´ì¦ˆ ì¶”ì¶œ
    words=[w for w in re.findall(r"[ê°€-í£]{2,}", text) if len(w)<=6]
    return words[0] if words else "ë²¨ë¼ë£¨ìŠ¤"

def build_slug(title:str)->str:
    s = re.sub(r"[^\w\s]", "", unicodedata.normalize("NFKD", title))
    s = s.replace(" ","-").lower()
    return s[:90]

def make_metadesc(html:str)->str:
    txt=re.sub(r"<[^>]+>","",html)
    return re.sub(r"\s+"," ",txt)[:150]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# íƒœê·¸ ì²˜ë¦¬
def create_or_get_tag_id(name:str)->Optional[int]:
    r=requests.get(TAG_API_URL, params={"search":name})
    if r.status_code==200 and r.json():
        return r.json()[0]["id"]
    r=requests.post(TAG_API_URL,
        auth=(WP_USERNAME,WP_APP_PASSWORD),
        json={"name":name})
    return r.json().get("id") if r.status_code==201 else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í¬ìŠ¤íŒ…
def post_to_wordpress(*, title:str, content:str, tags:List[int],
                      slug:str, focus_kw:str, meta_desc:str, source_url:str)->bool:

    data = {
        "title":   title,
        "content": content,
        "status":  "publish",
        "slug":    slug,
        "tags":    tags,
        "meta": {
            "_source_url": source_url,
            "yoast_wpseo_focuskw": focus_kw,
            "yoast_wpseo_metadesc": meta_desc,
            "yoast_wpseo_title":    title
        }
    }
    r=requests.post(
        WP_API_URL, json=data, headers=HEADERS,
        auth=HTTPBasicAuth(WP_USERNAME, WP_APP_PASSWORD), timeout=30
    )
    log.info("  â†³ ê²Œì‹œ %s %s", r.status_code, r.json().get("id"))
    return r.status_code==201

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸
if __name__ == "__main__":
    log.info("ğŸ” UDF í¬ë¡¤ë§ ì‹œì‘")
    seen = load_seen_urls()
    existing = get_existing_source_urls()
    links = get_article_links()
    log.info("ğŸ”— ë©”ì¸ í˜ì´ì§€ ë§í¬ %dê°œ ìˆ˜ì§‘", len(links))

    # ì‚­ì œëœ í¬ìŠ¤íŠ¸ëŠ” ì¬ì—…ë¡œë“œ í—ˆìš©
    targets = [u for u in links
               if normalize_url(u) not in seen and normalize_url(u) not in existing]

    log.info("âš¡ ì—…ë¡œë“œ ëŒ€ìƒ %dê°œ", len(targets))
    success=0

    for url in targets:
        log.info("===== ì²˜ë¦¬ ì‹œì‘: %s =====", url)
        art=extract_article(url)
        if not art or not art["content"]: continue

        html = rewrite_with_chatgpt(art)

        # ì œëª©Â·SEO
        h1_match = re.search(r"<h1[^>]*>(.+?)</h1>", html, flags=re.S)
        gpt_title = h1_match.group(1).strip() if h1_match else ""
        focus_kw  = pick_focus_kw(gpt_title or art["title"])
        final_title = ensure_catchy(gpt_title, focus_kw)
        slug      = build_slug(final_title)
        meta_desc = make_metadesc(html)

        # íƒœê·¸ (ë²¨ë¼ë£¨ìŠ¤ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ID=20 í¬í•¨)
        tag_ids = [20]
        kw_tag  = create_or_get_tag_id(focus_kw)
        if kw_tag: tag_ids.append(kw_tag)

        ok = post_to_wordpress(
            title=final_title,
            content=html,
            tags=tag_ids,
            slug=slug,
            focus_kw=focus_kw,
            meta_desc=meta_desc,
            source_url=normalize_url(url)
        )
        if ok:
            success+=1
            seen.add(normalize_url(url))
            save_seen_urls(seen)
        log.info("===== ì²˜ë¦¬ ë: %s =====\n", url)

    log.info("ğŸ‰ ìµœì¢… ì„±ê³µ %d / %d", success, len(targets))
