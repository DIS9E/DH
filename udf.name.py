#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
UDF.name â†’ WordPress ìë™ í¬ìŠ¤íŒ… v3.3
(ì´ë¯¸ì§€ ë³¸ë¬¸ ì‚½ì…, YoastÂ·íƒœê·¸Â·ë¬¸ì²´ ê°œì„ )
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ëª¨ë“ˆ
import os, re, json, random, logging, unicodedata
from datetime import datetime
from urllib.parse import urlparse
from typing import List, Dict, Optional

import requests
from bs4 import BeautifulSoup, NavigableString, Tag
from requests.auth import HTTPBasicAuth

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½
WP_USERNAME     = os.getenv("WP_USERNAME")
WP_APP_PASSWORD = os.getenv("WP_APP_PASSWORD")
WP_API_URL      = "https://belatri.info/wp-json/wp/v2/posts"
TAG_API_URL     = "https://belatri.info/wp-json/wp/v2/tags"
OPENAI_API_KEY  = os.getenv("OPENAI_API_KEY")

UDF_BASE_URL = "https://udf.name/news/"
HEADERS      = {"User-Agent":"Mozilla/5.0 (UDF-crawler)"}
SEEN_FILE    = "seen_urls.json"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s â”‚ %(levelname)s â”‚ %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
log = logging.getLogger("udf")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸
HANJA_R = re.compile(r"[\u3400-\u4DBF\u4E00-\u9FFF]+")
RUEN_R  = re.compile(r"[A-Za-zĞ-Ğ¯Ğ°-ÑĞÑ‘]")

def normalize_url(u:str)->str:
    p=urlparse(u); return f"{p.scheme}://{p.netloc}{p.path}"

def load_seen_urls()->set:
    if os.path.exists(SEEN_FILE):
        with open(SEEN_FILE,"r",encoding="utf-8") as f: return set(json.load(f))
    return set()

def save_seen_urls(s:set):
    with open(SEEN_FILE,"w",encoding="utf-8") as f:
        json.dump(sorted(list(s)),f,ensure_ascii=False,indent=2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ WP ì¤‘ë³µ ì²´í¬
def get_existing_source_urls(max_pages=40)->set:
    page,found=1,set()
    while page<=max_pages:
        r=requests.get(WP_API_URL,
            params={"per_page":100,"page":page,"_fields":"id,meta"},
            auth=(WP_USERNAME,WP_APP_PASSWORD))
        if r.status_code!=200 or not r.json(): break
        for p in r.json():
            u=p.get("meta",{}).get("_source_url"); 
            if u: found.add(normalize_url(u))
        page+=1
    log.info("WP _source_url %dê±´",len(found))
    return found

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë§í¬ ìˆ˜ì§‘
def get_article_links()->List[str]:
    r=requests.get(UDF_BASE_URL,headers=HEADERS,timeout=15); r.raise_for_status()
    soup=BeautifulSoup(r.text,"html.parser")
    links=set()
    for a in soup.find_all("a",href=True):
        h=a["href"]
        if h.startswith("https://udf.name/news/") and h.endswith(".html"):
            links.add(normalize_url(h))
    return list(links)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ì‚¬ íŒŒì‹±
def extract_article(url:str)->Optional[Dict]:
    try:
        r=requests.get(url,headers=HEADERS,timeout=15); r.raise_for_status()
    except Exception as e:
        log.warning("ìš”ì²­ ì‹¤íŒ¨ %s | %s",url,e); return None
    s=BeautifulSoup(r.text,"html.parser")
    title  = s.find("h1",class_="newtitle")
    author = s.find("div",class_="author")
    img    = s.find("img",class_="lazy")
    body   = s.find("div",id="zooming")

    # ë³¸ë¬¸
    lines=[]
    if body:
        for el in body.descendants:
            if isinstance(el,NavigableString):
                t=el.strip()
                if t: lines.append(t)
            elif isinstance(el,Tag) and el.name in ("p","br"): lines.append("\n")
    content="\n".join(l for l in lines if l.strip())
    content=re.sub(r"dle_leech_(begin|end)","",content).strip()

    return {
        "title":  title.get_text(strip=True) if title else "",
        "author": author.get_text(strip=True) if author else "",
        "image" : "https://udf.name"+img["data-src"] if img else "",
        "url":    url,
        "content":content
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPT í”„ë¡¬í”„íŠ¸
PROMPT = """
ë„ˆëŠ” 20â€“40ëŒ€ í•œêµ­ ë…ìë¥¼ ìœ„í•œ ë²¨ë¼ë£¨ìŠ¤ ì „ë¬¸ ì—ë””í„°ì•¼.

[ëª©í‘œ]
â€¢ **ì§ì—­ ê¸ˆì§€** / ì •ë³´ ëˆ„ë½ ìµœì†Œí™” â€” ì›ë¬¸ íë¦„ ëŒ€ë¶€ë¶„ ì‚´ë¦¬ê¸°
â€¢ êµ­ë‚´ ë…ìê°€ í´ë¦­í•  ë§Œí•œ **ì¹´í”¼í˜• ì œëª©** (25~35ì, ì´ëª¨ì§€ 1ê°œ)
â€¢ `h1/h2/h3/p` HTML íƒœê·¸ë¡œ ë ˆì´ì•„ì›ƒ
â€¢ í•œìÂ·ëŸ¬ì‹œì•„ì–´Â·ì¤‘ë³µ ê¸°í˜¸ ì œê±°
â€¢ ì¤‘ê°„ì— <img src="{image_url}"> í•œ ì¤„ ì‚½ì…
â€¢ â€œë‰´ë‹ˆì»¤â€ ê°™ì€ ë¸Œëœë“œ ë‹¨ì–´ ì“°ì§€ ë§ ê²ƒ
â€¢ ë§ˆë¬´ë¦¬ì—” `by. ì—ë””í„° LEEğŸŒ³`

[ì˜ˆì‹œ ë¼ˆëŒ€]
<h1>ì œëª© ğŸ“°</h1>
<blockquote>í•œ ì¤„ í¸ì§‘ì ì£¼</blockquote>

<h2>í¬ì¸íŠ¸ ìš”ì•½ âœï¸</h2>
<ul><li>â€¦</li><li>â€¦</li></ul>

<h2>í˜„ì§€ ìƒí™© ğŸ”</h2>
<h3>ì†Œì œëª©</h3><p>ë³¸ë¬¸</p>

<h2>ì‹œì‚¬ì  ğŸ’¡</h2><p>â€¦</p>

<em>by. ì—ë””í„° LEEğŸŒ³</em>

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì•„ë˜ ì›ë¬¸ì„ ì¬êµ¬ì„±í•´:
{article_body}
"""

def rewrite(article:dict)->str:
    p=PROMPT.format(article_body=article["content"][:6000],
                    image_url=article["image"])
    r=requests.post("https://api.openai.com/v1/chat/completions",
        headers={"Authorization":f"Bearer {OPENAI_API_KEY}",
                 "Content-Type":"application/json"},
        json={"model":"gpt-4o","messages":[{"role":"user","content":p}],
              "temperature":0.3},timeout=90)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì œëª©Â·SEO
def strip_hanja(t:str)->str: return HANJA_R.sub("",t)

EMOJIS=["ğŸš¨","ğŸŒ","ğŸ‡§ğŸ‡¾","âœˆï¸","ğŸ¤”","ğŸ’¡"]

def catchy_title(raw:str,fallback_kw:str)->str:
    t=strip_hanja(raw)
    if RUEN_R.search(t) or len(t)<15: t=""
    if t: return t
    return f"{fallback_kw[:12]}â€¦ {random.choice(EMOJIS)}"

def focus_kw(txt:str)->str:
    kws=[w for w in re.findall(r"[ê°€-í£]{2,6}",txt) if len(w)>=2]
    return kws[0] if kws else "ë²¨ë¼ë£¨ìŠ¤"

def slugify(t:str)->str:
    s=re.sub(r"[^\w\s-]","",unicodedata.normalize("NFKD",t))
    return re.sub(r"\s+","-",s).lower()[:90]

def meta_desc(html:str)->str:
    return re.sub(r"\s+"," ",re.sub(r"<[^>]+>","",html))[:150]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ íƒœê·¸
BASE_TAGS=["ë²¨ë¼ë£¨ìŠ¤","ë‰´ìŠ¤"]

def ensure_tags(kw:str)->List[int]:
    ids=[]
    for name in BASE_TAGS+[kw]:
        tid=requests.get(TAG_API_URL,params={"search":name}).json()
        tid=tid[0]["id"] if tid else requests.post(
            TAG_API_URL,auth=(WP_USERNAME,WP_APP_PASSWORD),
            json={"name":name}).json().get("id")
        if tid: ids.append(tid)
    return ids

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í¬ìŠ¤íŒ…
def wp_post(title,content,slug,kw,meta,source,tags)->bool:
    data={
        "title":title,"content":content,"status":"publish","slug":slug,
        "tags":tags,
        "meta":{
            "_source_url":source,
            "_yoast_wpseo_focuskw":kw,
            "_yoast_wpseo_metadesc":meta,
            "_yoast_wpseo_title":title
        }
    }
    r=requests.post(WP_API_URL,json=data,
        auth=HTTPBasicAuth(WP_USERNAME,WP_APP_PASSWORD),
        headers=HEADERS,timeout=40)
    log.info("  â†³ ê²Œì‹œ %s %s",r.status_code,r.json().get("id"))
    return r.status_code==201

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main
if __name__=="__main__":
    log.info("ğŸ” Start")
    seen=load_seen_urls()
    existing=get_existing_source_urls()
    links=get_article_links()

    targets=[u for u in links
             if normalize_url(u) not in seen and normalize_url(u) not in existing]
    log.info("ì—…ë¡œë“œ %d / ì´ %d",len(targets),len(links))

    ok=0
    for url in targets:
        log.info("â–¶ %s",url)
        art=extract_article(url);  # noqa
        if not art or not art["content"]: continue
        html=rewrite(art)

        h1=re.search(r"<h1[^>]*>(.+?)</h1>",html,re.S)
        gtitle=h1.group(1).strip() if h1 else ""
        kw=focus_kw(gtitle or art["title"] or "ë²¨ë¼ë£¨ìŠ¤")
        title=catchy_title(gtitle,kw)
        slug =slugify(title)
        meta =meta_desc(html)
        tags =ensure_tags(kw)

        if wp_post(title,html,slug,kw,meta,normalize_url(url),tags):
            ok+=1; seen.add(normalize_url(url)); save_seen_urls(seen)
    log.info("ğŸ‰ ì™„ë£Œ %d / %d",ok,len(targets))
