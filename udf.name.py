#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
udf.name.py â€“ v3.9  (ë¡±í¼Â·ì²´ë¥˜ì‹œê°„ ê°•í™”íŒ)
â€¢ ì›ë¬¸ 100 % ìœ ì§€ + ì¹´í…Œê³ ë¦¬ë³„ ì™¸ë¶€ ë°ì´í„° ì‚½ì…
â€¢ ì„¹ì…˜ë³„ 500ìâ†‘ ìë™ í™•ì¥Â·ì œëª© ì¤‘ë³µ ì œê±°Â·ì´ë¯¸ì§€ ìº¡ì…˜
â€¢ ì˜ˆìƒ ê¸€ ê¸¸ì´ 1,200â€“1,800ì â†’ ì²´ë¥˜ 1â€“2 ë¶„ í™•ë³´
"""

import os, sys, re, json, time, logging
from urllib.parse import urljoin, urlparse, urlunparse
import requests
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WP_URL = os.getenv("WP_URL", "https://belatri.info").rstrip("/")
USER = os.getenv("WP_USERNAME")
APP_PW = os.getenv("WP_APP_PASSWORD")
OPEN_KEY = os.getenv("OPENAI_API_KEY")
if not all([USER, APP_PW, OPEN_KEY]):
    sys.exit("âŒ  WP_USERNAME / WP_APP_PASSWORD / OPENAI_API_KEY ëˆ„ë½")

POSTS_API = f"{WP_URL}/wp-json/wp/v2/posts"
TAGS_API  = f"{WP_URL}/wp-json/wp/v2/tags"

UDF_BASE = "https://udf.name/news/"
HEADERS  = {"User-Agent": "UDFCrawler/3.9"}
SEEN_FILE = "seen_urls.json"
TARGET_CAT_ID = 20

norm = lambda u: urlunparse(urlparse(u)._replace(query="", params="", fragment=""))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ seen â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_seen():
    return set(json.load(open(SEEN_FILE))) if os.path.exists(SEEN_FILE) else set()

def save_seen(s):
    json.dump(list(s), open(SEEN_FILE, "w"), ensure_ascii=False, indent=2)

def wp_exists(u_norm: str) -> bool:
    r = requests.get(POSTS_API, params={"search": u_norm, "per_page": 1},
                     auth=(USER, APP_PW), timeout=10)
    return r.ok and bool(r.json())

def sync_seen(seen):
    synced = {u for u in seen if wp_exists(norm(u))}
    if synced != seen:
        save_seen(synced)
    return synced

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë§í¬ í¬ë¡¤ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_links():
    soup = BeautifulSoup(requests.get(UDF_BASE, headers=HEADERS, timeout=10).text,
                         "html.parser")
    return list({norm(urljoin(UDF_BASE, a["href"]))
                 for a in soup.select("div.article1 div.article_title_news a[href]")})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ì‚¬ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse(url: str):
    r = requests.get(url, headers=HEADERS, timeout=10)
    if not r.ok:
        return None
    s = BeautifulSoup(r.text, "html.parser")
    title_tag = s.find("h1", class_="newtitle")
    body_tag  = s.find("div", id="zooming")
    if not (title_tag and body_tag):
        return None

    img = s.find("img", class_="lazy") or s.find("img")
    src = None
    if img:
        src = img.get("data-src") or img.get("src")
        if src and ("placeholder" in src or "default" in src):
            src = None

    cat = url.split("/news/")[1].split("/")[0]  # economic, society, politic, war, â€¦

    return {
        "title":  title_tag.get_text(strip=True),
        "html":   str(body_tag),
        "image":  urljoin(url, src) if src else None,
        "url":    url,
        "cat":    cat
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì™¸ë¶€ ë¸Œë¦¬í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_brief(cat: str, headline: str) -> str:
    snippets = []
    try:
        rss = requests.get("https://www.reuters.com/rssFeed/ru/businessNews", timeout=10).text
        titles = re.findall(r"<title>(.*?)</title>", rss)[1:3]
        snippets += [f"â€¢ Reuters: {t}" for t in titles]
    except: pass

    if cat == "economic":
        try:
            r = requests.get("https://www.nbrb.by/api/exrates/rates/usd?parammode=2", timeout=10).json()
            snippets.append(f"â€¢ NBRB <a href='https://www.nbrb.by'>USD/BLR</a> {r['Cur_OfficialRate']} ({r['Date'][:10]})")
        except: pass
    else:
        try:
            bbc = requests.get("https://feeds.bbci.co.uk/news/world/rss.xml", timeout=10).text
            title = re.search(r"<title>(.*?)</title>", bbc).group(1)
            snippets.append(f"â€¢ BBC: {title}")
        except: pass
        try:
            eia = requests.get("https://api.eia.gov/series/?api_key=DEMO_KEY&series_id=PET.RWTC.D", timeout=10).json()
            price = eia["series"][0]["data"][0][1]
            snippets.append(f"â€¢ <a href='https://www.eia.gov'>WTI</a> ${price}")
        except: pass

    snippets.append(f"â€¢ í—¤ë“œë¼ì¸ í‚¤ì›Œë“œ: {headline[:60]}")
    return "\n".join(snippets)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STYLE_GUIDE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STYLE_GUIDE = """
<h1>ğŸ“° (ì´ëª¨ì§€) í¥ë¯¸ë¡œìš´ í•œêµ­ì–´ ì œëª©</h1>
<h2>âœï¸ í¸ì§‘ì ì£¼ â€” ê¸°ì‚¬ í•µì‹¬ì„ 2ë¬¸ì¥</h2>

<h3>ğŸ“Š ìµœì‹  ë°ì´í„°</h3>
<p>(extra_context ìˆ«ìÂ·ë§í¬ ì´ìš©, <strong>500ì ì´ìƒ</strong>)</p>

<h3>ğŸ’¬ ì „ë¬¸ê°€ ì „ë§</h3>
<p>(ì‹œë‚˜ë¦¬ì˜¤Â·ìˆ«ìÂ·ê¸°ê´€ ì¸ìš© í¬í•¨, <strong>500ì ì´ìƒ</strong>)</p>

<h3>â“ Q&A</h3>
<ul>
  <li>Q1â€¦?<br><strong>A.</strong> (2ë¬¸ì¥â†‘)</li>
  <li>Q2â€¦?<br><strong>A.</strong> (2ë¬¸ì¥â†‘)</li>
  <li>Q3â€¦?<br><strong>A.</strong> (2ë¬¸ì¥â†‘)</li>
</ul>

<h3>(ë³¸ë¬¸ í•´ì„¤)</h3>
<p>ì›ë¬¸ ë¬¸ì¥ ëª¨ë‘ ìì—°ìŠ¤ëŸ½ê²Œ ì¬ë°°ì¹˜â€¦</p>

<p>ğŸ·ï¸ íƒœê·¸: ëª…ì‚¬ 3â€“6ê°œ</p>
<p>ì´ ê¸°ì‚¬ëŠ” ë²¨ë¼ë£¨ìŠ¤ í˜„ì§€ ë³´ë„ë¥¼ ì¬êµ¬ì„±í•œ ì½˜í…ì¸ ì…ë‹ˆë‹¤.<br>by. ì—ë””í„° LEEğŸŒ³</p>
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPT í˜¸ì¶œ í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chat(prompt: str, max_tok=1800, temp=0.5, model="gpt-4o") -> str:
    h = {"Authorization": f"Bearer {OPEN_KEY}", "Content-Type": "application/json"}
    d = {"model": model,
         "messages": [{"role": "user", "content": prompt}],
         "temperature": temp,
         "max_tokens": max_tok}
    r = requests.post("https://api.openai.com/v1/chat/completions",
                      headers=h, json=d, timeout=120)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"].strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¦¬ë¼ì´íŒ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def rewrite(article: dict) -> str:
    extra = build_brief(article['cat'], article['title'])
    prompt = f"""{STYLE_GUIDE}

â—† ì›ë¬¸:
{article['html']}

â—† extra_context:
{extra}
"""
    return chat(prompt)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì œëª© ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CYRILLIC = re.compile(r"[Ğ-Ğ¯Ğ°-ÑĞÑ‘]")
def korean_title(src: str, context: str) -> str:
    if not CYRILLIC.search(src):
        return src
    prompt = ("ë‹¤ìŒ ì œëª©ì„ í•œêµ­ì–´ ì¹´í”¼ë¼ì´í„° ìŠ¤íƒ€ì¼(45ìâ†“, ì´ëª¨ì§€ 1â€“3ê°œ)ë¡œ:\n"
              f"Â«{src}Â»\në¬¸ë§¥:{context[:200]}")
    return chat(prompt, max_tok=60, temp=0.9, model="gpt-4o-mini")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ íƒœê·¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STOP = {"ë²¨ë¼ë£¨ìŠ¤", "ë‰´ìŠ¤", "ê¸°ì‚¬"}
def tag_names(txt: str):
    m = re.search(r"ğŸ·ï¸.*[:ï¼š]\s*(.+)", txt)
    out = []
    if not m:
        return out
    for t in re.split(r"[,\s]+", m.group(1)):
        t = t.strip("â€“-#â€¢")
        if 1 < len(t) <= 20 and t not in STOP and t not in out:
            out.append(t)
    return out[:6]

def tag_id(name: str):
    q = requests.get(TAGS_API, params={"search": name, "per_page": 1},
                     auth=(USER, APP_PW), timeout=10)
    if q.ok and q.json():
        return q.json()[0]["id"]
    c = requests.post(TAGS_API, json={"name": name},
                      auth=(USER, APP_PW), timeout=10)
    return c.json()["id"] if c.status_code == 201 else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸¸ì´Â·í—¤ë” ê°€ë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ensure_longform(html: str, title: str) -> str:
    soup = BeautifulSoup(html, "html.parser")

    # ì œëª© ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±°
    for tag in soup.find_all(string=True):
        if title.strip() == tag.strip():
            tag.extract()

    # ê° ì„¹ì…˜ 500ì ë¯¸ë§Œì´ë©´ í™•ì¥
    for blk in soup.find_all(["p", "ul"]):
        if len(blk.get_text()) < 500:
            prompt = (f"ì•„ë˜ ë¬¸ë‹¨ì„ ê·¼ê±°Â·ìˆ«ìÂ·ì „ë§ í¬í•¨ 500ì ì´ìƒìœ¼ë¡œ í™•ì¥:\n{blk}")
            try:
                expanded = chat(prompt, max_tok=200, temp=0.7, model="gpt-4o-mini")
                blk.clear()
                blk.append(BeautifulSoup(expanded, "html.parser"))
            except: pass

    return str(soup)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²Œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def publish(article: dict, txt: str, tag_ids: list[int]):
    txt = ensure_longform(txt, article["title"])

    hidden  = f'<a href="{article["url"]}" style="display:none">src</a>\n'
    img_tag = f'<p><img src="{article["image"]}" alt=""></p>\n' if article["image"] else ""

    soup = BeautifulSoup(txt, "html.parser")

    h1 = soup.find("h1")
    orig_title = h1.get_text(strip=True) if h1 else article["title"]
    title = korean_title(orig_title, soup.get_text(" ", strip=True))
    if h1:
        h1.decompose()
    new_h1 = soup.new_tag("h1")
    new_h1.string = title
    soup.insert(0, new_h1)

    # ì´ë¯¸ì§€ ìº¡ì…˜
    if img_tag:
        img = soup.find("img")
        if img and not img.find_next_sibling("em"):
            cap = soup.new_tag("em")
            cap.string = "Photo: UDF.name"
            img.insert_after(cap)

    body = hidden + img_tag + str(soup)

    payload = {
        "title": title,
        "content": body,
        "status": "publish",
        "categories": [TARGET_CAT_ID],
        "tags": tag_ids
    }
    r = requests.post(POSTS_API, json=payload,
                      auth=(USER, APP_PW), timeout=30)
    logging.info("â†³ ê²Œì‹œ %s %s", r.status_code, r.json().get("id"))
    r.raise_for_status()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s â”‚ %(levelname)s â”‚ %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S")

    seen = sync_seen(load_seen())
    links = fetch_links()
    todo = [u for u in links if norm(u) not in seen and not wp_exists(norm(u))]
    logging.info("ğŸ“° ìƒˆ ê¸°ì‚¬ %d / ì´ %d", len(todo), len(links))

    for url in todo:
        logging.info("â–¶ %s", url)
        art = parse(url)
        time.sleep(1)
        if not art:
            continue

        try:
            txt = rewrite(art)
        except Exception as e:
            logging.warning("GPT ì˜¤ë¥˜: %s", e)
            continue

        tag_ids = [tid for n in tag_names(txt) if (tid := tag_id(n))]
        try:
            publish(art, txt, tag_ids)
            seen.add(norm(url))
            save_seen(seen)
        except Exception as e:
            logging.warning("ì—…ë¡œë“œ ì‹¤íŒ¨: %s", e)

        time.sleep(1.5)

if __name__ == "__main__":
    main()
